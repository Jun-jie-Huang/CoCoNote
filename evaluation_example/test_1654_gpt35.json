[
 {
  "nbid": "NB_112122",
  "file": "NB_112122,cases_df-1.ipynb",
  "row_id": 14,
  "target_code": "cases_df=cases_df.merge(df2, on='url'     )\ncases_df",
  "generation": "cases_df['text'] = cases_df['url'].apply(lambda x: requests.get(x).text)\ncases_df.head()"
 },
 {
  "nbid": "NB_112122",
  "file": "NB_112122,cases_df-1.ipynb",
  "row_id": 24,
  "target_code": "cases_df= cases_df.replace('\\n'    ,''  , regex=True)\ncases_df",
  "generation": "stop_words = set(stopwords.words('english'))\ncases_df['text'] = cases_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\ncases_df.head()"
 },
 {
  "nbid": "NB_2924751",
  "file": "NB_2924751,lyrics,lyrics_2_df,j-1.ipynb",
  "row_id": 7,
  "target_code": "lyrics['lyrics_cleaned'                ] =  lyrics['lyrics_no_newline'                   ].apply(clean)\nlyrics['unique_words'              ] =  lyrics['lyrics_cleaned'                ].apply(unique_words)\nlyrics['unique_num'            ] = lyrics['unique_words'              ].apply(len)\nlyrics.head()",
  "generation": "lyrics['lyrics_cleaned'] = lyrics['lyrics_no_newline'].str.replace('[^\\w\\s]','').str.lower().str.split()\nlyrics['unique_words'] = lyrics['lyrics_cleaned'].apply(set)\nlyrics['unique_num'] = lyrics['unique_words'].apply(len)\nlyrics.head()"
 },
 {
  "nbid": "NB_2986059",
  "file": "NB_2986059,df_new-1.ipynb",
  "row_id": 97,
  "target_code": "df_new[['UK'    ,'US'    ]] = pd.get_dummies(df_new['country'         ])[['UK'    ,'US'    ]]\ndf_new.head(10)",
  "generation": "df_new[['UK', 'US']] = pd.get_dummies(df_new['country'])\ndf_new = df_new.drop('country', axis=1)\ndf_new.head()"
 },
 {
  "nbid": "NB_2987223",
  "file": "NB_2987223,spotify_df-1.ipynb",
  "row_id": 2,
  "target_code": "spotify_df = spotify_df.drop(columns = ['URL'     ,'Track Name'            ,'Artist'        ,'Album'       ,'Week Start Date'                 ,'Week End Date'               ,'Explicit'          ,'Image'       ])\nspotify_df.head()",
  "generation": "spotify_df = spotify_df[['Position', 'Streams', 'Acousticness', 'Danceability', 'Duration (ms)', 'Energy', 'Instrumentalness', 'Key', 'Liveness', 'Loudness', 'Mode', 'Popularity', 'Speechiness', 'Tempo', 'Time Signature', 'Valence']]\nspotify_df.head()"
 },
 {
  "nbid": "NB_3099839",
  "file": "NB_3099839,X1-1.ipynb",
  "row_id": 10,
  "target_code": "Y= X1['Exited'        ]\nY.head()\nX1 = X1.drop(['Exited'        ],axis=1)\nX1.head()",
  "generation": "X1 = X1.drop(['Exited'], axis=1)\nX = X1.drop(['Geography_Germany', 'Geography_Spain', 'Gender_Male'], axis=1)\ny = X1['Exited']\nX.head()"
 },
 {
  "nbid": "NB_3154555",
  "file": "NB_3154555,summary_df-1.ipynb",
  "row_id": 14,
  "target_code": "summary_df = summary_df.reset_index()\nsummary_df",
  "generation": "summary_df.reset_index(inplace=True)\nsummary_df.rename(columns={'index': 'Unnamed: 0', 'Total_Degree': 'Total_Degree'}, inplace=True)\nsummary_df.head()"
 },
 {
  "nbid": "NB_3155406",
  "file": "NB_3155406,df2-1.ipynb",
  "row_id": 76,
  "target_code": "df2['intercept'           ] = 1\ndf2[['control'         ,'ab_page'         ]] = pd.get_dummies(df2['group'       ])\ndf2.head()",
  "generation": "df2['intercept'] = 1\ndf2[['control', 'treatment']] = pd.get_dummies(df2['group'])\ndf2['ab_page'] = df2['treatment']"
 },
 {
  "nbid": "NB_3170218",
  "file": "NB_3170218,df2,y-1.ipynb",
  "row_id": 75,
  "target_code": "df2 = df2.join(pd.get_dummies(df2['landing_page'              ]))\ndf2 = df2.drop('old_page'          , axis=1)\ndf2['new_CA'        ] = df2['new_page'          ] * df2['CA'    ]\ndf2['new_UK'        ] = df2['new_page'          ] * df2['UK'    ]\ndf2.head()",
  "generation": "df2['new_page'] = df2['landing_page'].apply(lambda x: 1 if x == 'new_page' else 0)\ndf2[['new_CA', 'new_UK']] = pd.get_dummies(df2[['country', 'new_page']])[['CA', 'UK']]\ndf2.head()"
 },
 {
  "nbid": "NB_3252357",
  "file": "NB_3252357,X-1.ipynb",
  "row_id": 44,
  "target_code": "X['Family'        ] = X['SibSp'       ] + X['Parch'       ]\nX['Fare'      ] = ((X['Fare'      ] - X['Fare'      ].min())/(X['Fare'      ].max() - X['Fare'      ].min()))\ndel(X['PassengerId'             ])\ndel(X['Name'      ])\ndel(X['Ticket'        ])\ndel(X['Cabin'       ])\ndel(X['SibSp'       ])\ndel(X['Parch'       ])\ndel(X['Age'     ])\nX = pd.get_dummies(X)\nX.head()",
  "generation": "X = df_init\ny_first = pd.DataFrame()\ny_first['PassengerId'] = X['PassengerId']\nX['Family'] = X['SibSp'] + X['Parch']\nX['Sex'] = X['Sex'].map({'male': 1, 'female': 0})\nX['Embarked'] = X['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\nX = X.drop(['Name', 'Ticket', 'Cabin', 'SibSp', 'Parch', 'PassengerId'], axis=1)\nX = pd.get_dummies(X, columns=['Pclass', 'Embarked'])\nX['Fare'] = X['Fare'] / X['Fare'].max()\ny_first = X\ny_first.head()"
 },
 {
  "nbid": "NB_3296179",
  "file": "NB_3296179,df,x_scaled-1.ipynb",
  "row_id": 12,
  "target_code": "le = preprocessing.LabelEncoder()\ndf.job = le.fit_transform(df.job)\ndf.marital = le.fit_transform(df.marital)\ndf.education = le.fit_transform(df.education)\ndf.housing = le.fit_transform(df.housing)\ndf.loan = le.fit_transform(df.loan)\ndf.poutcome = le.fit_transform(df.poutcome)\ndf.head()",
  "generation": "le = preprocessing.LabelEncoder()\ndf['job'] = le.fit_transform(df['job'])\ndf['marital'] = le.fit_transform(df['marital'])\ndf['education'] = le.fit_transform(df['education'])\ndf['housing'] = le.fit_transform(df['housing'])\ndf['loan'] = le.fit_transform(df['loan'])\ndf['poutcome'] = le.fit_transform(df['poutcome'])\ndf['deposit'] = le.fit_transform(df['deposit'])\ndf = df.drop(['contact', 'default', 'day', 'month', 'duration', 'pdays'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_3303153",
  "file": "NB_3303153,df-1.ipynb",
  "row_id": 8,
  "target_code": "df['Age'     ] = df['Age'     ].fillna(df['Age'     ].mean())\ndf.head(10)",
  "generation": "df['Age'] = df['Age'].fillna(df['Age'].mean())"
 },
 {
  "nbid": "NB_3303153",
  "file": "NB_3303153,df-1.ipynb",
  "row_id": 33,
  "target_code": "df['Name'      ] = df['Name'      ].fillna(3)\ndf.head(10)",
  "generation": "df['Age'] = df['Age'].fillna(df['Age'].mean())\ndf['Fare'] = df['Fare'].fillna(df['Fare'].mean())\ndf['Embarked'] = df['Embarked'].fillna('S')\ndf['Embarked'] = df['Embarked'].map({\n    'S': 0,\n    'C': 1,\n    'Q': 2\n});\ndf['Sex'] = df['Sex'].map({\n    'male': 0,\n    'female': 1\n});\ndf['Name'] = df['Name'].map({\n    'Mr': 0,\n    'Mrs': 1,\n    'Miss': 2\n})"
 },
 {
  "nbid": "NB_3314470",
  "file": "NB_3314470,star_wars,females-1.ipynb",
  "row_id": 6,
  "target_code": "star_wars = star_wars.rename(columns={\n        \"Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.\"                                                                                                                                                       : \"ranking_1\"            ,\n        \"Unnamed: 10\"                     : \"ranking_2\"            ,\n        \"Unnamed: 11\"                     : \"ranking_3\"            ,\n        \"Unnamed: 12\"                     : \"ranking_4\"            ,\n        \"Unnamed: 13\"                     : \"ranking_5\"            ,\n        \"Unnamed: 14\"                     : \"ranking_6\"            \n})\nstar_wars.head()",
  "generation": "star_wars = star_wars.rename(columns={\n    \"Which of the following Star Wars films have you seen? Please select all that apply.\": \"seen_1\",\n        \"Unnamed: 4\": \"seen_2\",\n        \"Unnamed: 5\": \"seen_3\",\n        \"Unnamed: 6\": \"seen_4\",\n        \"Unnamed: 7\": \"seen_5\",\n        \"Unnamed: 8\": \"seen_6\"\n})"
 },
 {
  "nbid": "NB_3317955",
  "file": "NB_3317955,df-1.ipynb",
  "row_id": 4,
  "target_code": "df['City_Category'               ] = df['City_Category'               ].fillna('city'      )\ndf.head()",
  "generation": "df['City_Category'] = df['City_Category'].fillna('city')\ndf.head()"
 },
 {
  "nbid": "NB_3317955",
  "file": "NB_3317955,df-1.ipynb",
  "row_id": 5,
  "target_code": "df['City_Category'               ] = df['City_Category'               ].map({\n    'A'       : 'Metro Cities'               ,\n    'B'       : 'Small Towns'              ,\n    'C'       : 'Villages'           \n})\ndf.head()",
  "generation": "df['City_Category'] = df['City_Category'].fillna('Metro Cities')\ndf['City_Category'] = df['City_Category'].replace({'city': 'Villages'})\ndf.head()"
 },
 {
  "nbid": "NB_3317955",
  "file": "NB_3317955,df-1.ipynb",
  "row_id": 7,
  "target_code": "df['Marital_Status'                ] = df['Marital_Status'                ].map({\n    0:'Unmarried'           ,\n    1:'Married'         \n})\ndf.head()",
  "generation": "df['Marital_Status'] = df['Marital_Status'].map({\n    0: 'Unmarried',\n    1: 'Married'\n})\ndf.head()"
 },
 {
  "nbid": "NB_3344038",
  "file": "NB_3344038,data,scaled_data_df-1.ipynb",
  "row_id": 3,
  "target_code": "data = data.drop(['Name'      ,'#'   ],axis=1)\ndata.head(5)",
  "generation": "data = data.drop(['#', 'Name', 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed', 'Generation', 'Legendary'], axis=1)\ndata.head(5)"
 },
 {
  "nbid": "NB_3344038",
  "file": "NB_3344038,data,scaled_data_df-1.ipynb",
  "row_id": 6,
  "target_code": "data['Type 2'        ] = data['Type 2'        ].replace(np.NaN, 'Dummy'        )\ndata.head(5)",
  "generation": "data['Type 2'] = data['Type 2'].fillna('Dummy')"
 },
 {
  "nbid": "NB_3421952",
  "file": "NB_3421952,complete_df-1.ipynb",
  "row_id": 95,
  "target_code": "complete_df = complete_df.join(pd.get_dummies(complete_df['country'         ]))\ncomplete_df.head()",
  "generation": "complete_df[['CA', 'UK', 'US']] = pd.get_dummies(complete_df['country'])\ncomplete_df.drop(['country'], axis=1, inplace=True)\ncomplete_df.head()"
 },
 {
  "nbid": "NB_3525401",
  "file": "NB_3525401,suicide_rate-1.ipynb",
  "row_id": 10,
  "target_code": "ind = [2, 3, 4, 1, 5, 6]\nsuicide_rate[\"index_col\"           ] = ind\nsuicide_rate = suicide_rate.sort_values(\"index_col\"           )[\"suicide_rates_%\"                 ]\nsuicide_rate",
  "generation": "suicide_rate = suicide_rate.sort_index() \nsuicide_rate"
 },
 {
  "nbid": "NB_3525401",
  "file": "NB_3525401,suicide_sum-1.ipynb",
  "row_id": 6,
  "target_code": "suicide_sum = suicide_sum.unstack()\nsuicide_sum",
  "generation": "suicide_sum = pd.DataFrame(group_data[\"suicides_no\"].sum())\nsuicide_sum = suicide_sum.unstack()\nsuicide_sum.columns = suicide_sum.columns.droplevel()\nsuicide_sum.reset_index(inplace=True)\nsuicide_sum"
 },
 {
  "nbid": "NB_3525401",
  "file": "NB_3525401,suicide_sum-1.ipynb",
  "row_id": 7,
  "target_code": "ind = [2, 3, 4, 1, 5, 6]\nsuicide_sum[\"index_col\"           ] = ind\nsuicide_sum = suicide_sum.sort_values(\"index_col\"           )['suicides_no'             ]\nsuicide_sum",
  "generation": "suicide_sum = suicide_sum.sort_values(by=('age', ''), ascending=False)\nsuicide_sum.columns = suicide_sum.columns.droplevel()\nsuicide_sum.index.name = None\nsuicide_sum.columns.name = None\nsuicide_sum"
 },
 {
  "nbid": "NB_815682",
  "file": "NB_815682,output,y,selector-1.ipynb",
  "row_id": 18,
  "target_code": "output=output.drop(['std_total'           ], axis=1)\noutput",
  "generation": "output = df[['std_exp', 'contraception', 'abortion','obesity','tuberculosis','pre_puberty','park','internet','per_capita_income','cosmetic']]\noutput"
 },
 {
  "nbid": "NB_815798",
  "file": "NB_815798,df,no_show-1.ipynb",
  "row_id": 59,
  "target_code": "df['No_show'         ] = df['No_show'         ].replace(\"No\"    , 0)\ndf['No_show'         ] = df['No_show'         ].replace(\"Yes\"     , 1)\ndf.head()",
  "generation": "df['No_show'] = df['No_show'].replace({'No': 0, 'Yes': 1})\ndf = df.drop('appoint_timestamp', axis=1)\ndf.head(2)"
 },
 {
  "nbid": "NB_815903",
  "file": "NB_815903,Data-1.ipynb",
  "row_id": 3,
  "target_code": "Data = Data.drop('Unnamed: 5'             ,axis=1)\nData.head()",
  "generation": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nData = pd.read_csv('https://raw.githubusercontent.com/reddyprasade/Data-Sets-For-Machine-Learnig-and-Data-Science/master/DataSets/Weather.csv')\nData = Data.drop(['Unnamed: 5'], axis=1)\nData['Outlook'] = le.fit_transform(Data['Outlook'])\nData['Temp'] = le.fit_transform(Data['Temp'])\nData['Humidity'] = le.fit_transform(Data['Humidity'])\nData['Windy'] = le.fit_transform(Data['Windy'])\nData['Play'] = le.fit_transform(Data['Play'])\nData.head()"
 },
 {
  "nbid": "NB_819552",
  "file": "NB_819552,df-1.ipynb",
  "row_id": 6,
  "target_code": "df = df.replace(['yes'     ,'no'    ], [1,0])\ndf = df.replace(['female'        ,'male'      ], [1,0])\ndf.head()",
  "generation": "le = LabelEncoder()\ndf['sex'] = le.fit_transform(df['sex'])\ndf['smoker'] = le.fit_transform(df['smoker'])\ndf.head()"
 },
 {
  "nbid": "NB_822607",
  "file": "NB_822607,new_df-1.ipynb",
  "row_id": 12,
  "target_code": "new_df=new_df[new_df.crime != ('TRASLADO DE PRESOS A TERRITORIO NACIONAL DE CONFORMIDAD CON TRATADOS BILATERALES'                                                                                  )]\nnew_df",
  "generation": "df['2017'] = ((df['DESCONOCIDO_2017'] + df['HOMBRE_2017']) + df['MUJER_2017'])\nnew_df = df[['crime','Total_2010', 'Total_2011', 'Total_2012', 'Total_2013', 'Total_2014', 'Total_2015', 'Total_2016', 'Total_2017']].copy()\nnew_df = new_df.sort_values(by='Total_2017', ascending=False)\nnew_df"
 },
 {
  "nbid": "NB_834485",
  "file": "NB_834485,Y_condition2-1.ipynb",
  "row_id": 78,
  "target_code": "Y_condition2=Y_condition2.drop('condition'           , axis=1)\nY_condition2.head()",
  "generation": "Y_condition2 = Y_condition2.drop(['subject', 'trial'], axis=1)\nY_condition2 = Y_condition2[['group']]\nY_condition2.head(101)"
 },
 {
  "nbid": "NB_834485",
  "file": "NB_834485,Y_condition3-1.ipynb",
  "row_id": 80,
  "target_code": "Y_condition3=Y_condition3.drop('condition'           , axis=1)\nY_condition3.head()",
  "generation": "Y_condition3 = Y_condition3.drop(['condition'], axis=1)\nY_condition3.head(101)"
 },
 {
  "nbid": "NB_849177",
  "file": "NB_849177,churn_df-1.ipynb",
  "row_id": 54,
  "target_code": "churn_df = churn_df[[\"tenure\"        , \"age\"      , \"address\"          , \"income\"         , \"ed\"     , \"employ\"         , \"equip\"        , \"callcard\"           , \"wireless\"           , \"churn\"        ]]\nchurn_df[\"churn\"       ].astype(int)\nchurn_df.head()",
  "generation": "X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])\ny = np.asarray(churn_df['churn'])\ny = y.astype('int')"
 },
 {
  "nbid": "NB_850358",
  "file": "NB_850358,temp_by_country-1.ipynb",
  "row_id": 15,
  "target_code": "country_to_gdp = world_gdp.set_index('COUNTRY'         ).to_dict()['GDP (BILLIONS)'                ]\ntemp_by_country['GDP'     ] = temp_by_country['Country'         ].map(lambda c: country_to_gdp.get(c, np.nan))\ntemp_by_country.tail(3)",
  "generation": "temp_by_country['GDP'] = world_gdp['GDP']\ntemp_by_country.tail(3)"
 },
 {
  "nbid": "NB_918497",
  "file": "NB_918497,crime_anal,x_scaled,crime_anal_norm-1.ipynb",
  "row_id": 17,
  "target_code": "crime_anal['\uac15\uac04\uac80\uac70\uc728'       ] = crime_anal['\uac15\uac04 \uac80\uac70'       ]/crime_anal['\uac15\uac04 \ubc1c\uc0dd'       ]*100\ncrime_anal['\uac15\ub3c4\uac80\uac70\uc728'       ] = crime_anal['\uac15\ub3c4 \uac80\uac70'       ]/crime_anal['\uac15\ub3c4 \ubc1c\uc0dd'       ]*100\ncrime_anal['\uc0b4\uc778\uac80\uac70\uc728'       ] = crime_anal['\uc0b4\uc778 \uac80\uac70'       ]/crime_anal['\uc0b4\uc778 \ubc1c\uc0dd'       ]*100\ncrime_anal['\uc808\ub3c4\uac80\uac70\uc728'       ] = crime_anal['\uc808\ub3c4 \uac80\uac70'       ]/crime_anal['\uc808\ub3c4 \ubc1c\uc0dd'       ]*100\ncrime_anal['\ud3ed\ub825\uac80\uac70\uc728'       ] = crime_anal['\ud3ed\ub825 \uac80\uac70'       ]/crime_anal['\ud3ed\ub825 \ubc1c\uc0dd'       ]*100\ndel crime_anal['\uac15\uac04 \uac80\uac70'       ]\ndel crime_anal['\uac15\ub3c4 \uac80\uac70'       ]\ndel crime_anal['\uc0b4\uc778 \uac80\uac70'       ]\ndel crime_anal['\uc808\ub3c4 \uac80\uac70'       ]\ndel crime_anal['\ud3ed\ub825 \uac80\uac70'       ]\ncrime_anal.head()",
  "generation": "crime_anal.drop(['\uac15\uac04 \uac80\uac70', '\uac15\ub3c4 \uac80\uac70', '\uc0b4\uc778 \uac80\uac70', '\uc808\ub3c4 \uac80\uac70', '\ud3ed\ub825 \uac80\uac70'], axis=1, inplace=True)\ncrime_anal['\uac15\uac04\uac80\uac70\uc728' ] = crime_anal['\uac15\uac04 \uac80\uac70' ] / crime_anal['\uac15\uac04 \ubc1c\uc0dd' ] * 100\ncrime_anal['\uac15\ub3c4\uac80\uac70\uc728' ] = crime_anal['\uac15\ub3c4 \uac80\uac70' ] / crime_anal['\uac15\ub3c4 \ubc1c\uc0dd' ] * 100\ncrime_anal['\uc0b4\uc778\uac80\uac70\uc728' ] = crime_anal['\uc0b4\uc778 \uac80\uac70' ] / crime_anal['\uc0b4\uc778 \ubc1c\uc0dd' ] * 100\ncrime_anal['\uc808\ub3c4\uac80\uac70\uc728' ] = crime_anal['\uc808\ub3c4 \uac80\uac70' ] / crime_anal['\uc808\ub3c4 \ubc1c\uc0dd' ] * 100\ncrime_anal['\ud3ed\ub825\uac80\uac70\uc728' ] = crime_anal['\ud3ed\ub825 \uac80\uac70' ] / crime_anal['\ud3ed\ub825 \ubc1c\uc0dd' ] * 100\ncrime_anal.drop(['\uac15\uac04 \uac80\uac70', '\uac15\ub3c4 \uac80\uac70', '\uc0b4\uc778 \uac80\uac70', '\uc808\ub3c4 \uac80\uac70', '\ud3ed\ub825 \uac80\uac70'], axis=1, inplace=True)\ncrime_anal.head()"
 },
 {
  "nbid": "NB_919412",
  "file": "NB_919412,covid_train4-1.ipynb",
  "row_id": 20,
  "target_code": "covid_train4=pd.DataFrame(covid_train4.groupby('Country_Region'                )['ConfirmedCases'                ,'Fatalities'            ].sum())\ncovid_train4",
  "generation": "covid_train4 = covid_train4[['Country_Region','ConfirmedCases','Fatalities']]\ncovid_train4 = covid_train4[covid_train4['Date']=='2020-05-15']\ncovid_train4 = covid_train4.groupby('Country_Region').sum()\ncovid_train4.head()"
 },
 {
  "nbid": "NB_925972",
  "file": "NB_925972,vds-1.ipynb",
  "row_id": 50,
  "target_code": "import nltk \nfrom nltk.corpus import stopwords\nstop = stopwords.words(\"english\"         )\nvds['text'      ] = vds['text'      ].apply(lambda x: \" \"    .join([word for word in x.split() if word not in (stop)]))\nvds.head()",
  "generation": "stop_words = set(stopwords.words('english'))\nps = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess_text(text):\n    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n    text = ' '.join([ps.stem(word) for word in text.split()])\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n    return text\n\ndf['text'] = df['text'].apply(preprocess_text)\ndf.head()"
 },
 {
  "nbid": "NB_925972",
  "file": "NB_925972,vds-1.ipynb",
  "row_id": 51,
  "target_code": "from nltk.stem import PorterStemmer\nps = PorterStemmer()\nvds['text'      ] = vds['text'      ].apply(lambda x : ' '    .join([ps.stem(word) for word in x.split()]))\nvds.head()",
  "generation": "stemmer = PorterStemmer()\nvds['text'] = vds['text'].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))\nvds.head()"
 },
 {
  "nbid": "NB_926143",
  "file": "NB_926143,data,women-1.ipynb",
  "row_id": 53,
  "target_code": "data['split_frac'            ] = 1 - 2 * data['split_sec'           ] / data['final_sec'           ]\ndata.head()",
  "generation": "data['split'] = pd.to_timedelta(data['split'])\ndata['final'] = pd.to_timedelta(data['final'])\ndata['split_sec'] = data['split'].astype('timedelta64[s]')\ndata['final_sec'] = data['final'].astype('timedelta64[s]')\ndata['split_frac'] = 1 - 2 * data['split_sec'] / data['final_sec']\ndata.head()"
 },
 {
  "nbid": "NB_3351469",
  "file": "NB_3351469,review-1.ipynb",
  "row_id": 10,
  "target_code": "review['text_len'          ]=review['text'      ].apply(len)\nreview.head()",
  "generation": "review['text_len'] = review['text'].apply(len)\nreview"
 },
 {
  "nbid": "NB_3374803",
  "file": "NB_3374803,df,df_standard-1.ipynb",
  "row_id": 29,
  "target_code": "df['Price-Smoothing-max'                     ] = df.groupby('Bin'     )['Price'       ].transform('max'     )\ndf.head()",
  "generation": "df[\"Price-Smoothing-max\"] = df.groupby('Bin')['Price'].transform('max')\ndf.head()"
 },
 {
  "nbid": "NB_3389659",
  "file": "NB_3389659,crime-1.ipynb",
  "row_id": 41,
  "target_code": "con_list = ['\uac15\uac04\uac80\uac70\uc728'       , '\uac15\ub3c4\uac80\uac70\uc728'        , '\uc0b4\uc778\uac80\uac70\uc728'        , '\uc808\ub3c4\uac80\uac70\uc728'        , '\ud3ed\ub825\uac80\uac70\uc728'        ]\nfor i in con_list:\n    crime.loc[crime[i] > 100, i] = 100\ncrime.head()",
  "generation": "crime[['\uac15\uac04\uac80\uac70\uc728', '\uac15\ub3c4\uac80\uac70\uc728', '\uc0b4\uc778\uac80\uac70\uc728', '\uc808\ub3c4\uac80\uac70\uc728', '\ud3ed\ub825\uac80\uac70\uc728']] = preprocessing.MinMaxScaler().fit_transform(crime[['\uac15\uac04\uac80\uac70\uc728', '\uac15\ub3c4\uac80\uac70\uc728', '\uc0b4\uc778\uac80\uac70\uc728', '\uc808\ub3c4\uac80\uac70\uc728', '\ud3ed\ub825\uac80\uac70\uc728']]) * 100\ncrime.head()"
 },
 {
  "nbid": "NB_3391430",
  "file": "NB_3391430,wine,y-1.ipynb",
  "row_id": 4,
  "target_code": "wine['rating'        ] = wine.quality[wine.quality >= 6]\nwine['rating'        ] = wine['rating'        ].replace((6,7,8),1)\nwine['rating'        ] = wine['rating'        ].replace(np.nan,0)\nwine.head()",
  "generation": "bins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\ncategories = pd.cut(wine['quality'], bins, labels=group_names)\nwine['rating'] = categories\nwine.head()"
 },
 {
  "nbid": "NB_3394193",
  "file": "NB_3394193,critics-1.ipynb",
  "row_id": 8,
  "target_code": "critics = critics[~critics.quote.isnull()]\ncritics.head()",
  "generation": "critics = critics[critics.quote.notnull()]\ncritics = critics.reset_index(drop=True)\ncritics.head()"
 },
 {
  "nbid": "NB_3403156",
  "file": "NB_3403156,df-1.ipynb",
  "row_id": 28,
  "target_code": "df = df.drop(columns='item_price'            )\ndf.head(10)",
  "generation": "df = df.drop('item_price', axis=1) \ndf.head()"
 },
 {
  "nbid": "NB_3403259",
  "file": "NB_3403259,amazon_df,negative-1.ipynb",
  "row_id": 28,
  "target_code": "amazon_df.drop(['variation'           ], axis=1, inplace=True)\namazon_df = pd.concat([amazon_df, variation_dummies], axis=1)\namazon_df.head()",
  "generation": "vectorizer = TfidfVectorizer(stop_words='english')\nreviews = vectorizer.fit_transform(amazon_df['verified_reviews'])\nreviews_df = pd.DataFrame(reviews.toarray(), columns=vectorizer.get_feature_names())\namazon_df = pd.concat([amazon_df, reviews_df], axis=1)\namazon_df = amazon_df.drop(['variation', 'verified_reviews'], axis=1)\nvariation_dummies = pd.get_dummies(amazon_df['variation'], drop_first=True)\namazon_df = pd.concat([amazon_df, variation_dummies], axis=1)\namazon_df.head()"
 },
 {
  "nbid": "NB_3406992",
  "file": "NB_3406992,df,h_budget-1.ipynb",
  "row_id": 12,
  "target_code": "del_col = [ 'id'     , 'imdb_id'          , 'popularity'             , 'budget_adj'             , 'revenue_adj'              , 'homepage'           , 'keywords'           , 'overview'           , 'production_companies'                       , 'vote_count'             , 'vote_average'               ]\ndf = df.drop(del_col, 1)\ndf.head()",
  "generation": "df = df[['budget', 'revenue', 'original_title', 'cast', 'director', 'tagline', 'runtime', 'genres', 'release_date', 'release_year']]\ndf.head()"
 },
 {
  "nbid": "NB_3416728",
  "file": "NB_3416728,df-1.ipynb",
  "row_id": 8,
  "target_code": "df = df[df[3] != \"Not Available\"                ]\ndf.head()",
  "generation": "df = df[df[3] != 'Not Available']\ndf = df.reset_index(drop=True)\ndf.head()"
 },
 {
  "nbid": "NB_3421824",
  "file": "NB_3421824,housing-1.ipynb",
  "row_id": 2,
  "target_code": "housing[\"mainroad\"          ] = housing[\"mainroad\"          ].map({\"yes\"     : 1, \"no\"     : 0})\nhousing[\"guestroom\"           ] = housing[\"guestroom\"           ].map({\"yes\"     : 1, \"no\"     : 0})\nhousing[\"basement\"          ] = housing[\"basement\"          ].map({\"yes\"     : 1, \"no\"     : 0})\nhousing[\"hotwaterheating\"                 ] = housing[\"hotwaterheating\"                 ].map({\"yes\"     : 1, \"no\"     : 0})\nhousing[\"airconditioning\"                 ] = housing[\"airconditioning\"                 ].map({\"yes\"     : 1, \"no\"     : 0})\nhousing[\"prefarea\"          ] = housing[\"prefarea\"          ].map({\"yes\"     : 1, \"no\"     : 0})\nhousing.head()",
  "generation": "housing['furnishingstatus'] = pd.get_dummies(housing['furnishingstatus'], drop_first=True)\nhousing['mainroad'        ] = housing['mainroad'        ].map({'yes': 1, 'no': 0})\nhousing['guestroom'       ] = housing['guestroom'       ].map({'yes': 1, 'no': 0})\nhousing['basement'        ] = housing['basement'        ].map({'yes': 1, 'no': 0})\nhousing['hotwaterheating' ] = housing['hotwaterheating' ].map({'yes': 1, 'no': 0})\nhousing['airconditioning' ] = housing['airconditioning' ].map({'yes': 1, 'no': 0})\nhousing['prefarea'        ] = housing['prefarea'        ].map({'yes': 1, 'no': 0})\nhousing.head()"
 },
 {
  "nbid": "NB_3421824",
  "file": "NB_3421824,housing-1.ipynb",
  "row_id": 4,
  "target_code": "housing = housing.drop(\"furnishingstatus\"                  , axis=1)\nhousing = pd.concat([housing, fur], axis = 1)\nhousing.head()",
  "generation": "housing = pd.get_dummies(housing, columns=[\"furnishingstatus\"])\nhousing.head()"
 },
 {
  "nbid": "NB_3431568",
  "file": "NB_3431568,combi-1.ipynb",
  "row_id": 26,
  "target_code": "combi['clean_tweet'             ] = combi['clean_tweet'             ].str.replace(\"[^a-zA-Z#]\"            , \" \"    )\ncombi.head(5)",
  "generation": "combi['clean_tweet'] = combi['clean_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ncombi.head()"
 },
 {
  "nbid": "NB_3437147",
  "file": "NB_3437147,star_wars,females-1.ipynb",
  "row_id": 6,
  "target_code": "yes_no = {\n    \"Yes\"         : True,\n    \"No\"        : False\n}\nstar_wars['Have you seen any of the 6 films in the Star Wars franchise?'                                                              ] =star_wars['Have you seen any of the 6 films in the Star Wars franchise?'                                                              ].map(yes_no)\nstar_wars['Do you consider yourself to be a fan of the Star Wars film franchise?'                                                                       ] =star_wars['Do you consider yourself to be a fan of the Star Wars film franchise?'                                                                       ].map(yes_no)\nstar_wars.head()",
  "generation": "star_wars['RespondentID'] = star_wars['RespondentID'].notnull()\nstar_wars.head()"
 },
 {
  "nbid": "NB_3439308",
  "file": "NB_3439308,df-1.ipynb",
  "row_id": 2,
  "target_code": "max_pop = max(df['population'            ])\ndf['relative_pop'              ] = df['population'            ]/max_pop\ndf.head()",
  "generation": "df['relative_pop'] = df['population_proper'] / df['population']\ndf.head()"
 },
 {
  "nbid": "NB_3439696",
  "file": "NB_3439696,df-1.ipynb",
  "row_id": 1,
  "target_code": "y = df[['Survived'          ]]\ndf = df.drop(['Name'      , 'PassengerId'              , 'Ticket'         , 'Survived'           ], axis=1)\ny_copy = y.copy()\ndf_copy = df.copy()\ndf",
  "generation": "df = df.drop(['PassengerId', 'Survived', 'Name', 'Ticket'], axis=1)\ndf = df.dropna(subset=['Embarked'])\ndf = df.drop(['Cabin'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_3441992",
  "file": "NB_3441992,tc-1.ipynb",
  "row_id": 19,
  "target_code": "tc['family_size'             ]=tc['sibsp'       ]+tc['parch'       ]+1\ntc.head(3)",
  "generation": "tc['family_size'] = tc['sibsp'] + tc['parch'] + 1\ntc.head(3)"
 },
 {
  "nbid": "NB_3446252",
  "file": "NB_3446252,df,y,garis_miring-1.ipynb",
  "row_id": 4,
  "target_code": "df = df.rename(columns={'parental level of education'                             :'parental_level_of_education'                             , 'math score'             :'math_score'            , 'reading score'                :'reading_score'               , 'writing score'                :'writing_score'               })\ndf",
  "generation": "df.columns = df.columns.str.replace(' ', '_')\ndf.head()"
 },
 {
  "nbid": "NB_3449798",
  "file": "NB_3449798,df-1.ipynb",
  "row_id": 85,
  "target_code": "df['page_received'               ] = pd.get_dummies(df['landing_page'              ])['new_page'          ]\ndf['intercept'           ] = 1\ndf['ab_page'         ] = pd.get_dummies(df['group'       ])['treatment'           ]\ndf.head()",
  "generation": "df['intercept'] = 1\ndf[['control', 'ab_page']] = pd.get_dummies(df['group'])\ndf['page_received'] = pd.get_dummies(df['landing_page'])['new_page']\ndf = df.drop(columns=['control'])\ndf.head()"
 },
 {
  "nbid": "NB_3449798",
  "file": "NB_3449798,df_new-1.ipynb",
  "row_id": 101,
  "target_code": "df_new['US'    ] = pd.get_dummies(df_new['country'         ])['US'    ]\ndf_new['UK'    ] = pd.get_dummies(df_new['country'         ])['UK'    ]\ndf_new['CA'    ] = pd.get_dummies(df_new['country'         ])['CA'    ]\ndf_new.head()",
  "generation": "df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])\ndf_new = df_new.drop(columns=['country'])\ndf_new = df_new.drop(columns=['timestamp'])\ndf_new = df_new.drop(columns=['landing_page'])\ndf_new = df_new.drop(columns=['group'])\ndf_new = df_new.drop(columns=['Unnamed: 1_level_1'])\ndf_new = df_new.drop(columns=['Unnamed: 2_level_1'])\ndf_new = df_new.drop(columns=['Unnamed: 3_level_1'])\ndf_new = df_new.drop(columns=['Unnamed: 4_level_1'])\ndf_new = df_new.drop(columns=['Unnamed: 5_level_1'])\ndf_new = df_new.drop(columns=['Unnamed: 6_level_1'])\ndf_new = df_new.drop(columns=['Unnamed: 7_level_1'])\ndf_new = df_new.drop(columns=['Unnamed: 8_level_1'])\ndf_new.head()"
 },
 {
  "nbid": "NB_735590",
  "file": "NB_735590,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.drop(axis= 0, index= 0)\nprint('The shape of the dataset is'                             , df.shape)\ndf.head()",
  "generation": "df = df.dropna()\ndf.head()"
 },
 {
  "nbid": "NB_735590",
  "file": "NB_735590,df-1.ipynb",
  "row_id": 10,
  "target_code": "df = df.drop(labels=df[df['Non-State'           ].isnull()==False].isnull().index.tolist(),axis=0)\ndf = df.drop(columns='Non-State'           )\nprint('The new shape of the dataset is'                                 , df.shape)\ndf.head()",
  "generation": "df = df.drop(axis=0, index=0)\ndf = df.set_index('Country')\ndf.head()"
 },
 {
  "nbid": "NB_7749954",
  "file": "NB_7749954,X_train-1.ipynb",
  "row_id": 3,
  "target_code": "from sklearn.preprocessing import LabelEncoder\nlbenc = LabelEncoder()\nX_train.fillna(value=0)\nX_train = pd.get_dummies(X_train)\nX_train.drop(['Street_Pave'             , 'SaleCondition_Partial'                        ], axis=1, inplace=True)\nX_train.dropna(axis=0, inplace=True)\nX_train.head()",
  "generation": "le = LabelEncoder()\nX_train['Street'] = le.fit_transform(X_train['Street'])\nX_test['Street'] = le.transform(X_test['Street'])\nX_train = pd.get_dummies(X_train, columns=['SaleCondition'])\nX_test = pd.get_dummies(X_test, columns=['SaleCondition'])\ny_train = X_train.pop('SalePrice')\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nX_test['SalePrice'] = y_pred\nX_test['Street_Grvl'] = 0\nX_test['SaleCondition_AdjLand'] = 0\nX_test['SaleCondition_Alloca'] = 0\nX_test['SaleCondition_Family'] = 0\nX_test = X_test[['MSSubClass', 'LotArea', 'SalePrice', 'Street_Grvl', 'SaleCondition_Abnorml', 'SaleCondition_AdjLand', 'SaleCondition_Alloca', 'SaleCondition_Family', 'SaleCondition_Normal']]\nX_test.head()"
 },
 {
  "nbid": "NB_878619",
  "file": "NB_878619,data_test-1.ipynb",
  "row_id": 29,
  "target_code": "data_test.superstructures = data_test.superstructures.map(factorization_map)\ndata_test.drop(['has_superstructure_rc_engineered'                                  , 'has_superstructure_rc_non_engineered'                                       , 'has_superstructure_cement_mortar_brick'                                         , \n           'has_superstructure_mud_mortar_brick'                                                , 'has_superstructure_adobe_mud'                               , 'has_superstructure_mud_mortar_stone'                                      ,  \n           'has_superstructure_bamboo'                                      , 'has_superstructure_timber'                            , 'has_superstructure_stone_flag'                                ,\n           'has_superstructure_cement_mortar_stone'                                                   , 'has_superstructure_other'                           ], inplace=True, axis=1)\ndata_test.head()",
  "generation": "data_test.drop(['has_secondary_use_school', 'has_secondary_use_industry', 'has_secondary_use_health_post', 'has_secondary_use_gov_office', 'has_secondary_use_use_police', 'superstructures'], axis=1, inplace=True)\ndata_test"
 },
 {
  "nbid": "NB_878619",
  "file": "NB_878619,data_test-1.ipynb",
  "row_id": 38,
  "target_code": "data_test.secondary = data_test.secondary.map(factorization_map).astype(int)\ndata_test.drop(['has_secondary_use_rental'                          , 'has_secondary_use_hotel'                          , 'has_secondary_use'                    , \n           'has_secondary_use_institution'                                          , 'has_secondary_use_school'                           , 'has_secondary_use_rental'                           , \n           'has_secondary_use_industry'                                       , 'has_secondary_use_health_post'                                , 'has_secondary_use_gov_office'                               , \n           'has_secondary_use_use_police'                                         , 'has_secondary_use_other'                          , 'has_secondary_use_agriculture'                                ], inplace=True, axis=1)\ndel mapping, factorization_map, factorization, secondary_factorized, superstructures_factorized\ndata_test.head()",
  "generation": "data_test['secondary'] = 0\ndata_test = map_data(data_test)\nbuilding_ids = data_test.building_id\ndata_test.drop('building_id', axis=1, inplace=True)\ndata_test = add_features(data_test)\ndata_test.superstructures = data_test.superstructures.map(factorization_map)\ndata_test.drop(['has_superstructure_rc_engineered', 'has_superstructure_rc_non_engineered', 'has_superstructure_cement_mortar_brick', \n           'has_superstructure_mud_mortar_brick', 'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone',  \n           'has_superstructure_bamboo', 'has_superstructure_timber', 'has_superstructure_stone_flag',\n           'has_superstructure_cement_mortar_stone', 'has_superstructure_other'], inplace=True, axis=1)\ndata_test['secondary'] = cat_clf.predict(data_test)\ndata_test.insert(0, 'building_id', building_ids)\ndata_test.head()"
 },
 {
  "nbid": "NB_896604",
  "file": "NB_896604,cars-1.ipynb",
  "row_id": 10,
  "target_code": "cars['company_name'              ] = cars['CarName'         ].str.split(\" \"   , n = 1, expand = True)[0]\ncars.drop(['CarName'         ], axis=1, inplace=True)\ncars.head()",
  "generation": "cars['company_name'] = cars['CarName'].apply(lambda x : x.split(' ')[0])\ncars.drop('CarName', inplace=True, axis=1)\ncars.head()"
 },
 {
  "nbid": "NB_896681",
  "file": "NB_896681,y_pred_final-1.ipynb",
  "row_id": 116,
  "target_code": "numbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_pred_final[i]= y_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_pred_final.head()",
  "generation": "cutoffs = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in cutoffs:\n    y_pred_final[i] = y_pred_final['Churn_Prob'].map(lambda x: 1 if x > i else 0)\ny_pred_final.head()"
 },
 {
  "nbid": "NB_903345",
  "file": "NB_903345,store_item_df,pred-1.ipynb",
  "row_id": 27,
  "target_code": "i = store_item_df['All'     ].sort_values().index\nc = store_item_df.loc['All'     ,:].sort_values().index\nstore_item_df = store_item_df[c]\nstore_item_df = store_item_df.reindex(i)\nstore_item_df.drop('All'     ,axis=1,inplace=True)\nstore_item_df.drop('All'     ,axis=0,inplace = True)\nstore_item_df.head()",
  "generation": "store_item_df = pd.pivot_table(data, index='item', values='sales', columns='store',margins=True, aggfunc=np.mean)\nstore_item_df = store_item_df.reindex(columns=[7,6,5,1,4,9,10,3,8,2])\nstore_item_df.head()"
 },
 {
  "nbid": "NB_906600",
  "file": "NB_906600,house_data-1.ipynb",
  "row_id": 5,
  "target_code": "house_data = house_data.drop(['id'    ,'date'      ], axis=1)\nhouse_data.head(5)",
  "generation": "house_data = house_data.drop(['id', 'date'], axis=1)\nhouse_data.insert(0,\"Unnamed: 0\"          , house_data.index)\nhouse_data.head()"
 },
 {
  "nbid": "NB_906600",
  "file": "NB_906600,house_data-1.ipynb",
  "row_id": 7,
  "target_code": "feetFeatures = ['sqft_living'             ,'sqft_lot'          ,'sqft_above'            ,'sqft_basement'               ,'sqft_living15'               ,'sqft_lot15'            ]\nhouse_data[feetFeatures] = house_data[feetFeatures].apply(lambda x: x * 0.3048 * 0.3048)\nhouse_data.columns = ['price'       ,'bedrooms'          ,'bathrooms'           ,'sqm_living'            ,'sqm_lot'         ,'floors'        ,'waterfront'            ,'view'      ,'condition'           ,\n                      'grade'                             ,'sqm_above'           ,'sqm_basement'              ,'yr_built'          ,'yr_renovated'              ,'zip_code'          ,'lat'     ,'long'      ,\n                      'sqm_living15'                                    ,'sqm_lot15'           ]\nhouse_data.head(5)",
  "generation": "house_data['sqm_living'] = house_data['sqft_living'] * 0.092903\nhouse_data['sqm_lot'   ] = house_data['sqft_lot'   ] * 0.092903\nhouse_data = house_data.rename(columns={'sqft_above': 'sqm_above', 'sqft_basement': 'sqm_basement', 'zipcode': 'zip_code'})\nhouse_data.head()"
 },
 {
  "nbid": "NB_3459728",
  "file": "NB_3459728,trends-1.ipynb",
  "row_id": 7,
  "target_code": "for column in trends.columns:\n    if \"<\"     in trends[column].to_string():\n        trends[column] = trends[column].str.replace(\"<\"   , \" \"    )\n        trends[column] = pd.to_numeric(trends[column])\ntrends.info()\ntrends.head()",
  "generation": "for col in trends.columns[2:]:\n    if trends[col].dtype == 'O':\n        trends[col] = trends[col].str.replace('<', '').astype(int)\ntrends = trends.rename(columns={'Month': 'month'})\ntrends.head()"
 },
 {
  "nbid": "NB_3463121",
  "file": "NB_3463121,mental_health_consequence_Unitedstate-1.ipynb",
  "row_id": 5,
  "target_code": "mental_health_consequence_Unitedstate=mental_health_consequence_Unitedstate.groupby(['family_history'                ])['family_history'                ].count().reset_index(name=\"count\"       )\nmental_health_consequence_Unitedstate.head(100)",
  "generation": "mental_health_consequence_Unitedstate = Raw_Data[(Raw_Data['mental_health_consequence'] == 'Yes') & (Raw_Data['Country'] == 'United States')]\nmental_health_consequence_Unitedstate = mental_health_consequence_Unitedstate[['family_history']]\nmental_health_consequence_Unitedstate['count'] = 1\nmental_health_consequence_Unitedstate = mental_health_consequence_Unitedstate.groupby(['family_history']).count().reset_index()\nmental_health_consequence_Unitedstate"
 },
 {
  "nbid": "NB_3471038",
  "file": "NB_3471038,raw_df,X-1.ipynb",
  "row_id": 2,
  "target_code": "from sklearn.utils import shuffle\nraw_df = shuffle(raw_df)\nraw_df.head()",
  "generation": "shuffled_df = shuffle(raw_df)\nshuffled_df.reset_index(inplace=True, drop=True)\nshuffled_df.head()"
 },
 {
  "nbid": "NB_3473128",
  "file": "NB_3473128,df,X_,y,y_-1.ipynb",
  "row_id": 7,
  "target_code": "df=df.drop([\"PassengerId\"             ,\"Name\"      , \"Cabin\"        ,\"Ticket\"        ],axis =1)\ndf",
  "generation": "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\ndf = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\ndf = df.dropna()\ndf = pd.concat([df, pd.get_dummies(df['Embarked'])], axis=1)\ndf = df.drop(['Embarked'], axis=1)"
 },
 {
  "nbid": "NB_3481758",
  "file": "NB_3481758,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.rename(columns = {'city'      : 'city'       , 'area'       : 'area'       , 'rooms'        : 'rooms'        ,'bathroom'          : 'bathroom'           ,\n                          'parking spaces'                                          : 'parking_spaces'                 , 'floor'        : 'floor'        , 'animal'         : 'animal'         ,\n                          'furniture'                                     : 'furniture'            , 'hoa (R$)'           : 'hoa'      , 'rent amount (R$)'                   : 'rent_amount'              ,\n                          'property tax (R$)'                                             : 'property_tax'               , 'fire insurance (R$)'                      : 'fire_insurance'                 ,\n                          'total (R$)'                                      : 'total'        })\ndf.head(5)",
  "generation": "df = df.rename(columns={'parking spaces': 'parking_spaces', 'hoa (R$)': 'hoa', 'rent amount (R$)': 'rent_amount', 'property tax (R$)': 'property_tax', 'fire insurance (R$)': 'fire_insurance', 'total (R$)': 'total'})\ndf.head()"
 },
 {
  "nbid": "NB_3483895",
  "file": "NB_3483895,reviews-1.ipynb",
  "row_id": 5,
  "target_code": "reviews = reviews.iloc[:,1:]\nreviews.head()",
  "generation": "reviews = reviews.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\nreviews.head()"
 },
 {
  "nbid": "NB_3483895",
  "file": "NB_3483895,reviews-1.ipynb",
  "row_id": 19,
  "target_code": "reviews = reviews.iloc[:, 1:]\nreviews",
  "generation": "reviews['title'] = reviews.apply(lambda row: row['title'] if row['title'] is not None else row['url'].split(\"/\")[-1], axis=1)\nreviews = reviews[['title', 'url', 'platform', 'score', 'genre', 'editors_choice', 'release_year', 'release_month', 'release_day']]\nreviews.head()"
 },
 {
  "nbid": "NB_3492057",
  "file": "NB_3492057,df_new-1.ipynb",
  "row_id": 86,
  "target_code": "df_new[['CA'    ,'UK'    ,'US'    ]] = pd.get_dummies(df_new['country'         ]) \ndf_new.drop(['US'    ],axis=1, inplace=True)\ndf_new.head()",
  "generation": "df_new[['CA', 'UK']] = pd.get_dummies(df_new['country'])[['CA','UK']]\ndf_new.drop('US', axis=1, inplace=True)\ndf_new.head()"
 },
 {
  "nbid": "NB_3504290",
  "file": "NB_3504290,df2-1.ipynb",
  "row_id": 75,
  "target_code": "df2['intercept'           ] = 1\ndf2['ab_page'         ] = pd.get_dummies(df2.group).treatment\ndf2.head(5)",
  "generation": "df2['intercept'] = 1\ndf2['ab_page'] = pd.get_dummies(df2['group'])['treatment']\ndf2.head()"
 },
 {
  "nbid": "NB_3504290",
  "file": "NB_3504290,df4-1.ipynb",
  "row_id": 89,
  "target_code": "df4 = df4.join(pd.get_dummies(df4['country'         ]))\ndf4.head(5)",
  "generation": "df4 = pd.get_dummies(df4, columns=['country'])\ndf4.head()"
 },
 {
  "nbid": "NB_3515299",
  "file": "NB_3515299,leaf_data-1.ipynb",
  "row_id": 24,
  "target_code": "leaf_data['eccentricity'              ] = leaf_data['eccentricity'              ].apply(lambda x: (x-leaf_data['eccentricity'              ].mean())/leaf_data['eccentricity'              ].std())\nleaf_data.head()  ",
  "generation": "leaf_data['eccentricity'] = (leaf_data['eccentricity'] - leaf_data['eccentricity'].mean()) / leaf_data['eccentricity'].std()\nleaf_data.head()"
 },
 {
  "nbid": "NB_3520299",
  "file": "NB_3520299,df-1.ipynb",
  "row_id": 3,
  "target_code": "df = df.dropna()\ndf.head()",
  "generation": "df = df.drop(df.index[0])\ndf.head()"
 },
 {
  "nbid": "NB_3529590",
  "file": "NB_3529590,df-1.ipynb",
  "row_id": 25,
  "target_code": "df['highway-mpg'             ] = 235/df['highway-mpg'             ]\ndf.rename(columns = {'highway-mpg'              : 'highway-L/100'                }, inplace = True)\ndf.head()",
  "generation": "df['highway-L/100'] = 235/df['highway-mpg']\ndf.head()"
 },
 {
  "nbid": "NB_3537024",
  "file": "NB_3537024,star_wars-1.ipynb",
  "row_id": 4,
  "target_code": "star_wars = star_wars.rename(columns={\n        \"Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.\"                                                                                                                                                       : \"ranking_1\"            ,\n        \"Unnamed: 10\"                     : \"ranking_2\"            ,\n        \"Unnamed: 11\"                     : \"ranking_3\"            ,\n        \"Unnamed: 12\"                     : \"ranking_4\"            ,\n        \"Unnamed: 13\"                     : \"ranking_5\"            ,\n        \"Unnamed: 14\"                     : \"ranking_6\"            \n        })\nstar_wars.head()",
  "generation": "star_wars.rename(columns={\"Which of the following Star Wars films have you seen? Please select all that apply.\": \"seen_1\",\n        \"Unnamed: 4\": \"seen_2\",\n        \"Unnamed: 5\": \"seen_3\",\n        \"Unnamed: 6\": \"seen_4\",\n        \"Unnamed: 7\": \"seen_5\",\n        \"Unnamed: 8\": \"seen_6\"}, inplace=True)\n\nranking_cols = star_wars.columns[9:15]\nstar_wars[raking_cols] = star_wars[raking_cols].astype(float)\n\nstar_wars.rename(columns={ranking_cols[0]: 'ranking_1',\n                          ranking_cols[1]: 'ranking_2',\n                          ranking_cols[2]: 'ranking_3',\n                          ranking_cols[3]: 'ranking_4',\n                          ranking_cols[4]: 'ranking_5',\n                          ranking_cols[5]: 'ranking_6'}, inplace=True)\n\nstar_wars.head()"
 },
 {
  "nbid": "NB_3537207",
  "file": "NB_3537207,df-1.ipynb",
  "row_id": 4,
  "target_code": "df['Date'      ] = pd.to_datetime(df['Date'      ], format='%d-%b-%y'          , errors='coerce'        )\ndf.head(3)",
  "generation": "df['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%y')\ndf.head()"
 },
 {
  "nbid": "NB_3540602",
  "file": "NB_3540602,img_data-1.ipynb",
  "row_id": 8,
  "target_code": "img_data['emotion'         ]=img_data.emotion.apply(lambda em:emotions_dict[em])\nimg_data.head()",
  "generation": "img_data['emotion'] = img_data['em_id'].map(emotions_dict)\nimg_data.head()"
 },
 {
  "nbid": "NB_3540694",
  "file": "NB_3540694,df-1.ipynb",
  "row_id": 8,
  "target_code": "df['dept'      ]=df['dept'      ].map({'customer_service'                  :0 ,'marketing'           :1, 'data_science'               :2, 'engineer'           :3, 'sales'        :4, 'design'         :5}).astype(int)\ndf.head()",
  "generation": "dept_dict = {'customer_service': 0, 'marketing': 1, 'data_science': 2, 'engineer': 3, 'sales': 4, 'design': 5}\ndf['dept'] = df['dept'].map(dept_dict)\ndf.head()"
 },
 {
  "nbid": "NB_3545299",
  "file": "NB_3545299,titanic_train-1.ipynb",
  "row_id": 45,
  "target_code": "titanic_train = titanic_train.drop('Ticket'        , axis=1)\ntitanic_train.head()",
  "generation": "titanic_train = titanic_train.drop(['Name', 'PassengerId', 'Ticket'], axis=1)\ntitanic_train.head()"
 },
 {
  "nbid": "NB_3545299",
  "file": "NB_3545299,titanic_train-1.ipynb",
  "row_id": 48,
  "target_code": "titanic_train['Cabin'       ] = titanic_train['Cabin'       ].str[0]\ntitanic_train.head()",
  "generation": "# Drop 'Name' and 'PassengerId' columns\ntitanic_train = titanic_train.drop(['Name', 'PassengerId'], axis=1)\n\n# Drop 'Ticket' column\ntitanic_train = titanic_train.drop('Ticket', axis=1)\n\n# Extract first letter of 'Cabin' value to get deck letter\ntitanic_train['Cabin'] = titanic_train['Cabin'].str[0]\n\n# Impute 'Age' column with mean value\ntitanic_train['Age'] = titanic_train['Age'].fillna(titanic_train['Age'].mean())\n\n# Impute 'Embarked' column with most common value\ntitanic_train['Embarked'] = titanic_train['Embarked'].fillna(titanic_train['Embarked'].value_counts().idxmax())\n\n# Convert 'Pclass' column to ordered categorical variable and then to numerical codes\nclass_categories = CategoricalDtype(categories=[\"First\", \"Second\", \"Third\"], ordered=True)\ntitanic_train[\"Pclass\"] = titanic_train[\"Pclass\"].astype(class_categories)\ntitanic_train[\"Pclass\"] = titanic_train[\"Pclass\"].cat.codes\n\n# Convert 'Sex' column to numerical codes\ntitanic_train['Sex'] = titanic_train['Sex'].replace({'male': 0, 'female': 1})\n\n# Convert 'Embarked' column to numerical codes\ntitanic_train['Embarked'] = titanic_train['Embarked'].replace({'S': 0, 'C': 1, 'Q': 2})\n\n# Fill missing values in 'Cabin' column with 'U' for 'unassigned'\ntitanic_train['Cabin'] = titanic_train['Cabin'].fillna('U')\n\n# Convert 'Cabin' column to numerical codes\ntitanic_train['Cabin'] = pd.Categorical(titanic_train['Cabin']).codes\n\n# Check for missing values\ntitanic_train.isna().sum()\n\n# Output the cleaned dataframe\ntitanic_train.head()"
 },
 {
  "nbid": "NB_3545848",
  "file": "NB_3545848,Skewness-1.ipynb",
  "row_id": 58,
  "target_code": "Skewness2 = pd.DataFrame(skewed_features2,columns=['Features'          ])\nSkewness = pd.concat([Skewness,Skewness2],axis=1)\nSkewness",
  "generation": "skewed_features2 = pd.DataFrame({'Features': Skewness.index, 'Skewness': Skewness['Skewness'].values})\nskewed_features2.head()"
 },
 {
  "nbid": "NB_3547874",
  "file": "NB_3547874,df-1.ipynb",
  "row_id": 76,
  "target_code": "df['highway-L/100km'                 ] = 235/df['highway-mpg'             ]\ndf.head()",
  "generation": "df['highway-L/100km'] = 235/df[\"highway-mpg\"]\ndf.head()"
 },
 {
  "nbid": "NB_3548457",
  "file": "NB_3548457,train_data-1.ipynb",
  "row_id": 5,
  "target_code": "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfor col in train_data.columns[train_data.dtypes=='object'        ]:\n    le = LabelEncoder()\n    le.fit(train_data[col])\n    train_data[col] = le.transform(train_data[col])\n    test_data[col] = le.transform(test_data[col])\ntrain_data.head()",
  "generation": "# Label encoding for categorical variables\nle = LabelEncoder()\ntrain_data['job'] = le.fit_transform(train_data['job'])\ntrain_data['marital'] = le.fit_transform(train_data['marital'])\ntrain_data['education'] = le.fit_transform(train_data['education'])\ntrain_data['default'] = le.fit_transform(train_data['default'])\ntrain_data['housing'] = le.fit_transform(train_data['housing'])\ntrain_data['loan'] = le.fit_transform(train_data['loan'])\ntrain_data['contact'] = le.fit_transform(train_data['contact'])\ntrain_data['poutcome'] = le.fit_transform(train_data['poutcome'])\n\ntest_data['job'] = le.fit_transform(test_data['job'])\ntest_data['marital'] = le.fit_transform(test_data['marital'])\ntest_data['education'] = le.fit_transform(test_data['education'])\ntest_data['default'] = le.fit_transform(test_data['default'])\ntest_data['housing'] = le.fit_transform(test_data['housing'])\ntest_data['loan'] = le.fit_transform(test_data['loan'])\ntest_data['contact'] = le.fit_transform(test_data['contact'])\ntest_data['poutcome'] = le.fit_transform(test_data['poutcome'])\n\ntrain_data.head()"
 },
 {
  "nbid": "NB_3548457",
  "file": "NB_3548457,train_data-1.ipynb",
  "row_id": 7,
  "target_code": "map_dict = {'poutcome'          :['mean'      ,'min'     ]}\ntrain_groupby = train_data.groupby(['month'       ,'day'     ]).agg(map_dict)\ntest_groupby = test_data.groupby(['month'       ,'day'     ]).agg(map_dict)\ntrain_data = pd.merge(train_groupby, train_data, on=['month'       ,'day'     ])\ntest_data = pd.merge(test_groupby, test_data, on=['month'       ,'day'     ])\ntrain_data.head()",
  "generation": "train_data['month'] = train_data['month'].apply(lambda x: int(x))\ntest_data['month'] = test_data['month'].apply(lambda x: int(x))\n\nfor col in train_data.columns[train_data.dtypes=='object']:\n    le = LabelEncoder()\n    le.fit(train_data[col])\n    train_data[col] = le.transform(train_data[col])\n    test_data[col] = le.transform(test_data[col])\n\ntrain_data['day'] = train_data['day'].apply(lambda x: int(x))\ntest_data['day'] = test_data['day'].apply(lambda x: int(x))\n\ntrain_data = pd.concat([train_data, pd.get_dummies(train_data['poutcome'], prefix='poutcome')], axis=1)\ntest_data = pd.concat([test_data, pd.get_dummies(test_data['poutcome'], prefix='poutcome')], axis=1)\n\ntrain_data.drop(['poutcome'], axis=1, inplace=True)\ntest_data.drop(['poutcome'], axis=1, inplace=True)\n\nscaler = StandardScaler()\ntrain_data[['balance', 'duration']] = scaler.fit_transform(train_data[['balance', 'duration']])\ntest_data[['balance', 'duration']] = scaler.transform(test_data[['balance', 'duration']])\n\ntrain_data.head()"
 },
 {
  "nbid": "NB_3551139",
  "file": "NB_3551139,df,df3-1.ipynb",
  "row_id": 11,
  "target_code": "from sklearn.preprocessing import LabelEncoder\ndf['jenis'       ] = LabelEncoder().fit_transform(df['jenis'       ])\ndf['genre'       ] = LabelEncoder().fit_transform(df['genre'       ])\ndf.head()",
  "generation": "df = pd.read_csv('../data/apa.csv')\ndf.head()\n\ndf['movie'] = LabelEncoder().fit_transform(df['movie'])\ndf.head()"
 },
 {
  "nbid": "NB_715719",
  "file": "NB_715719,y_train_pred_final-1.ipynb",
  "row_id": 245,
  "target_code": "y_train_pred_final['Predicted_Hot_Lead'                    ] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()",
  "generation": "y_train_pred_final['Predicted_Hot_Lead'] = np.where(y_train_pred_final['Lead_Score_Prob'] > 0.5, 1, 0)\ny_train_pred_final.head()"
 },
 {
  "nbid": "NB_715719",
  "file": "NB_715719,y_train_pred_final-1.ipynb",
  "row_id": 247,
  "target_code": "y_train_pred_final['Lead_Score'            ] = round((y_train_pred_final['Lead_Score_Prob'                 ] * 100),0)\ny_train_pred_final['Lead_Score'            ] = y_train_pred_final['Lead_Score'            ].astype(int)\ny_train_pred_final.head()",
  "generation": "y_train_pred_final['Lead_Score'] = y_train_pred_final['Lead_Score_Prob'] * 100\ny_train_pred_final.head()"
 },
 {
  "nbid": "NB_748338",
  "file": "NB_748338,data,data_new-1.ipynb",
  "row_id": 6,
  "target_code": "data = data.drop(columns=['Value Footnotes'                 ])\ndata.head(5)",
  "generation": "data.drop(columns=[\"Value Footnotes\"], inplace=True)\ndata.head()"
 },
 {
  "nbid": "NB_749785",
  "file": "NB_749785,favorites_time-1.ipynb",
  "row_id": 167,
  "target_code": "favorites_time.loc[:, 'hour'       ] = favorites_time.time.apply(lambda col_value: col_value.hour)\nfavorites_time.head()",
  "generation": "favorites_time['hour'] = pd.to_datetime(favorites_time['time']).dt.hour\nfavorites_time.head()"
 },
 {
  "nbid": "NB_753166",
  "file": "NB_753166,df_grouped_Age,ax2-1.ipynb",
  "row_id": 95,
  "target_code": "df_grouped_Age = df_grouped_Age.drop(df_grouped_Age.columns.tolist()[1:6],axis=1)\ndf_grouped_Age",
  "generation": "df_grouped_Age = df_grouped_Age[['Population', 'Cases per 1M P', 'Tests per 1M P', 'Deaths per 1M P', 'Recovered per 1M P', 'Positives Rate', 'Case Fatality Rate', 'Real Case Fatality Rate', 'Case Recovery Rate', 'Real Case Recovery Rate', 'Active Cases Rate', 'Age_Rank']]\ndf_grouped_Age.head()"
 },
 {
  "nbid": "NB_754935",
  "file": "NB_754935,daily,y-1.ipynb",
  "row_id": 39,
  "target_code": "daily = counts.resample('d'   ).sum()\ndaily['Total'       ] = daily.sum(axis=1)\ndaily.head()",
  "generation": "daily['Total'] = daily.sum(axis=1)\ndaily.head()"
 },
 {
  "nbid": "NB_754935",
  "file": "NB_754935,daily,y-1.ipynb",
  "row_id": 40,
  "target_code": "daily  = daily[['Total'       ]] \ndaily.head()",
  "generation": "daily = daily[['Total', 'Date']]\ndaily.head()"
 },
 {
  "nbid": "NB_754935",
  "file": "NB_754935,daily,y-1.ipynb",
  "row_id": 48,
  "target_code": "daily['dailight_hrs'              ] = [hours_of_daylight(x) for x in daily.index]\ndaily.head()",
  "generation": "daily['dailight_hrs'] = (weather['TMAX'] - weather['TMIN'])/2\ndaily.head()"
 },
 {
  "nbid": "NB_762402",
  "file": "NB_762402,concat_data_DAX-1.ipynb",
  "row_id": 23,
  "target_code": "concat_data_DAX = concat_data_DAX[['Date'      , 'Open'       , 'High'       , 'Low'      , 'Close'        , 'Adj Close'            , 'Volume'         ]]\nconcat_data_DAX",
  "generation": "concat_data_DAX = concat_data_DAX[['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]\nconcat_data_DAX.head(5)"
 },
 {
  "nbid": "NB_774701",
  "file": "NB_774701,df-1.ipynb",
  "row_id": 3,
  "target_code": "df = df.drop('Unnamed: 0'            ,axis=1)\ndf.head()",
  "generation": "df = df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\ndf = df[['BOROUGH', 'NEIGHBORHOOD', 'BUILDING CLASS CATEGORY', 'BLOCK', 'LOT', 'BUILDING CLASS AT PRESENT', 'ADDRESS', 'ZIP CODE', 'RESIDENTIAL UNITS', 'COMMERCIAL UNITS', 'TOTAL UNITS', 'LAND SQUARE FEET', 'GROSS SQUARE FEET', 'YEAR BUILT', 'SALE PRICE', 'SALE DATE']]\ndf.head()"
 },
 {
  "nbid": "NB_781961",
  "file": "NB_781961,apps,paidapps-1.ipynb",
  "row_id": 13,
  "target_code": "apps['size_bytes'            ] = apps['size_bytes'            ] / 1000000\napps.rename(columns={'size_bytes'            :'size_mb'         }, inplace=True)\napps.head(5)",
  "generation": "apps['size_mb'] = apps['size_bytes'] / (1024 * 1024)\napps = apps.drop(columns=['size_bytes'])\napps"
 },
 {
  "nbid": "NB_794754",
  "file": "NB_794754,df_t-1.ipynb",
  "row_id": 30,
  "target_code": "for i in colsTo0:\n    df_t[i] = df_t[i].replace(np.nan, 0)\ndf_t.head()",
  "generation": "df_t['NU_NOTA_LC'           ] = df_t['NU_NOTA_LC'           ].fillna(0)\ndf_t['NU_NOTA_CN'           ] = df_t['NU_NOTA_CN'           ].fillna(0)\ndf_t['NU_NOTA_CH'           ] = df_t['NU_NOTA_CH'           ].fillna(0)\ndf_t['NU_NOTA_REDACAO'      ] = df_t['NU_NOTA_REDACAO'      ].fillna(0)\ndf_t['TP_STATUS_REDACAO'    ] = df_t['TP_STATUS_REDACAO'    ].fillna(0)\ndf_t['NU_NOTA_COMP1'        ] = df_t['NU_NOTA_COMP1'        ].fillna(0)\ndf_t['NU_NOTA_COMP2'        ] = df_t['NU_NOTA_COMP2'        ].fillna(0)\ndf_t['NU_NOTA_COMP3'        ] = df_t['NU_NOTA_COMP3'        ].fillna(0)\ndf_t['NU_NOTA_COMP4'        ] = df_t['NU_NOTA_COMP4'        ].fillna(0)\ndf_t['NU_NOTA_COMP5'        ] = df_t['NU_NOTA_COMP5'        ].fillna(0)\ndf_t['TP_PRESENCA_CN'       ] = df_t['TP_PRESENCA_CN'       ].fillna(0)\ndf_t['TP_PRESENCA_CH'       ] = df_t['TP_PRESENCA_CH'       ].fillna(0)\ndf_t['TP_PRESENCA_LC'       ] = df_t['TP_PRESENCA_LC'       ].fillna(0)\ndf_t['TP_PRESENCA_MT'       ] = df_t['TP_PRESENCA_MT'       ].fillna(0)\ndf_t['TP_LINGUA'            ] = df_t['TP_LINGUA'            ].fillna(0)\ndf_t['TP_STATUS_REDACAO'    ] = df_t['TP_STATUS_REDACAO'    ].fillna(0)\ndf_t['Q027'                 ] = df_t['Q027'                 ].fillna('A')\ndf_t.head()"
 },
 {
  "nbid": "NB_794754",
  "file": "NB_794754,df_t-1.ipynb",
  "row_id": 31,
  "target_code": "df_t['TP_ENSINO'           ] = df_t['TP_ENSINO'           ].replace(np.nan, -1)\ndf_t['TP_DEPENDENCIA_ADM_ESC'                        ] = df_t['TP_DEPENDENCIA_ADM_ESC'                        ].replace(np.nan, -1)\ndf_t['Q027'      ] = df_t['Q027'      ].replace(np.nan, -1)\ndf_t.head()",
  "generation": "for i in colsTo0:\n    df_test[i] = df_test[i].fillna(-1)"
 },
 {
  "nbid": "NB_794754",
  "file": "NB_794754,df_t-1.ipynb",
  "row_id": 33,
  "target_code": "df_t = pd.concat([df_t, pd.get_dummies(df_t['TP_SEXO'         ])], axis=1)\ndf_t = df_t.drop('TP_SEXO'         , axis=1)\ndf_t.head()",
  "generation": "df_test['TP_SEXO'] = df_test['TP_SEXO'].map({'M': 1, 'F': 0})\ndf_test['TP_COR_RACA'] = df_test['TP_COR_RACA'].map({0: 1, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0})\ndf_test['TP_NACIONALIDADE'] = df_test['TP_NACIONALIDADE'].map({0: 1, 1: 0, 2: 0, 3: 0, 4: 0})\ndf_test['TP_ST_CONCLUSAO'] = df_test['TP_ST_CONCLUSAO'].map({0: 1, 1: 0, 2: 0, 3: 0, 4: 0})\ndf_test['TP_ESCOLA'] = df_test['TP_ESCOLA'].map({0: 1, 1: 0, 2: 0, 3: 0})\ndf_test['IN_BAIXA_VISAO'] = df_test['IN_BAIXA_VISAO'].map({0: 1, 1: 0})\ndf_test['IN_CEGUEIRA'] = df_test['IN_CEGUEIRA'].map({0: 1, 1: 0})\ndf_test['IN_SURDEZ'] = df_test['IN_SURDEZ'].map({0: 1, 1: 0})\ndf_test['IN_DISLEXIA'] = df_test['IN_DISLEXIA'].map({0: 1, 1: 0})\ndf_test['IN_DISCALCULIA'] = df_test['IN_DISCALCULIA'].map({0: 1, 1: 0})\ndf_test['IN_SABATISTA'] = df_test['IN_SABATISTA'].map({0: 1, 1: 0})\ndf_test['IN_GESTANTE'] = df_test['IN_GESTANTE'].map({0: 1, 1: 0})\ndf_test['IN_IDOSO'] = df_test['IN_IDOSO'].map({0: 1, 1: 0})\ndf_test['TP_PRESENCA_CN'] = df_test['TP_PRESENCA_CN'].map({0: 1, 1: 0, 2: 0})\ndf_test['TP_PRESENCA_CH'] = df_test['TP_PRESENCA_CH'].map({0: 1, 1: 0, 2: 0})\ndf_test['TP_PRESENCA_LC'] = df_test['TP_PRESENCA_LC'].map({0: 1, 1: 0, 2: 0})\ndf_test['TP_PRESENCA_MT'] = df_test['TP_PRESENCA_MT'].map({0: 1, 1: 0, 2: 0})\ndf_test['Q001'] = df_test['Q001'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7})\ndf_test['Q002'] = df_test['Q002'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7})\ndf_test['Q006'] = df_test['Q006'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16})\ndf_test['Q024'] = df_test['Q024'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4})\ndf_test['Q025'] = df_test['Q025'].map({'A': 0, 'B': 1})\ndf_test['Q026'] = df_test['Q026'].map({'A': 0, 'B': 1})\ndf_test['Q027'] = df_test['Q027'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16}) \ndf_test['Q047'] = df_test['Q047'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}) \ndf_test['TP_DEPENDENCIA_ADM_ESC'] = df_test['TP_DEPENDENCIA_ADM_ESC'].replace(-1, np.nan)\ndf_test['TP_ENSINO'] = df_test['TP_ENSINO'].replace(-1, np.nan)\ndf_test['Q027'] = df_test['Q027'].replace(-1, np.nan)\ndf_test = df_test.fillna(df_test.mean())\ndf_test['TP_DEPENDENCIA_ADM_ESC'] = df_test['TP_DEPENDENCIA_ADM_ESC'].astype(int)\ndf_test['TP_ENSINO'] = df_test['TP_ENSINO'].astype(int)\ndf_test['Q027'] = df_test['Q027'].astype(int)\ndf_test['F'] = (df_test['TP_SEXO'] == 0).astype(int)\ndf_test['M'] = (df_test['TP_SEXO'] == 1).astype(int)\ndf_test = df_test.drop(['TP_SEXO'], axis=1)\ndf_test.head()"
 },
 {
  "nbid": "NB_794754",
  "file": "NB_794754,df_t-1.ipynb",
  "row_id": 34,
  "target_code": "qColsFact = ['Q001'      , 'Q002'       , 'Q006'       , 'Q024'       , 'Q025'       , 'Q026'       , 'Q027'       , 'Q047'       ]\nfor i in qColsFact:\n    df_t[i], uniques = pd.factorize(df_t[i], sort=True)\ndf_t.head()",
  "generation": "import numpy as np\n\nfor i in colsTo0:\n    df_test[i] = df_test[i].replace(np.nan, 0)\n\ndf_test['TP_ENSINO'] = df_test['TP_ENSINO'].replace(np.nan, -1)\ndf_test['TP_DEPENDENCIA_ADM_ESC'] = df_test['TP_DEPENDENCIA_ADM_ESC'].replace(np.nan, -1)\ndf_test['Q027'] = df_test['Q027'].replace(np.nan, -1)\n\ndf_test = pd.concat([df_test, pd.get_dummies(df_test['TP_SEXO'])], axis=1)\ndf_test = df_test.drop('TP_SEXO', axis=1)"
 },
 {
  "nbid": "NB_794754",
  "file": "NB_794754,df_t-1.ipynb",
  "row_id": 35,
  "target_code": "answer = df_t['NU_INSCRICAO'              ]  \ndf_t = df_t.drop('NU_INSCRICAO'              , axis=1)\ndf_t = df_t.drop('SG_UF_RESIDENCIA'                  , axis=1)\ndf_t = df_t.drop('IN_CEGUEIRA'             , axis=1)\nprint(answer.head())\ndf_t.head()",
  "generation": "import numpy as np\ndf_test[colsTo0] = df_test[colsTo0].fillna(0)\ndf_test['TP_ENSINO'] = df_test['TP_ENSINO'].fillna(-1)\ndf_test['TP_DEPENDENCIA_ADM_ESC'] = df_test['TP_DEPENDENCIA_ADM_ESC'].fillna(-1)\ndf_test['Q027'] = df_test['Q027'].fillna(-1)\ndf_test = pd.concat([df_test, pd.get_dummies(df_test['TP_SEXO'])], axis=1)\ndf_test = df_test.drop('TP_SEXO', axis=1)\nfor i in qColsFact:\n    df_test[i], uniques = pd.factorize(df_test[i], sort=True)\ndf_test.head()"
 },
 {
  "nbid": "NB_799515",
  "file": "NB_799515,df-1.ipynb",
  "row_id": 13,
  "target_code": "df = df.dropna(thresh=18)\ndf.tail(2)",
  "generation": "df.id_nb = df.id_nb.apply(lambda x: int(x.replace('#', '')))\ndf.set_index('id_nb', inplace=True)\ndf.data_species = df.data_species.apply(lambda x: x.replace('Pok\u00e9mon', '').replace(' ', ''))\ndf = df.dropna(thresh=18)"
 },
 {
  "nbid": "NB_811001",
  "file": "NB_811001,pcs-1.ipynb",
  "row_id": 51,
  "target_code": "pcs = pcs.dropna()\npcs.head(5)",
  "generation": "pcs = cps.dropna().pct_change()\npcs.head()"
 },
 {
  "nbid": "NB_2925748",
  "file": "NB_2925748,lego_df-1.ipynb",
  "row_id": 21,
  "target_code": "columns=['theme'       , 'subtheme'           , 'year'       , 'name'       , 'minifigs'           , 'pieces'         , 'us_price'           , 'owner'        , 'want_it'          ]\nlego_df=lego_df[columns]\nlego_df.head()",
  "generation": "lego_df = lego_df[['theme', 'subtheme', 'year', 'name', 'minifigs', 'pieces', 'us_price', 'owner', 'want_it', 'number']]\nlego_df.columns = lego_df.columns.str.replace('number', 'Unnamed: 0_level_0')\nlego_df.columns = lego_df.columns.str.replace('Unnamed: 0_level_0', 'number')\nlego_df.columns = lego_df.columns.str.replace('theme', 'Unnamed: 1_level_1')\nlego_df.columns = lego_df.columns.str.replace('subtheme', 'Unnamed: 2_level_1')\nlego_df.columns = lego_df.columns.str.replace('year', 'Unnamed: 3_level_1')\nlego_df.columns = lego_df.columns.str.replace('name', 'Unnamed: 4_level_1')\nlego_df.columns = lego_df.columns.str.replace('minifigs', 'Unnamed: 5_level_1')\nlego_df.columns = lego_df.columns.str.replace('pieces', 'Unnamed: 6_level_1')\nlego_df.columns = lego_df.columns.str.replace('us_price', 'Unnamed: 7_level_1')\nlego_df.columns = lego_df.columns.str.replace('owner', 'Unnamed: 8_level_1')\nlego_df.columns = lego_df.columns.str.replace('want_it', 'Unnamed: 9_level_1')\nlego_df.head()"
 },
 {
  "nbid": "NB_2926738",
  "file": "NB_2926738,df-1.ipynb",
  "row_id": 15,
  "target_code": "df = df.drop(['Number'        , 'Player Id'            , 'Position'           , 'High School Location'                       ], axis=1)\ndf.head()",
  "generation": "df.drop(['Number', 'Player Id', 'Position', 'High School Location'], axis=1, inplace=True)\ndf.rename(columns={'Height (inches)': 'Height', 'Weight (lbs)': 'Weight'}, inplace=True)\ndf['Experience'] = df['Experience'].str.extract('(\\d+)').astype(int)\ndf.head()"
 },
 {
  "nbid": "NB_2927280",
  "file": "NB_2927280,df-1.ipynb",
  "row_id": 7,
  "target_code": "df = df.set_index('Year'      )\ndf",
  "generation": "df = df.set_index('Year')\ndf.head(5)"
 },
 {
  "nbid": "NB_2928866",
  "file": "NB_2928866,alldata-1.ipynb",
  "row_id": 20,
  "target_code": "alldata = alldata.reset_index().set_index('Google_week'             )  \nalldata.head()",
  "generation": "alldata.set_index('Google_week', inplace=True)\nalldata.head()"
 },
 {
  "nbid": "NB_2928866",
  "file": "NB_2928866,alldata-1.ipynb",
  "row_id": 21,
  "target_code": "alldata['MA_p'      ] = alldata['Debt_paper'            ].shift(1).rolling(window = 3).mean()  \nalldata['MA_d'      ] = alldata['Debt_download'               ].shift(1).rolling(window = 3).mean()\nalldata.head()",
  "generation": "alldata['MA_p'] = alldata['Debt_paper'].rolling(window=3).mean().shift(1)\nalldata['MA_d'] = alldata['Debt_download'].rolling(window=3).mean().shift(1)\nalldata.head()"
 },
 {
  "nbid": "NB_2928866",
  "file": "NB_2928866,alldata-1.ipynb",
  "row_id": 23,
  "target_code": "alldata['signal_p'          ] = np.where(alldata['Debt_paper'            ] > alldata['MA_p'      ], -1, 1)  \nalldata['signal_d'          ] = np.where(alldata['Debt_download'               ] > alldata['MA_d'      ], -1, 1)\nalldata.head()  ",
  "generation": "alldata['signal_p'] = np.where(alldata['Debt_paper'] > alldata['MA_p'], -1, 1)\nalldata['signal_d'] = np.where(alldata['Debt_download'] > alldata['MA_d'], -1, 1)\nalldata.head()"
 },
 {
  "nbid": "NB_2928866",
  "file": "NB_2928866,alldata-1.ipynb",
  "row_id": 26,
  "target_code": "alldata['pct_change'            ] = alldata['DJClose'         ].pct_change()\nalldata.head()",
  "generation": "alldata['pct_change'] = alldata['DJClose'].pct_change()\nalldata.head()"
 },
 {
  "nbid": "NB_2928866",
  "file": "NB_2928866,alldata-1.ipynb",
  "row_id": 27,
  "target_code": "alldata['ret_p'       ] = alldata['pct_change'            ]*alldata['signal_p'          ].shift(1)  \nalldata['ret_d'       ] = alldata['pct_change'            ]*alldata['signal_d'          ].shift(1)\nalldata.head()  ",
  "generation": "alldata['ret_p'] = np.where(alldata['signal_p'] == -1, -alldata['pct_change'], alldata['pct_change']).shift(1)\nalldata['ret_d'] = np.where(alldata['signal_d'] == -1, -alldata['pct_change'], alldata['pct_change']).shift(1)\nalldata.head()"
 },
 {
  "nbid": "NB_2942264",
  "file": "NB_2942264,df3-1.ipynb",
  "row_id": 87,
  "target_code": "df3[['CA'    ,'UK'    ,'US'    ]] = pd.get_dummies(df3['country'         ])\ndf3.head()",
  "generation": "df2 = df.copy()\ndf2['ab_page'] = pd.get_dummies(df['group'])['treatment']\ncountries = pd.read_csv('countries.csv')\ndf3 = df2.join(pd.get_dummies(df2['country']))\ndf3 = df3.drop(['country'], axis=1)\ndf3.head()"
 },
 {
  "nbid": "NB_2946189",
  "file": "NB_2946189,star_wars,females-1.ipynb",
  "row_id": 5,
  "target_code": "yes_no = {\n    \"Yes\"         : True,\n    \"No\"        : False\n}\nstar_wars['Have you seen any of the 6 films in the Star Wars franchise?'                                                              ] = star_wars['Have you seen any of the 6 films in the Star Wars franchise?'                                                              ].map(yes_no)\nstar_wars['Do you consider yourself to be a fan of the Star Wars film franchise?'                                                                       ] = star_wars['Do you consider yourself to be a fan of the Star Wars film franchise?'                                                                       ].map(yes_no)\nstar_wars.head()",
  "generation": "yes_no = {\"Yes\": True, \"No\": False}\nstar_wars.replace(yes_no, inplace=True)\nstar_wars.head()"
 },
 {
  "nbid": "NB_2947906",
  "file": "NB_2947906,datamart-1.ipynb",
  "row_id": 9,
  "target_code": "labels_recency = range(4,0,-1)\nlabels_frequency = range(1,5)\nlabels_monetary = range(1,5)\nr_deciles = pd.qcut(datamart['Recency'         ], 4, labels = labels_recency)\nf_deciles = pd.qcut(datamart['Frequency'           ], 4, labels = labels_frequency)\nm_deciles = pd.qcut(datamart['MonetaryValue'               ], 4, labels = labels_monetary)\ndatamart = datamart.assign(R = r_deciles.values)\ndatamart = datamart.assign(F = f_deciles.values)\ndatamart = datamart.assign(M = m_deciles.values)\ndatamart.head(10)",
  "generation": "# Calculate quartiles\ndatamart['R'] = pd.qcut(datamart['Recency'], 4, labels = [4, 3, 2, 1])\ndatamart['F'] = pd.qcut(datamart['Frequency'].rank(method=\"first\"), 4, labels = [1, 2, 3, 4])\ndatamart['M'] = pd.qcut(datamart['MonetaryValue'], 4, labels = [1, 2, 3, 4])\n\n# Concatenate RFM quartile values\ndatamart['RFM_Segment'] = datamart.R.astype(str) + datamart.F.astype(str) + datamart.M.astype(str)\n\n# Calculate RFM score\ndatamart['RFM_Score'] = datamart[['R', 'F', 'M']].sum(axis=1)\n\n# Show data\ndatamart.head()"
 },
 {
  "nbid": "NB_2948517",
  "file": "NB_2948517,londonGroupedProportions-1.ipynb",
  "row_id": 64,
  "target_code": "sumDataframe = londonGroupedProportions.sum(axis=1)\nfor column in list(londonGroupedProportions.columns.values):\n    londonGroupedProportions[column]=londonGroupedProportions[column]/sumDataframe\nlondonGroupedProportions.head()",
  "generation": "londonGroupedProportions = londonGrouped.drop('Name', axis=1).div(londonGrouped.drop('Name', axis=1).sum(axis=1), axis=0)\nlondonGroupedProportions.insert(0, 'Name', londonGrouped['Name'])\nlondonGroupedProportions.head()"
 },
 {
  "nbid": "NB_2952252",
  "file": "NB_2952252,df-1.ipynb",
  "row_id": 9,
  "target_code": "amino_acids = [x for x in list(df.columns.values) if x not in [\"site\"      , \"RSA\"      , \"wt\"     , \"entropy\"          ]]\ndf[\"entropy\"         ] = df[amino_acids].apply(lambda r: tuple(r), axis=1).apply(scipy.array)\ndf[\"entropy\"         ] = df[\"entropy\"         ].apply(lambda x: scipy.log(entropy(x)))\ndf.head()",
  "generation": "df = pd.merge(RSA, prefs, on='site', how='inner')\ndf['entropy'] = df.apply(lambda row: entropy(row[['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']].values), axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_2952303",
  "file": "NB_2952303,df-1.ipynb",
  "row_id": 2,
  "target_code": "df = df.pivot(index='nama'      , columns='mobil'       )\ndf",
  "generation": "df = df.pivot(index='nama', columns='mobil', values='jumlah')\ndf.head(3)"
 },
 {
  "nbid": "NB_2956376",
  "file": "NB_2956376,aapl-1.ipynb",
  "row_id": 3,
  "target_code": "aapl = aapl.sample(frac=1)\naapl.head()",
  "generation": "aapl['Date'] = pd.to_datetime(aapl['Date'])\naapl = aapl.sort_values(by='Date', ascending=False)\naapl = aapl.reset_index(drop=True)\naapl.head()"
 },
 {
  "nbid": "NB_2956376",
  "file": "NB_2956376,aapl-1.ipynb",
  "row_id": 5,
  "target_code": "aapl = aapl.sample(frac=1)\naapl.reset_index(inplace=True, drop=True)\naapl.head()",
  "generation": "aapl['Year'] = aapl['Date'].apply(extract_year)\naapl['Month'] = aapl['Date'].apply(extract_month)\naapl = aapl.sample(frac=1).reset_index(drop=True)\naapl.head()"
 },
 {
  "nbid": "NB_2957897",
  "file": "NB_2957897,df-1.ipynb",
  "row_id": 2,
  "target_code": "df = df.drop([\"pdays\"       ,\"previous\"          ,\"poutcome\"          ,\"default\"         ,\"month\"       ,\"day\"     ,\"contact\"         ],axis=1)\ndf.head()",
  "generation": "df = df.drop(['default', 'contact', 'day', 'month', 'pdays', 'previous', 'poutcome'], axis=1)\nle = LabelEncoder()\ndf['housing'] = le.fit_transform(df['housing'])\ndf['loan'] = le.fit_transform(df['loan'])\ndf['y'] = le.fit_transform(df['y'])\ndf.head()"
 },
 {
  "nbid": "NB_2980910",
  "file": "NB_2980910,star_wars,females-1.ipynb",
  "row_id": 12,
  "target_code": "star_wars = star_wars.rename(columns={\n        \"Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.\"                                                                                                                                                       : \"ranking_1\"            ,\n        \"Unnamed: 10\"                     : \"ranking_2\"            ,\n        \"Unnamed: 11\"                     : \"ranking_3\"            ,\n        \"Unnamed: 12\"                     : \"ranking_4\"            ,\n        \"Unnamed: 13\"                     : \"ranking_5\"            ,\n        \"Unnamed: 14\"                     : \"ranking_6\"            \n        })\nstar_wars.head()",
  "generation": "star_wars = star_wars.rename(columns={\n        \"Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.\": \"ranking_1\",\n        \"Unnamed: 28\": \"expanded_universe\",\n        \"Do you consider yourself to be a fan of the Star Trek franchise?\": \"star_trek_fan\"\n        })"
 },
 {
  "nbid": "NB_2981830",
  "file": "NB_2981830,data-1.ipynb",
  "row_id": 4,
  "target_code": "data = data.iloc[:,1:]\ndata.head()",
  "generation": "data = data.drop(['Id'], axis=1)\ndata.head()"
 },
 {
  "nbid": "NB_2993668",
  "file": "NB_2993668,titanic,not_survived,index,df,ax-1.ipynb",
  "row_id": 17,
  "target_code": "titanic['IdadeDisc'           ] = pd.cut(titanic['Idade'       ], bins = [0,15, 25, 60, 80], \n                              labels=['Crian\u00e7a'         , 'Jovem'        , 'Adulto'         , 'Idoso'        ])\ntitanic.head()",
  "generation": "titanic['IdadeDisc'] = pd.cut(titanic['Idade'], bins=[0, 15, 25, 60, 100], labels=['Crian\u00e7a', 'Jovem', 'Adulto', 'Idoso'])\ntitanic.drop(['Idade'], axis=1, inplace=True)\ntitanic"
 },
 {
  "nbid": "NB_2993668",
  "file": "NB_2993668,titanic,not_survived,index,df,ax-1.ipynb",
  "row_id": 22,
  "target_code": "titanic['Tipo_Tarifa'             ] = pd.qcut(titanic['Tarifa'        ], 3).astype(str).map( {intervals[0]: 'Barato'         , \n                                                                         intervals[1]: 'Medio'        , \n                                                                         intervals[2]: 'Caro'       } ).astype(str)\ntitanic.head()",
  "generation": "from sklearn import preprocessing\ntitanic['Tipo_Tarifa'] = pd.qcut(titanic['Tarifa'], 3, labels=['Barato', 'Medio', 'Caro'])\ntitanic.head()"
 },
 {
  "nbid": "NB_2993668",
  "file": "NB_2993668,titanic,not_survived,index,df,ax-1.ipynb",
  "row_id": 27,
  "target_code": "titanic['Sexo'      ] = titanic['Sexo'      ].map( {'female'        : 0, 'male'       : 1} ).astype(int)\ntitanic['Embarque'          ] = titanic['Embarque'          ].map( {'S'   : 0, 'C'    : 1, 'Q'    :2} ).astype(int)\ntitanic.head()",
  "generation": "titanic['Sexo'] = titanic['Sexo'].map({'male': 1, 'female': 0})\ntitanic['Embarque'] = titanic['Embarque'].map({'S': 0, 'C': 1, 'Q': 2})\ntitanic = pd.get_dummies(titanic, columns=['IdadeDisc', 'Tipo_Tarifa'], prefix=['Idade', 'Tarifa'])\ntitanic.head()"
 },
 {
  "nbid": "NB_2993668",
  "file": "NB_2993668,titanic,not_survived,index,df,ax-1.ipynb",
  "row_id": 28,
  "target_code": "dummy = pd.get_dummies(titanic['IdadeDisc'           ])\ntitanic = pd.concat([titanic, dummy], axis=1)\ndummy = pd.get_dummies(titanic['Tipo_Tarifa'             ])\ntitanic = pd.concat([titanic, dummy], axis=1)\ntitanic.drop(['IdadeDisc'           , 'Tipo_Tarifa'              ], axis=1, inplace=True)\ntitanic.head()",
  "generation": "titanic = pd.get_dummies(titanic, columns=['IdadeDisc', 'Tipo_Tarifa']) \ntitanic.head()"
 },
 {
  "nbid": "NB_2997802",
  "file": "NB_2997802,normal_movies-1.ipynb",
  "row_id": 8,
  "target_code": "normal_movies = normal_movies.sort_values('Year'      )\nnormal_movies.head()",
  "generation": "paranormal_activity = hollywood_movies[hollywood_movies['Film'] == 'Paranormal Activity']\nhollywood_movies = hollywood_movies[hollywood_movies['Film'] != 'Paranormal Activity']\nhollywood_movies.head()"
 },
 {
  "nbid": "NB_3001318",
  "file": "NB_3001318,diamonds-1.ipynb",
  "row_id": 19,
  "target_code": "import string\na=list(string.ascii_uppercase)[3:10]\na.reverse()     \nb=[0,1,2,3,4,5,6]\nfor i in range(7):\n    diamonds['color'       ]= diamonds['color'       ].replace(a[i], b[i])\ndiamonds.head(10)",
  "generation": "import pandas as pd\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndiamonds=pd.read_csv('../M2-mini-project1/diamonds.csv',index_col=0)\na=['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF']\nb=[0,1,2,3,4,5,6,7]\n\nfor i in range(8):\n    diamonds['clarity']= diamonds['clarity'].replace(a[i], b[i])\n\na=['J', 'I', 'H', 'G', 'F', 'E', 'D']\nb=[0,1,2,3,4,5,6]\n\nfor i in range(7):\n    diamonds['color']= diamonds['color'].replace(a[i], b[i])\n\ndiamonds.head(20)"
 },
 {
  "nbid": "NB_3005231",
  "file": "NB_3005231,df-1.ipynb",
  "row_id": 4,
  "target_code": "df['title_tokenized'                 ] = df['title'       ].apply(lambda x: list(tokenize(x, to_lower=True)))\ndf['title_cleaned'               ] = df['title_tokenized'                 ].apply(lambda x: \" \"    .join([z.lower() for z in x if z.isalpha()]))\nprint(df.shape)\ndf.head()",
  "generation": "vectorizer = CountVectorizer(tokenizer=lambda text: tokenize(text, lowercase=True))\ndf[\"title_tokenized\"] = df[\"title\"].apply(lambda text: list(tokenize(text, lowercase=True)))\ndf[\"title_cleaned\"] = df[\"title_tokenized\"].apply(lambda tokens: \" \".join(tokens))\ndf.head(2)"
 },
 {
  "nbid": "NB_3005231",
  "file": "NB_3005231,df-1.ipynb",
  "row_id": 5,
  "target_code": "df.index = df.title_cleaned.values\ndf = df.drop(['title_cleaned'               , 'title_tokenized'                  ], axis=1)\ndf.head()",
  "generation": "vectorizer = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x)\ndf2['title'] = df2['title_cleaned'].apply(lambda x: [x])\ndf2['title_tokenized'] = vectorizer.fit_transform(df2['title']).toarray().tolist()\ndf2 = df2[['title_cleaned', 'title_tokenized']]\ndf2 = df2.rename(columns={'title_cleaned': 'title', 'title_tokenized': 'txt_pl_lem_tokenized'})\ndf2.head()"
 },
 {
  "nbid": "NB_3010839",
  "file": "NB_3010839,data1-1.ipynb",
  "row_id": 8,
  "target_code": "data1 = data1.drop(['profit on operating activities / financial expenses'                                                     ,\n                    'gross profit (in 3 years) / total assets'                                                              ,\n                    'working capital / fixed assets'                                                    ,\n                    'equity / fixed assets'                                           ,\n                    'working capital'                                     ], axis=1) \ndata1.head()",
  "generation": "data1 = data1.drop(['constant capital / total assets', '(current assets - inventory - receivables) / short-term liabilities'], axis=1)\ndata1.head()"
 },
 {
  "nbid": "NB_3012104",
  "file": "NB_3012104,tot-1.ipynb",
  "row_id": 7,
  "target_code": "tot = pd.merge(tot, building_numbers,left_on=\"zip\"     ,right_on=0)\ntot = tot.drop(0,axis=1)\ntot = tot.rename(columns={1:\"Total Buildings\"                 })\ntot[\"Percent Insured\"                 ] = tot[\"Locations\"           ]/tot[\"Total Buildings\"                 ]*100\ntot.head()",
  "generation": "tot = pd.merge(pricing, flood,left_on=\"zip\",right_on=\"PostalCode\")\ntot = tot.drop([\"PostalCode\",\"high\",\"low\",\"property tax\",\"StateCode\",\"CountyName\",\"Building_TIV\",\"Contents_TIV\",\"Building_Limit\",\"Contents_Limit\"],axis=1)\nbuilding_numbers = building_numbers.reset_index(drop=True)\ntot = pd.merge(tot, building_numbers, on='zip')\ntot['Percent Insured'] = tot['Total Buildings'] / tot['Building Count']\ntot.head()"
 },
 {
  "nbid": "NB_3023511",
  "file": "NB_3023511,dataset2-1.ipynb",
  "row_id": 39,
  "target_code": "dataset2 = dataset2[col]\ndataset2",
  "generation": "col = (col[1:] + col[0:1])\ndataset2 = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data', sep= ',', header = None) \ndataset2 = dataset2.iloc[:, ::-1]\ndataset2.columns = col\ndataset2.head()"
 },
 {
  "nbid": "NB_3026571",
  "file": "NB_3026571,baby_names-1.ipynb",
  "row_id": 34,
  "target_code": "baby_names = baby_names.reset_index().iloc[:,1:]\nbaby_names",
  "generation": "baby_names = pd.read_csv(namesbystate_path, names=column_labels, compression='zip')\nbaby_names.head()"
 },
 {
  "nbid": "NB_687078",
  "file": "NB_687078,df_x,X_axis-1.ipynb",
  "row_id": 4,
  "target_code": "kmeans = KMeans(n_clusters = 3, init ='k-means++'           )\nkmeans.fit(df_x[df_x.columns[1:3]]) \ndf_x['cluster_label'               ] = kmeans.fit_predict(df_x[df_x.columns[1:3]])\ncenters = kmeans.cluster_centers_ \nlabels = kmeans.predict(df_x[df_x.columns[1:3]]) \ndf_x.head(10)",
  "generation": "kmeans = KMeans(n_clusters=1, init='k-means++', max_iter=300, n_init=10, random_state=0)\npred_y = kmeans.fit_predict(df_x)\ndf_x[\"cluster_label\"] = pred_y\ndf = pd.concat([df, df_x[\"cluster_label\"]], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_687105",
  "file": "NB_687105,all_stats-1.ipynb",
  "row_id": 45,
  "target_code": "all_stats[\"Male competition index\"                        ] = all_stats[\"Male Winners\"              ]/all_stats[\"Male Participants\"                   ]\nall_stats[\"Female competition index\"                          ] = all_stats[\"Female Winners\"                ]/all_stats[\"Female Participants\"                     ]\nall_stats[\"Total Competition\"                   ] = (all_stats[\"Male Winners\"              ]+all_stats[\"Female Winners\"                ])/(all_stats[\"Male Participants\"                   ]+all_stats[\"Female Participants\"                     ])\nall_stats.head()",
  "generation": "all_stats['Male competition index'  ] = all_stats['Male Winners'        ] / all_stats['Male Participants'    ]\nall_stats['Female competition index'] = all_stats['Female Winners'      ] / all_stats['Female Participants'  ]\nall_stats['Total Competition'       ] = (all_stats['Male Winners'      ] + all_stats['Female Winners'      ]) / (all_stats['Male Participants' ] + all_stats['Female Participants' ]) \nall_stats.head()"
 },
 {
  "nbid": "NB_689824",
  "file": "NB_689824,drugs,age,m_use-1.ipynb",
  "row_id": 55,
  "target_code": "drugs['alcohol_users'               ] = (drugs['alcohol-use'             ]/100) * drugs['n'   ]\ndrugs['marijuana_users'                 ] = (drugs['marijuana-use'               ]/100) * drugs['n'   ]\ndrugs['heroin_users'              ] = (drugs['heroin-use'            ]/100) * drugs['n'   ]\ndrugs['oxycontin_users'                 ] = (drugs['oxycontin-use'               ]/100) * drugs['n'   ]\ndrugs.head(20)",
  "generation": "drugs['alcohol_users'] = (drugs['n'] * drugs['alcohol-use']) / 100\ndrugs['marijuana_users'] = (drugs['n'] * drugs['marijuana-use']) / 100\ndrugs['heroin_users'] = (drugs['n'] * drugs['heroin-use']) / 100\ndrugs['oxycontin_users'] = (drugs['n'] * drugs['oxycontin-use']) / 100\ndrugs.head()"
 },
 {
  "nbid": "NB_697497",
  "file": "NB_697497,train_set,female_lowclass,male_lowclass,male_highclass-1.ipynb",
  "row_id": 6,
  "target_code": "train_set = train_set.drop(['Cabin'       ],axis=1)\ntrain_set",
  "generation": "train_set.drop('Cabin', axis=1, inplace=True)\ntrain_set"
 },
 {
  "nbid": "NB_698830",
  "file": "NB_698830,df-1.ipynb",
  "row_id": 5,
  "target_code": "df['Fecha'       ]=  pd.to_datetime(df.A\u00f1o.astype(str)+df.Mes.astype(str)+df.Dia.astype(str), format=\"%Y%m%d\"        )\ndf.head()",
  "generation": "df['Fecha'] = pd.to_datetime(df[['A\u00f1o', 'Mes', 'Dia']])  \ndf"
 },
 {
  "nbid": "NB_7160465",
  "file": "NB_7160465,df-1.ipynb",
  "row_id": 8,
  "target_code": "df = handel_non_numerical_data(df)\ndf.head()",
  "generation": "df = df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\ndf['Sex'      ] = np.where(df['Sex'] == 'male', 0, 1)\ndf['Embarked' ] = preprocessing.LabelEncoder().fit_transform(df['Embarked'].astype(str))\ndf = df.dropna()\ndf = df.reset_index(drop=True)\ndf.head()"
 },
 {
  "nbid": "NB_719483",
  "file": "NB_719483,polyTrain,myDF3-1.ipynb",
  "row_id": 7,
  "target_code": "polyTrain = scaler.fit_transform(polyTrain)\npolyTrain = pd.DataFrame(polyTrain)\npolyTrain = polyTrain.rename(columns = {'sqft_living'             : 0,  'sqft_lot'            : 1, 'floors'         :2, 'condition'            :3, 'yr_built'           :4,'yr_renovated'              :5, 'bedrooms'           :6, \n                  'lat'                       :7, 'long'       :8, 'bathrooms'            :9, 'waterfront'             : 10, 'view'       :11, 'grade'        :12, 'sqft_basement'                :13, 'sqft_above'             :14})\npolyTrain",
  "generation": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\npolyTrain = scaler.fit_transform(polyTrain)\npolyTrain = pd.DataFrame(polyTrain)\npolyTrain.columns = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14']\npolyTrain"
 },
 {
  "nbid": "NB_7681552",
  "file": "NB_7681552,df,y-1.ipynb",
  "row_id": 2,
  "target_code": "dummy = pd.get_dummies(df['Car Model'           ])\ndf = pd.concat([df,dummy],axis=1)\ndf = df.drop(['Car Model'           ,'Mercedez Benz C class'                       ],axis=1)\ndf",
  "generation": "car_dummies = pd.get_dummies(df['Car Model'])\ndf = pd.concat([df, car_dummies], axis='columns')\ndf = df.drop(columns=['Car Model'])"
 },
 {
  "nbid": "NB_3033065",
  "file": "NB_3033065,df,top_corr_features-1.ipynb",
  "row_id": 2,
  "target_code": "df = df.drop(columns=['Case No'         , 'Sex'      , 'Ethnicity'            , 'Jaundice '            , 'Family_ASD'             , 'Residence'            , 'Used_App_Before'                  ,\n                      'Screening Type'                                      , 'Language'           , 'User'       ])\ndf.head()",
  "generation": "df = df.drop(['Case No', 'Sex', 'Ethnicity', 'Jaundice', 'Family_ASD', 'Residence', 'Used_App_Before', 'Screening Type', 'Language', 'User'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_3033065",
  "file": "NB_3033065,df,top_corr_features-1.ipynb",
  "row_id": 3,
  "target_code": "df = pd.get_dummies(df)\ndf = df.drop(columns=['Class_NO'          ])\ndf.head()",
  "generation": "df['Class_YES'] = df['Class'].apply(lambda x: 1 if x == 'YES' else 0)\ndf = df.drop('Class', axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_3041696",
  "file": "NB_3041696,df,dtm-1.ipynb",
  "row_id": 10,
  "target_code": "print(\"Combining text features\"                         )\ndf['combined_text'               ] =df['Type'      ] + ' '     + df['Effects'         ] + ' '     + df['Flavor'        ] + df['Description'             ] + ' '    \ndf.head()",
  "generation": "df['combined_text'] = df['Type'] + ' ' + df['Effects'] + ' ' + df['Flavor'] + ' ' + df['Description']\ndf.head()"
 },
 {
  "nbid": "NB_3050817",
  "file": "NB_3050817,df-1.ipynb",
  "row_id": 75,
  "target_code": "df[\"highway-mpg\"             ]=235/df[\"highway-mpg\"             ]\ndf.rename(columns={\"highway-mpg\"             :\"highway-L/100km\"                 } ,inplace=True)\ndf.head()",
  "generation": "df['highway-L/100km'] = 235/df[\"highway-mpg\"]\ndf = df.rename(columns={\"city-L/100km\": \"city-L/100km\", \"highway-L/100km\": \"highway-L/100km\"})\ndf.head()"
 },
 {
  "nbid": "NB_3057422",
  "file": "NB_3057422,X-1.ipynb",
  "row_id": 2,
  "target_code": "X = X.drop(['PassengerId'             , 'Ticket'         , 'Name'       , 'Embarked'           ], axis = 1)\nX.head()",
  "generation": "X = pd.read_csv('datasets/Titanic/train.csv')\nX = X[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin']]\nX.head()"
 },
 {
  "nbid": "NB_3060559",
  "file": "NB_3060559,df-1.ipynb",
  "row_id": 41,
  "target_code": "df[\"highway-mpg\"             ] = 235/df[\"highway-mpg\"             ]\ndf.head()",
  "generation": "df['highway-L/100km'] = 235/df[\"highway-mpg\"]\ndf.rename(columns={'highway-mpg':'highway-L/100km'}, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_3063842",
  "file": "NB_3063842,Y_pp-1.ipynb",
  "row_id": 95,
  "target_code": "Y_pp['pred_class_thresh10'                     ] = [1 if x >= 0.10 else 0 for x in Y_pp.class_1_pp.values]\nY_pp.head(50)",
  "generation": "Y_pp['pred_class_thresh10'] = np.where(Y_pp['class_1_pp']>0.1, 1, 0)\nY_pp.head()"
 },
 {
  "nbid": "NB_3072041",
  "file": "NB_3072041,data_predicted,logis_predicted-1.ipynb",
  "row_id": 4,
  "target_code": "from sklearn.linear_model import LogisticRegression\nlogisticModel = LogisticRegression(solver='liblinear'           )\nlogisticModel.fit(X, Y)\npredicted = pd.DataFrame({'LogisPredicted'                :logisticModel.predict(X)})\ndata_predicted = pd.concat([data_predicted, predicted], axis =1)\ndata_predicted",
  "generation": "logisticModel = LogisticRegression(random_state=0)\nlogisticModel.fit(X, Y)\npredicted = pd.DataFrame({'LogisPredicted':logisticModel.predict(X)})\ndata_predicted = pd.concat([data_predicted, predicted], axis =1)\ndata_predicted.head()"
 },
 {
  "nbid": "NB_3076413",
  "file": "NB_3076413,df-1.ipynb",
  "row_id": 16,
  "target_code": "df['ABV_float'           ] = df['ABV'     ].str.replace('%'   , ''   ).astype('float'       )\ndf.head()",
  "generation": "df['ABV_float'] = df['ABV'].str.replace('%','').astype(float)\ndf.head()"
 },
 {
  "nbid": "NB_3076427",
  "file": "NB_3076427,data-1.ipynb",
  "row_id": 3,
  "target_code": "data = pd.get_dummies(data, columns=['V6'    ], prefix=['V6'    ])\ndata.head()",
  "generation": "data = pd.get_dummies(data, columns=['V6'], prefix=['V6'])\ndata.head()"
 },
 {
  "nbid": "NB_3078402",
  "file": "NB_3078402,tx_retention-1.ipynb",
  "row_id": 24,
  "target_code": "tx_retention = pd.DataFrame(retention_array)\ntx_retention['RetentionRate'               ]=tx_retention['RetainedUserCount'                   ]/tx_retention['TotalUserCount'                ]\ntx_retention.head()",
  "generation": "tx_retention['TotalUserCount' ] = tx_retention.iloc[:,1:].sum(axis=1)\ntx_retention['RetainedUserCount' ] = tx_retention.iloc[:,1:].apply(lambda x: x[x!=0].count(), axis=1)\ntx_retention['RetentionRate' ] = tx_retention['RetainedUserCount' ]/tx_retention['TotalUserCount' ]\ntx_retention.head()"
 },
 {
  "nbid": "NB_3089214",
  "file": "NB_3089214,df-1.ipynb",
  "row_id": 102,
  "target_code": "df.head()",
  "generation": "df['highway-L/100km'] = 235/df[\"highway-mpg\"]\ndf.head()"
 },
 {
  "nbid": "NB_3095742",
  "file": "NB_3095742,test_df-1.ipynb",
  "row_id": 35,
  "target_code": "test_df['dayofweek'           ]=test_df['effective_date'                ].dt.dayofweek\ntest_df['weekend'         ] = test_df['dayofweek'           ].apply(lambda x: 1 if (x>3)  else 0)\ntest_df.head()",
  "generation": "le = preprocessing.LabelEncoder()\ntest_df['education'] = le.fit_transform(test_df['education'])\ntest_df['due_date'] = pd.to_datetime(test_df['due_date'])\ntest_df['effective_date'] = pd.to_datetime(test_df['effective_date'])\ntest_df['dayofweek'] = test_df['effective_date'].dt.dayofweek\ntest_df['weekend'] = test_df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0)\ntest_df.head()"
 },
 {
  "nbid": "NB_3098915",
  "file": "NB_3098915,data,av_price-1.ipynb",
  "row_id": 25,
  "target_code": "data['item_price_float'                  ] = data['item_price'            ].str.replace('$'   , ''   ).astype(float)\ndata.tail(10)",
  "generation": "data['item_price_float'] = data['item_price'].apply(lambda x: float(x[1:]))"
 },
 {
  "nbid": "NB_3098915",
  "file": "NB_3098915,data,av_price-1.ipynb",
  "row_id": 29,
  "target_code": "data = data.drop(['item_price'            ], axis=1)\ndata.tail(10)",
  "generation": "data['item_price_float'] = data['item_price'].str.replace('$', '').astype(float)\ndata = data.drop('item_price', axis=1)\ndata.tail(10)"
 },
 {
  "nbid": "NB_3099358",
  "file": "NB_3099358,df2-1.ipynb",
  "row_id": 101,
  "target_code": "df2['intercept'           ] = 1\ndf2['ab_page'         ] = np.where(df2['group'       ] == 'control'          , 0, 1)\ndf2.head()",
  "generation": "df2['intercept'] = 1\ndf2['ab_page'] = pd.get_dummies(df2['group'])['treatment']\ndf2.head()"
 },
 {
  "nbid": "NB_3099358",
  "file": "NB_3099358,df_new-1.ipynb",
  "row_id": 116,
  "target_code": "df_new['CA_int_ab_page'                ] = df_new['CA'    ] * df_new['ab_page'         ]\ndf_new['UK_int_ab_page'                ] = df_new['UK'    ] * df_new['ab_page'         ]\ndf_new['US_int_ab_page'                ] = df_new['US'    ] * df_new['ab_page'         ]\ndf_new.head()",
  "generation": "df_new['CA_int_ab_page'] = df_new['CA'] * df_new['ab_page']\ndf_new['UK_int_ab_page'] = df_new['UK'] * df_new['ab_page']\ndf_new['US_int_ab_page'] = df_new['US'] * df_new['ab_page']\ndf_new.head()"
 },
 {
  "nbid": "NB_3105003",
  "file": "NB_3105003,df-1.ipynb",
  "row_id": 68,
  "target_code": "df['highway-mpg'             ] = 235/df['highway-mpg'             ]\ndf.rename(columns = {\"highway-mpg\"              : \"highway-L/100km\"                  }, inplace = True)\ndf.head()",
  "generation": "df['highway-L/100km'] = 235/df[\"highway-mpg\"]\ndf.head()"
 },
 {
  "nbid": "NB_3111745",
  "file": "NB_3111745,RFresults_df-1.ipynb",
  "row_id": 34,
  "target_code": "RFresults_df=RFresults_df.set_index('feature_names'               )\nRFresults_df",
  "generation": "RFresults_df = RFresults_df.reset_index()\nRFresults_df.columns = ['Unnamed: 0_level_0', 'feature_importances', 'feature_names', 'Unnamed: 1_level_1']\nRFresults_df = RFresults_df.drop('Unnamed: 0_level_0', axis=1)\nRFresults_df.head()"
 },
 {
  "nbid": "NB_3111838",
  "file": "NB_3111838,scraped_raw-1.ipynb",
  "row_id": 55,
  "target_code": "scraped_raw['full_description'                  ] = scraped_raw[['headline'          ,'article'         ]].agg(' '   .join, axis = 1)\nscraped_raw['full_description'                  ] = scraped_raw['full_description'                  ].astype(\"string\"        )\nscraped_raw['full_description'                  ] = scraped_raw['full_description'                  ].str.replace(r'[^\\w\\s]+'           ,''  ) \nscraped_raw['full_description'                  ] = scraped_raw['full_description'                  ].str.lower().str.split()\nscraped_raw['full_description'                  ] = scraped_raw['full_description'                  ].apply(lambda x: [item for item in x if item not in stop])\nscraped_raw.head(10)",
  "generation": "scraped_raw['full_description'] = scraped_raw['headline'] + scraped_raw['article']\nscraped_raw['full_description'] = scraped_raw['full_description'].str.replace('[^\\w\\s]','')\nscraped_raw['full_description'] = scraped_raw['full_description'].str.lower()\nscraped_raw['full_description'] = scraped_raw['full_description'].apply(lambda x: [item for item in x.split() if item not in stop])"
 },
 {
  "nbid": "NB_3114580",
  "file": "NB_3114580,df-1.ipynb",
  "row_id": 6,
  "target_code": "df = df.set_index(\"ID\"    )\ndf",
  "generation": "df.columns = pd.MultiIndex.from_product([[''], df.columns])\ndf.head()"
 },
 {
  "nbid": "NB_3114580",
  "file": "NB_3114580,df-1.ipynb",
  "row_id": 11,
  "target_code": "df.columns = [col.lower().replace(\" \"   , \"_\"    ) for col in df.columns]\ndf.head()",
  "generation": "df = df.drop(columns = [\"URL\", 'Icon URL', 'Subtitle', 'Description', \"In-app Purchases\"])\ndf = df.set_index(\"ID\")\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\ndf.head()"
 },
 {
  "nbid": "NB_3133578",
  "file": "NB_3133578,titanic,y-1.ipynb",
  "row_id": 5,
  "target_code": "titanic = titanic.replace({'1st'     :1, '2nd'      :2, '3rd'      :3, 'female'         :1, 'male'       :0})\ntitanic",
  "generation": "titanic['pclass'] = titanic['pclass'].map({'1st': 1, '2nd': 2, '3rd': 3})\ntitanic['gender'] = titanic['gender'].map({'male': 0, 'female': 1})\ntitanic = titanic.rename(columns={'gender': 'female'})\ntitanic.head()"
 },
 {
  "nbid": "NB_3136010",
  "file": "NB_3136010,df-1.ipynb",
  "row_id": 4,
  "target_code": "forecast_col = 'Close'        \ndf['label'       ] = df[forecast_col].shift(-1)\ndf = df.drop(df[df.isnull().any(axis=1)].index)\nX = np.array(df.drop(['label'       ], 1))\ndf.head()",
  "generation": "df_full['label'] = df_full['Adj Close']\ndf = df_full.loc[:,['Close','Volume']]\ndf['HL_PCT'] = (df_full['High'] - df_full['Low']) / df_full['Close'] * 100.0\ndf['PCT_change'] = (df_full['Close'] - df_full['Open']) / df_full['Open'] * 100.0\ndf['Low'] = df_full['Low']\ndf['High'] = df_full['High']\ndf['Open'] = df_full['Open']\ndf = df.iloc[1:]\ndf.head()"
 },
 {
  "nbid": "NB_632440",
  "file": "NB_632440,df-1.ipynb",
  "row_id": 3,
  "target_code": "df[\"Men Percentage\"                ] = df[\"Men\"     ]/df[\"TotalPop\"          ]\ndf[\"Women Percentage\"                  ] = df[\"Women\"       ]/df[\"TotalPop\"          ]\ndf.head()",
  "generation": "df['Men Percentage'  ] = df['Men'  ] / df['TotalPop']\ndf['Women Percentage'] = df['Women'] / df['TotalPop']\ndf.head()"
 },
 {
  "nbid": "NB_634366",
  "file": "NB_634366,df-1.ipynb",
  "row_id": 28,
  "target_code": "df=df.sort_values('diff'      )\ndf",
  "generation": "df['TICKET_ITEM_ID'] = df['TICKET_ITEM_ID'].astype(str)\ndf['SERIAL_ID'] = df['SERIAL_ID'].astype(str)\ndf['TICKET_ENTRANCE_NO'] = df['TICKET_ENTRANCE_NO'].astype(str)\ndf['ORDER_ID'] = df['ORDER_ID'].astype(str)\ndf['diff_hour'] = np.floor(df['diff'].astype('timedelta64[s]') / 3600)\ndf.head()"
 },
 {
  "nbid": "NB_637149",
  "file": "NB_637149,airpassengers-1.ipynb",
  "row_id": 12,
  "target_code": "airpassengers['TravelDate'            ] = pd.to_datetime(airpassengers['TravelDate'            ])\nairpassengers.head()",
  "generation": "airpassengers['TravelDate'] = pd.to_datetime(airpassengers['TravelDate'], format='%m/%d/%Y')\nairpassengers.head()"
 },
 {
  "nbid": "NB_650300",
  "file": "NB_650300,testMercBenz-1.ipynb",
  "row_id": 22,
  "target_code": "columnsToAdd = list(filter(lambda x: x not in testMercBenz.columns, mercedesBenz.columns))\ncolumnsToAdd = DataFrame(data=np.zeros((len(testMercBenz), len(columnsToAdd)), dtype=int), columns=columnsToAdd)\ntestMercBenz = testMercBenz.join(columnsToAdd)\ntestMercBenz.head()",
  "generation": ""
 },
 {
  "nbid": "NB_707721",
  "file": "NB_707721,df-1.ipynb",
  "row_id": 12,
  "target_code": "df['diagnosis'           ] = df['diagnosis'           ].map(class_mapping)\ndf.head()",
  "generation": "df = df.drop(\"id\", 1)\ndf['diagnosis'] = df['diagnosis'].map(class_mapping)\ndf = df.drop(['radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave_points_se', 'symmetry_se', 'fractal_dimension_se'], axis=1)\ndf = df.rename(columns={'concave_points_mean': 'concavity_points_mean', 'concave_points_worst': 'concavity_points_worst', 'fractal_dimension_mean': 'fractal_dimension', 'fractal_dimension_worst': 'fractal_dimension_w', 'symmetry_mean': 'symmetry', 'symmetry_worst': 'symmetry_w'})\ndf.head()"
 },
 {
  "nbid": "NB_707744",
  "file": "NB_707744,df_origin-1.ipynb",
  "row_id": 74,
  "target_code": "df_origin = df_origin.drop(\"origin_Asia\"             , axis=1)\ndf_origin.head()",
  "generation": "df_origin.drop('origin_Asia', axis=1, inplace=True)\ndf_origin.head()"
 },
 {
  "nbid": "NB_709153",
  "file": "NB_709153,df_raiden_all-1.ipynb",
  "row_id": 21,
  "target_code": "df_raiden_all = pd.merge(df_raiden_all, df_raiden_settled, how=\"outer\"       , on=\"Channel_ID\"            , suffixes=(\"_all\"      , \"_settled\"           ))\ndf_raiden_all.head(10)",
  "generation": "df_raiden_all = pd.merge(df_raiden_all, df_raiden_settled, how=\"outer\", on=\"Channel_ID\")\ndf_raiden_all.head(10)"
 },
 {
  "nbid": "NB_709153",
  "file": "NB_709153,df_raiden_all-1.ipynb",
  "row_id": 22,
  "target_code": "df_raiden_all = df_raiden_all.join(df_raiden_deposit_tempframe)\ndf_raiden_all.head(10)",
  "generation": "df_raiden_all = pd.merge(df_raiden_all, df_raiden_deposit_tempframe, how=\"outer\", on=\"Channel_ID\")\ndf_raiden_all.head(10)"
 },
 {
  "nbid": "NB_709153",
  "file": "NB_709153,df_raiden_all-1.ipynb",
  "row_id": 48,
  "target_code": "df_raiden_all = df_raiden_all.join(df_raiden_time_tempframe)\ndf_raiden_all.head(10)",
  "generation": "df_raiden_all = df_raiden_all.join(df_raiden_time_tempframe)\ndf_raiden_all.head(10)"
 },
 {
  "nbid": "NB_709153",
  "file": "NB_709153,df_raiden_events-1.ipynb",
  "row_id": 62,
  "target_code": "df_raiden_events[\"BOOL_opened\"             ] = df_raiden_events.Transaction_opened.notna()\ndf_raiden_events[\"BOOL_closed\"             ] = df_raiden_events.Transaction_closed.notna()\ndf_raiden_events.head(10)",
  "generation": "df_raiden_events['BOOL_opened'] = df_raiden_events['Transaction_opened'].notna()\ndf_raiden_events['BOOL_closed'] = df_raiden_events['Transaction_closed'].notna()\ndf_raiden_events.head()"
 },
 {
  "nbid": "NB_709153",
  "file": "NB_709153,df_raiden_events-1.ipynb",
  "row_id": 63,
  "target_code": "df_raiden_events[\"Block_Number\"              ] = df_raiden_events.index\ndf_raiden_events = df_raiden_events.sort_index(axis = \"index\"        )\ndf_raiden_events.head()",
  "generation": "df_raiden_events['Block_Number'] = df_raiden_events.index\ndf_raiden_events.head()"
 },
 {
  "nbid": "NB_709153",
  "file": "NB_709153,df_raiden_events-1.ipynb",
  "row_id": 64,
  "target_code": "df_raiden_events[\"cumsum_opened\"               ] = df_raiden_events[\"BOOL_opened\"             ].cumsum(axis=\"index\"       )\ndf_raiden_events[\"cumsum_closed\"               ] = df_raiden_events[\"BOOL_closed\"             ].cumsum(axis=\"index\"       )\ndf_raiden_events[\"cumsum_diff\"             ] = df_raiden_events[\"cumsum_opened\"               ] - df_raiden_events[\"cumsum_closed\"               ]\ndf_raiden_events.head(10)",
  "generation": "df_raiden_events[\"cumsum_opened\"] = df_raiden_events[\"BOOL_opened\"].cumsum()\ndf_raiden_events[\"cumsum_closed\"] = df_raiden_events[\"BOOL_closed\"].cumsum()\ndf_raiden_events[\"cumsum_diff\"] = df_raiden_events[\"cumsum_opened\"] - df_raiden_events[\"cumsum_closed\"]\ndf_raiden_events.head()"
 },
 {
  "nbid": "NB_709362",
  "file": "NB_709362,data-1.ipynb",
  "row_id": 19,
  "target_code": "data.coffees = data.coffees.astype(int)\ndata.head()",
  "generation": "data['coffees'] = pd.to_numeric(data['coffees'], errors='coerce')\ndata = data.dropna(subset=['coffees'])\ndata = data.astype({'coffees': 'int32'})\ndata.head()"
 },
 {
  "nbid": "NB_709362",
  "file": "NB_709362,weekday_counts-1.ipynb",
  "row_id": 36,
  "target_code": "weekday_counts = data.groupby('weekdays'          ).count()\nweekday_counts = weekday_counts.loc[weekday_names]\nweekday_counts",
  "generation": "weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nweekday_counts = data.groupby('weekdays').count()\nweekday_counts = weekday_counts.reindex(weekday_names, axis=0)\nweekday_counts"
 },
 {
  "nbid": "NB_711020",
  "file": "NB_711020,crime_norm-1.ipynb",
  "row_id": 49,
  "target_code": "col=['\uac15\uac04'    , '\uac15\ub3c4'     , '\uc0b4\uc778'     , '\uc808\ub3c4'     , '\ud3ed\ub825'     ]\ncrime_norm['\ubc94\uc8c4'    ] = np.sum(crime_norm[col], axis=1)\ncrime_norm.head()",
  "generation": "crime_norm['\ubc94\uc8c4'] = np.sum(crime_norm[col], axis=1)\ncrime_norm.head()"
 },
 {
  "nbid": "NB_711020",
  "file": "NB_711020,crime_norm-1.ipynb",
  "row_id": 50,
  "target_code": "col2 =['\uac15\uac04\uac80\uac70\uc728'       , '\uac15\ub3c4\uac80\uac70\uc728'        , '\uc0b4\uc778\uac80\uac70\uc728'        , '\uc808\ub3c4\uac80\uac70\uc728'        , '\ud3ed\ub825\uac80\uac70\uc728'        ]\ncrime_norm['\uac80\uac70'    ] = np.sum(crime_norm[col2], axis=1)\ncrime_norm.head()",
  "generation": "crime_norm['\uac80\uac70'] = np.sum(crime_norm[col2], axis=1)\ncrime_norm.head()"
 },
 {
  "nbid": "NB_7382748",
  "file": "NB_7382748,X-1.ipynb",
  "row_id": 9,
  "target_code": "X = X.drop(['cut'     , 'color'        , 'clarity'          ], axis=1)\nX.head(n=2)",
  "generation": "X = X.drop(['cut', 'color', 'clarity'], axis=1)\nX.head(n=2)"
 },
 {
  "nbid": "NB_3140621",
  "file": "NB_3140621,df,name-1.ipynb",
  "row_id": 2,
  "target_code": "df = df.sort_values('PER CAPITA INCOME '                    , ascending = False)\ndf.head()",
  "generation": "df = df.sort_values(by=['HARDSHIP INDEX'], ascending=True)\ndf = df.head()\ndf"
 },
 {
  "nbid": "NB_3142520",
  "file": "NB_3142520,df,X,y-1.ipynb",
  "row_id": 15,
  "target_code": "df = df.drop([\"date\"      , \"time\"       , \"username\"           ], axis=1)\ndf.head()",
  "generation": "df = df.select_dtypes(include=[float, int])\ndf = df.drop(['Unnamed: 0', 'username', 'date', 'time'], axis=1)"
 },
 {
  "nbid": "NB_3147887",
  "file": "NB_3147887,df-1.ipynb",
  "row_id": 20,
  "target_code": "df['NewCol1'         ] = 5\ndf['NewCol2'         ] = df['NewCol1'         ] + df['Col1'      ]\ndf2 = df \ndf",
  "generation": "df['NewCol1'] = 5\ndf['NewCol2'] = [i+5 for i in range(5)]\ndf.columns = ['Unnamed: 0', 'Col1', 'NewCol1', 'NewCol2']\ndf.head()"
 },
 {
  "nbid": "NB_3147887",
  "file": "NB_3147887,df-1.ipynb",
  "row_id": 21,
  "target_code": "df = df.drop(columns=['NewCol1'         ])\ndf",
  "generation": "df.drop(['NewCol1'], axis=1, inplace=True)\ndf.rename(columns={'NewCol2': 'NewCol1'}, inplace=True)\ndf.drop(['Unnamed: 0'], axis=1, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_3159109",
  "file": "NB_3159109,data_df-1.ipynb",
  "row_id": 7,
  "target_code": "data_df[\"hispanic\"          ] = np.select(\n    [data_df[\"ethcty\"        ] == \"HISPANIC\"           ],\n    [1],\n    default=0\n)\ndata_df[\"white\"       ] = np.select(\n    [(data_df[\"ethcty\"        ] == \"WHITE NON HISPANIC\"                     ) | (data_df[\"ethcty\"        ] == \"WHITE NON HISP\"                 )],\n    [1],\n    default=0\n)\ndata_df[\"asian_and_pacific_islander\"                            ] = np.select(\n    [(data_df[\"ethcty\"        ] == \"ASIAN AND PACIFIC ISLANDER\"                             ) | (data_df[\"ethcty\"        ] == \"ASIAN AND PACI\"                 )],\n    [1], \n    default=0\n)\ndata_df[\"black\"       ] = np.select(\n    [(data_df[\"ethcty\"        ] == \"BLACK NON HISPANIC\"                     ) | (data_df[\"ethcty\"        ] == \"BLACK NON HISP\"                 )],\n    [1],\n    default=0\n)\ndel data_df[\"ethcty\"        ]\ndata_df.head()",
  "generation": "data_df = pd.concat([data_df,pd.get_dummies(data_df['ethcty'])],axis=1)\ndel data_df['ethcty']\ndata_df.head()"
 },
 {
  "nbid": "NB_3159109",
  "file": "NB_3159109,data_df-1.ipynb",
  "row_id": 9,
  "target_code": "data_df = data_df.rename(columns={\"brth_yr\"         : \"birth_year\"             , \"cnt\"      : \"count\"        , \"rnk\"      : \"rank\"       , \"fml\"      : \"female\"         , \"ml\"     : \"male\"       })\ndata_df.head()",
  "generation": "data_df = data_df.rename(columns={'brth_yr': 'birth_year', 'nm': 'count'})\ndata_df.head()"
 },
 {
  "nbid": "NB_3159886",
  "file": "NB_3159886,number-1.ipynb",
  "row_id": 2,
  "target_code": "number[\"1\uc778\uac00\uad6c\uc728\"       ] = (number[\"1\uc778\uac00\uad6c\"      ] / number[\"\uacc4\"   ]) * 100\nnumber.head()",
  "generation": "number['1\uc778\uac00\uad6c\uc728'] = number['1\uc778\uac00\uad6c']/number['\uacc4']*100\nnumber.head()"
 },
 {
  "nbid": "NB_3165529",
  "file": "NB_3165529,oil-1.ipynb",
  "row_id": 5,
  "target_code": "oil = oil.dropna()\noil.head()",
  "generation": "oil_production = oil_production.loc['1994-03-01':'2005-01-01']\noil_price = oil_price.loc['1994-03-01':'2005-01-01']\noil = pd.concat((oil_production,oil_price),axis = 1) # 1: columnwise 0: (default): rollwise\noil.head()"
 },
 {
  "nbid": "NB_3169044",
  "file": "NB_3169044,vols-1.ipynb",
  "row_id": 25,
  "target_code": "vols = vols.sort_values(by=['Date'      ,'K'   ,'Newton'        ],ascending=[True,True,True])\nvols.head()",
  "generation": "vols = vols.dropna()"
 },
 {
  "nbid": "NB_3169485",
  "file": "NB_3169485,df4-1.ipynb",
  "row_id": 82,
  "target_code": "df4[['CA'    ,'UK'    ,'US'    ]] = pd.get_dummies(df4.country)\ndf4.drop(columns=['country'         ,'CA'    ],inplace=True)\ndf4.head()",
  "generation": "df_countries = pd.read_csv('countries.csv')\ndf3 = pd.get_dummies(df2['country'])\ndf4 = pd.concat([df2, df3], axis=1)\ndf4 = df4.drop(['country'], axis=1)\ndf4 = df4.join(df_countries.set_index('user_id'),on='user_id')\ndf4 = pd.concat([df4, pd.get_dummies(df4['country'])], axis=1)\ndf4 = df4.drop(['country'], axis=1)\ndf4.head()"
 },
 {
  "nbid": "NB_3169485",
  "file": "NB_3169485,df6,conversion_mean_day_new,conversion_mean_day_old,unique_days-1.ipynb",
  "row_id": 105,
  "target_code": "df6['isdaytime'           ] = ((df6.hour >= 0) & (df6.hour < 12))                         \ndf6['first_days'            ] = ((df6.day >= 2) & (df6.day < 13))                          \ndf6['isweekday'           ] = ((df6['weekday'         ]%7 == 5) | (df6['weekday'         ]%7 == 6))         \ndf6[['weekend'         ,'weekday'         ]] = pd.get_dummies(df6.isweekday)\ndf6.drop(columns=['isweekday'           ,'weekend'         ,'date'      ],inplace=True)\ndf6[['pm'    ,'am'    ]] = pd.get_dummies(df6.isdaytime)\ndf6[['later_days'            ,'early_days'            ]] = pd.get_dummies(df6.first_days)\ndf6.drop(columns=['isdaytime'           ,'pm'    ,'later_days'            ,'first_days'            ],inplace=True)\ndf6.head()",
  "generation": "df['am'        ] = pd.to_datetime(df['timestamp']).apply(lambda x: 1 if x.hour < 12 else 0)\ndf['early_days'] = pd.to_datetime(df['timestamp']).apply(lambda x: 1 if x.day <= 3 else 0)\ndf.head()"
 },
 {
  "nbid": "NB_3169508",
  "file": "NB_3169508,df,Y-1.ipynb",
  "row_id": 4,
  "target_code": "df['Gender'        ] = df['Gender'        ].replace(\"Male\"      , 0).replace(\"Female\"        , 1)\ndf.head()",
  "generation": "df['Gender'] = df['Gender'].replace(['Male', 'Female'], [0, 1])\ndf.head()"
 },
 {
  "nbid": "NB_3179042",
  "file": "NB_3179042,df,y-1.ipynb",
  "row_id": 1,
  "target_code": "df=df.dropna(axis=1,thresh=6000)\ndf=df.drop(['status_published'                  ,'status_id'           ],axis=1)\ndf.head()",
  "generation": "df = df[['status_type', 'num_reactions', 'num_comments', 'num_shares', 'num_likes', 'num_loves', 'num_wows', 'num_hahas', 'num_sads', 'num_angrys']]\ndf.head()"
 },
 {
  "nbid": "NB_3179042",
  "file": "NB_3179042,df,y-1.ipynb",
  "row_id": 2,
  "target_code": "df=pd.get_dummies(df)\ndf.head()",
  "generation": "df = pd.get_dummies(df, columns=['status_type'])\ndf.head()"
 },
 {
  "nbid": "NB_3195340",
  "file": "NB_3195340,df-1.ipynb",
  "row_id": 75,
  "target_code": "df['highway-mpg'             ] = 235 / df['highway-mpg'             ]\ndf.rename({'highway-mpg'             : 'highway-L/100km'                  }, inplace=True)\ndf.head()",
  "generation": "df['highway-L/100km'] = 235/df[\"highway-mpg\"]\ndf = df.rename(columns={\"city-L/100km\": \"city-L/100km\", \"highway-L/100km\": \"highway-L/100km\"})\ndf.head()"
 },
 {
  "nbid": "NB_3215401",
  "file": "NB_3215401,data-1.ipynb",
  "row_id": 4,
  "target_code": "data['Time'      ] = pd.to_datetime(data['Time'      ])\ndata = data.set_index('Time'      )\ndata.head()",
  "generation": "data = data[['Time', 'L06_347', 'LS06_347', 'LS06_348']]\ndata.head()"
 },
 {
  "nbid": "NB_3215475",
  "file": "NB_3215475,star_wars,females-1.ipynb",
  "row_id": 3,
  "target_code": "star_wars = star_wars[pd.notnull(star_wars['RespondentID'              ])]\nstar_wars.head()",
  "generation": "star_wars = star_wars[pd.notnull(star_wars['RespondentID'])]"
 },
 {
  "nbid": "NB_3215475",
  "file": "NB_3215475,star_wars,females-1.ipynb",
  "row_id": 6,
  "target_code": "yes_no = {'Yes'     :True, 'No'     :False}\nfor cols in [\n    \"Have you seen any of the 6 films in the Star Wars franchise?\"                                                                  ,\n    \"Do you consider yourself to be a fan of the Star Wars film franchise?\"                                                                           \n    ]:\n    star_wars[cols] = star_wars[cols].map(yes_no)\nstar_wars.head(10)    ",
  "generation": "star_wars.replace({'Yes': True, 'No': False}, inplace=True)\nstar_wars.head()"
 },
 {
  "nbid": "NB_3215475",
  "file": "NB_3215475,star_wars,females-1.ipynb",
  "row_id": 7,
  "target_code": "import numpy as np \nmovie_mapping = {\n    \"Star Wars: Episode I  The Phantom Menace\"                                              : True,\n    np.nan: False,\n    \"Star Wars: Episode II  Attack of the Clones\"                                                 : True,\n    \"Star Wars: Episode III  Revenge of the Sith\"                                                 : True,\n    \"Star Wars: Episode IV  A New Hope\"                                       : True,\n    \"Star Wars: Episode V The Empire Strikes Back\"                                                  : True,\n    \"Star Wars: Episode VI Return of the Jedi\"                                              : True\n}\nfor col in star_wars.columns[3:9]:\n    star_wars[col] = star_wars[col].map(movie_mapping)\nstar_wars.head()",
  "generation": "star_wars = star_wars.rename(columns={\n    \"Star Wars: Episode I  The Phantom Menace\": \"seen_1\",\n    \"Star Wars: Episode II  Attack of the Clones\": \"seen_2\",\n    \"Star Wars: Episode III  Revenge of the Sith\": \"seen_3\",\n    \"Star Wars: Episode IV  A New Hope\": \"seen_4\",\n    \"Star Wars: Episode V The Empire Strikes Back\": \"seen_5\",\n    \"Star Wars: Episode VI Return of the Jedi\": \"seen_6\"\n})\n\nyes_no = {'Yes':True, 'No':False}\n\nfor cols in [\n    \"Have you seen any of the 6 films in the Star Wars franchise?\",\n    \"Do you consider yourself to be a fan of the Star Wars film franchise?\"\n    ]:\n    star_wars[cols] = star_wars[cols].map(yes_no)\n\nmovie_mapping = {\n    \"Star Wars: Episode I  The Phantom Menace\": True,\n    np.nan: False,\n    \"Star Wars: Episode II  Attack of the Clones\": True,\n    \"Star Wars: Episode III  Revenge of the Sith\": True,\n    \"Star Wars: Episode IV  A New Hope\": True,\n    \"Star Wars: Episode V The Empire Strikes Back\": True,\n    \"Star Wars: Episode VI Return of the Jedi\": True\n}\n\nfor col in star_wars.columns[3:9]:\n    star_wars[col] = star_wars[col].map(movie_mapping)\n\nstar_wars.head(2)"
 },
 {
  "nbid": "NB_3222914",
  "file": "NB_3222914,df-1.ipynb",
  "row_id": 75,
  "target_code": "df[\"highway-mpg\"             ] = 235/df[\"highway-mpg\"             ]\ndf.rename(columns={'\"highway-mpg\"'               :'highway-L/100km'                 }, inplace=True)\ndf.rename(columns={'\"highway-mpg\"'               :'highway-L/100km'                 }, inplace=True)\ndf.head()",
  "generation": "df['highway-L/100km'] = 235/df[\"highway-mpg\"]\ndf.rename(columns={'city-L/100km':'city-L/100km', 'highway-L/100km':'highway-L/100km'}, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_3224601",
  "file": "NB_3224601,bj,labels-1.ipynb",
  "row_id": 24,
  "target_code": "bins = [30,60,90,120,180,300]\nbj['\u9762\u79ef\u5206\u7ec4'      ] = pd.cut(bj.\u623f\u5c4b\u9762\u79ef,bins)\nbj.head(2)",
  "generation": "bins = [0, 30, 60, 90, 120, 150, 200, 300, 1000]\ngroup_names = ['(0, 30]', '(30, 60]', '(60, 90]', '(90, 120]', '(120, 150]', '(150, 200]', '(200, 300]', '(300, 1000]']\nbj['\u9762\u79ef\u5206\u7ec4'] = pd.cut(bj['\u623f\u5c4b\u9762\u79ef'], bins, labels=group_names)\nbj.head(2)"
 },
 {
  "nbid": "NB_3228239",
  "file": "NB_3228239,df,errors,ax-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.sort_values('Total'       ,ascending=False)\ndf.head(10)",
  "generation": "df = df.sort_values('Total', ascending=False)\ndf.head()"
 },
 {
  "nbid": "NB_3235966",
  "file": "NB_3235966,df-1.ipynb",
  "row_id": 2,
  "target_code": "df = df.drop(columns=[\"host_id\"         ,\n                      \"host_name\"                                 ,\n                      \"last_review\"                                   ,\n                      \"reviews_per_month\"                                         ])\nprint(df.shape)\ndf.head()",
  "generation": "df = df[['id', 'name', 'neighbourhood_group', 'neighbourhood', 'latitude', 'longitude', 'room_type', 'price', 'minimum_nights', 'number_of_reviews', 'calculated_host_listings_count', 'availability_365']]\ndf.head()"
 },
 {
  "nbid": "NB_603565",
  "file": "NB_603565,crim_anal,x_scaled,crim_anal_norm-1.ipynb",
  "row_id": 21,
  "target_code": "crim_anal['\uac15\uac04\uac80\uac70\uc728'       ]=crim_anal['\uac15\uac04 \uac80\uac70'       ]/crim_anal['\uac15\uac04 \ubc1c\uc0dd'       ]*100\ncrim_anal.head()",
  "generation": "crim_anal['\uac15\uac04\uac80\uac70\uc728'] = crim_anal['\uac15\uac04 \uac80\uac70'] / crim_anal['\uac15\uac04 \ubc1c\uc0dd'] * 100\ncrim_anal.head()"
 },
 {
  "nbid": "NB_603565",
  "file": "NB_603565,crim_anal,x_scaled,crim_anal_norm-1.ipynb",
  "row_id": 22,
  "target_code": "crim_anal['\uac15\ub3c4\uac80\uac70\uc728'       ]=crim_anal['\uac15\ub3c4 \uac80\uac70'       ]/crim_anal['\uac15\ub3c4 \ubc1c\uc0dd'       ]*100\ncrim_anal['\uc0b4\uc778\uac80\uac70\uc728'       ]=crim_anal['\uc0b4\uc778 \uac80\uac70'       ]/crim_anal['\uc0b4\uc778 \ubc1c\uc0dd'       ]*100\ncrim_anal['\uc808\ub3c4\uac80\uac70\uc728'       ]=crim_anal['\uc808\ub3c4 \uac80\uac70'       ]/crim_anal['\uc808\ub3c4 \ubc1c\uc0dd'       ]*100\ncrim_anal['\ud3ed\ub825\uac80\uac70\uc728'       ]=crim_anal['\ud3ed\ub825 \uac80\uac70'       ]/crim_anal['\ud3ed\ub825 \ubc1c\uc0dd'       ]*100\ncrim_anal.head()",
  "generation": "crim_anal['\uac15\ub3c4\uac80\uac70\uc728']=crim_anal['\uac15\ub3c4 \uac80\uac70']/crim_anal['\uac15\ub3c4 \ubc1c\uc0dd']*100\ncrim_anal['\uc0b4\uc778\uac80\uac70\uc728']=crim_anal['\uc0b4\uc778 \uac80\uac70']/crim_anal['\uc0b4\uc778 \ubc1c\uc0dd']*100\ncrim_anal['\uc808\ub3c4\uac80\uac70\uc728']=crim_anal['\uc808\ub3c4 \uac80\uac70']/crim_anal['\uc808\ub3c4 \ubc1c\uc0dd']*100\ncrim_anal['\ud3ed\ub825\uac80\uac70\uc728']=crim_anal['\ud3ed\ub825 \uac80\uac70']/crim_anal['\ud3ed\ub825 \ubc1c\uc0dd']*100\ncrim_anal"
 },
 {
  "nbid": "NB_655825",
  "file": "NB_655825,auto-1.ipynb",
  "row_id": 9,
  "target_code": "auto = auto.drop('car_ID'        , axis=1)\nauto.head()",
  "generation": "auto.drop(['car_ID'], axis = 1, inplace = True)\nauto.rename(columns={'CarName':'Car_Name'}, inplace=True)\nauto['Car_Manufacturer'] = auto['Car_Name'].apply(lambda x : x.split(\" \")[0])\nauto.drop(['Car_Name'], axis = 1, inplace = True)\nauto.head()"
 },
 {
  "nbid": "NB_655825",
  "file": "NB_655825,auto-1.ipynb",
  "row_id": 10,
  "target_code": "auto['CarName'         ] = auto['CarName'         ].str.split(' '   ).str.get(0)\nauto.rename(columns={'CarName'         : 'maker'        }, inplace=True)\nauto.head()",
  "generation": "auto['maker'] = auto['CarName'].apply(lambda x: x.split(' ')[0])\nauto = auto.drop('CarName', axis=1)\nauto = auto.rename(columns={'carbody': 'body_style', 'drivewheel': 'drive_wheels', 'enginelocation': 'engine_location', 'wheelbase': 'wheel_base', 'enginesize': 'engine_size', 'fuelsystem': 'fuel_system', 'boreratio': 'bore_ratio', 'compressionratio': 'compression_ratio', 'citympg': 'city_mpg', 'highwaympg': 'highway_mpg'})\nauto.head()"
 },
 {
  "nbid": "NB_658290",
  "file": "NB_658290,datos-1.ipynb",
  "row_id": 2,
  "target_code": "datos = datos.drop(['NO (ug/m3)'            ,'SH2 (ug/m3)'             , 'PST (ug/m3)'              ],  axis=1 )\ndatos.head()",
  "generation": "datos = datos[['CO (mg/m3)', 'NO2 (ug/m3)', 'O3 (ug/m3)', 'PM10 (ug/m3)', 'PM25 (ug/m3)', 'SO2 (ug/m3)', 'PROVINCIA', 'ESTACI\u00d3N', 'DIA']]\ndatos.head()"
 },
 {
  "nbid": "NB_658290",
  "file": "NB_658290,datos-1.ipynb",
  "row_id": 5,
  "target_code": "datos['CO (mg/m3)'            ]=pd.to_numeric(datos['CO (mg/m3)'            ])*1000\ndatos.rename(columns={'CO (mg/m3)'            :'CO (ug/m3)'            }, inplace=True)\ndatos.head()",
  "generation": "datos['CO (ug/m3)'] = datos['CO (mg/m3)'] * 1145\ndatos = datos.drop(['CO (mg/m3)'], axis=1)\ndatos.head()"
 },
 {
  "nbid": "NB_658290",
  "file": "NB_658290,datos-1.ipynb",
  "row_id": 6,
  "target_code": "datos['CO (ug/m3)'            ]=pd.to_numeric(datos['CO (ug/m3)'            ])/ 1.145\ndatos['NO2 (ug/m3)'             ]=pd.to_numeric(datos['NO2 (ug/m3)'             ])/ 1.88\ndatos['O3 (ug/m3)'            ]=pd.to_numeric(datos['O3 (ug/m3)'            ])/ 2.00\ndatos['SO2 (ug/m3)'             ]=pd.to_numeric(datos['SO2 (ug/m3)'             ])/ 2.62\ndatos.rename(columns={'CO (ug/m3)'            :'CO (ppm)'          }, inplace=True)\ndatos.rename(columns={'NO2 (ug/m3)'             :'NO2 (ppm)'           }, inplace=True)\ndatos.rename(columns={'O3 (ug/m3)'            :'O3 (ppm)'          }, inplace=True)\ndatos.rename(columns={'SO2 (ug/m3)'             :'SO2 (ppm)'           }, inplace=True)\ndatos['PM10 (ug/m3)'              ]= pd.to_numeric(datos['PM10 (ug/m3)'              ])\ndatos['PM25 (ug/m3)'              ]= pd.to_numeric(datos['PM25 (ug/m3)'              ])\ndatos.head()",
  "generation": "datos['CO (ppm)'] = pd.to_numeric(datos['CO (ug/m3)']) / 1145\ndatos['NO2 (ppm)'] = pd.to_numeric(datos['NO2 (ug/m3)']) / 1880\ndatos['O3 (ppm)'] = pd.to_numeric(datos['O3 (ug/m3)']) / 2000\ndatos['SO2 (ppm)'] = pd.to_numeric(datos['SO2 (ug/m3)']) / 2620\ndatos = datos.drop(['CO (ug/m3)', 'NO2 (ug/m3)', 'O3 (ug/m3)', 'SO2 (ug/m3)'], axis=1)\ndatos.head()"
 },
 {
  "nbid": "NB_658290",
  "file": "NB_658290,datos-1.ipynb",
  "row_id": 8,
  "target_code": "datos['IGCA'      ] = datos[['IPCO'      ,'IPNO2'       ,'IPO3'      , 'IPPM10'         , 'IPPM25'         , 'IPSO2'        ]].max(axis=1)\ndatos.head()",
  "generation": "datos['IGCA'] = datos[['IPCO', 'IPNO2', 'IPO3', 'IPPM10', 'IPPM25', 'IPSO2']].max(axis=1)\ndatos"
 },
 {
  "nbid": "NB_658737",
  "file": "NB_658737,df1-1.ipynb",
  "row_id": 47,
  "target_code": "df1 = df1.drop(['consulate_country'                   ], axis = 1)\ndf1.head()",
  "generation": "df1.drop(['consulate_country'], axis=1, inplace=True)\ndf1['accepted'] = df1['total_uv+mev_issued'] - df1['uv_unissued']\ndf1.head()"
 },
 {
  "nbid": "NB_658737",
  "file": "NB_658737,df2-1.ipynb",
  "row_id": 76,
  "target_code": "df2 = df2.drop(['consulate_country'                   ], axis = 1)\ndf2.head()",
  "generation": "df2 = df[df['consulate_country'] == 'INDIA']\ndf2 = df2.drop(['consulate_country'], axis=1)\ndf2 = df2[['sch_state', 'consulate', 'uv_applied', 'total_uv+mev_issued', 'mev_issued', 'shared_mev_issued', 'LTVs_issued', 'uv_unissued', 'unissued_uv_rate', 'decisions', 'rejection_rate']]\ndf2['accepted'] = df2['total_uv+mev_issued']\ndf2.head()"
 },
 {
  "nbid": "NB_658737",
  "file": "NB_658737,df3-1.ipynb",
  "row_id": 103,
  "target_code": "df3 = df3.drop(['consulate_country'                   ], axis = 1)\ndf3.head()",
  "generation": "df3 = df3.drop(columns=['consulate_country'])\ndf3 = df3.rename(columns={'consulate': 'consulate_country'})\ndf3.head()"
 },
 {
  "nbid": "NB_2106695",
  "file": "NB_2106695,df-1.ipynb",
  "row_id": 14,
  "target_code": "df['Bag_of_words'              ] = ''   \ncolumns = ['Genre'       , 'Director'           , 'Cast'       , 'Key_words'            ]\nfor index, row in df.iterrows():\n    words = ''   \n    for col in columns:\n        words += ' '    .join(row[col]) + ' '    \n    row['Bag_of_words'              ] = words\ndf = df[['Movie Name'            ,'Bag_of_words'              ]]\ndf",
  "generation": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\ndf['Bag_of_words'] = df['Genre'] + df['Director'] + df['Cast'] + df['Plot']\ndf['Bag_of_words'] = df['Bag_of_words'].apply(lambda x: ' '.join(x))\ndf['Bag_of_words'] = vectorizer.fit_transform(df['Bag_of_words']).toarray()\ndf = df[['Movie Name', 'Bag_of_words']]\ndf.head()"
 },
 {
  "nbid": "NB_2114389",
  "file": "NB_2114389,df-1.ipynb",
  "row_id": 5,
  "target_code": "df= df.rename(columns={1:'clump_thickness'                 , 2:'uniformity_of_cell_size'                         , 3:'uniformity_of_cell_shape'                          ,\n                  4:'marginal_adhesion'                   , 5:'single_epithelial_cell_size'                             ,6:'bare_nuclei'             ,\n                   7:'bland_chromatin'                 , 8: 'normal_nucleoli'                  , 9:'mitoses'         ,\n                   10:'class'       })\ndf.head()",
  "generation": "df.columns = ['clump_thickness', 'uniformity_of_cell_size', 'uniformity_of_cell_shape', 'marginal_adhesion', 'single_epithelial_cell_size', 'bare_nuclei', 'bland_chromatin', 'normal_nucleoli', 'mitoses', 'class']\ndf.head()"
 },
 {
  "nbid": "NB_2114694",
  "file": "NB_2114694,df-1.ipynb",
  "row_id": 75,
  "target_code": "df['highway-mpg'             ] = 235/df['highway-mpg'             ]\ndf.rename(columns={'highway-mpg'             :'highway-L/100km'                 },inplace=True)\ndf.head()",
  "generation": "df['highway-L/100km'] = 235/df[\"highway-mpg\"]\ndf.rename(columns={'highway-L/100km': 'highway-L/100km.1'}, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_2116197",
  "file": "NB_2116197,data_mar-1.ipynb",
  "row_id": 14,
  "target_code": "data_mar = data_mar[data_mar[\"FECHA_TOMA_DATO\"                 ]>\"2017-03\"         ]\ndata_mar.head()",
  "generation": "data_mar = data_mar[pd.to_datetime(data_mar['FECHA_TOMA_DATO']).dt.month == 3]\ndata_mar = data_mar[data_mar['ENERGIA_ACTIVA_ACUMULADA'] != 0]"
 },
 {
  "nbid": "NB_2122495",
  "file": "NB_2122495,data-1.ipynb",
  "row_id": 7,
  "target_code": "data = data.drop(categ_col.columns, axis =1)\ndata= pd.concat([data,categ_col], axis =1 )\ndata.head()",
  "generation": "la_en = preprocessing.LabelEncoder()\ndata['month'] = la_en.fit_transform(data['month'])\ndata['day'] = la_en.fit_transform(data['day'])\ndata.head()"
 },
 {
  "nbid": "NB_2128766",
  "file": "NB_2128766,passengers-1.ipynb",
  "row_id": 5,
  "target_code": "passengers.dropna(inplace=True)\npassengers.index = pd.to_datetime(passengers.index)\npassengers.head()",
  "generation": "passengers.index = pd.to_datetime(passengers.index, format='%Y-%m')\npassengers.head()"
 },
 {
  "nbid": "NB_2130029",
  "file": "NB_2130029,df-1.ipynb",
  "row_id": 6,
  "target_code": "df.columns = [clean_columns(col) for col in df.columns]\ndf.head()",
  "generation": "df.columns = df.columns.str.lower().str.replace(' ', '_')\ndf.head()"
 },
 {
  "nbid": "NB_2131562",
  "file": "NB_2131562,df1-1.ipynb",
  "row_id": 11,
  "target_code": "df1['Date'      ]=pd.to_datetime(df1['Date'      ])\ndf1.head()",
  "generation": "df1['Date'] = pd.to_datetime(df1['Date'])\ndf1.head()"
 },
 {
  "nbid": "NB_2131562",
  "file": "NB_2131562,df1-1.ipynb",
  "row_id": 14,
  "target_code": "df1=df1.set_index('Date'      )\ndf1.head()",
  "generation": "df1 = df1[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\ndf1.head()"
 },
 {
  "nbid": "NB_2137973",
  "file": "NB_2137973,df,y-1.ipynb",
  "row_id": 2,
  "target_code": "df = df.drop(['Index'       ], axis=1)\ndf.head()",
  "generation": "df = df[['Unnamed: 0', 'Height(Inches)', 'Weight(Pounds)']]\ndf.head()"
 },
 {
  "nbid": "NB_3246332",
  "file": "NB_3246332,df_idx-1.ipynb",
  "row_id": 5,
  "target_code": "df_idx = df_idx.sort_index(axis=1, ascending=True)\ndf_idx = df_idx.iloc[::-1]\ndf_idx.head(5)",
  "generation": "df_flipped = df_idx.iloc[::-1]\ndf_flipped = df_flipped[['Change %', 'High', 'Low', 'Open', 'Price']]\ndf_flipped.head(5)"
 },
 {
  "nbid": "NB_3249794",
  "file": "NB_3249794,data,y_train,x_val,y_val-1.ipynb",
  "row_id": 5,
  "target_code": "data['cdate'       ]=pd.to_datetime(data['cdate'       ])\ndata.head()",
  "generation": "data['cdate'] = pd.to_datetime(data['cdate'], format='%m/%d/%Y')\ndata.rename(columns={'\u0915\u0943\u0937\u093f \u0909\u092a\u091c': 'Vegetable', '\u0908\u0915\u093e\u0907': 'Quantity', '\u0928\u094d\u092f\u0942\u0928\u0924\u092e': 'MinPrice', '\u0905\u0927\u093f\u0915\u0924\u092e': 'MaxPrice', '\u0914\u0938\u0924': 'Average'}, inplace=True)\ndata"
 },
 {
  "nbid": "NB_3250306",
  "file": "NB_3250306,df-1.ipynb",
  "row_id": 75,
  "target_code": "df.rename(columns={\"highway-mpg\"             :'highway-L/100km'                 }, inplace=True)\ndf[\"highway-L/100km\"                 ] = 235/df[\"highway-L/100km\"                 ]\ndf.head()",
  "generation": "df['highway-L/100km'] = 235/df[\"highway-mpg\"]\ndf.head(5)"
 },
 {
  "nbid": "NB_3254439",
  "file": "NB_3254439,trends-1.ipynb",
  "row_id": 7,
  "target_code": "for col in trends.columns:\n    if \"<\"     in trends[col].to_string():         \n        trends[col]=trends[col].str.replace('<'   ,''  )\n        trends[col]=pd.to_numeric(trends[col])\ntrends.info()\ntrends.head()",
  "generation": "for col in trends.columns[2:]:\n    trends[col] = trends[col].apply(lambda x: int(x.replace('<', '')) if isinstance(x, str) else x)\ntrends.rename(columns={'Month': 'month', 'Kim Kardashian: (Worldwide)': 'kim', 'Khlo\u00e9 Kardashian: (Worldwide)': 'khloe', 'Kourtney Kardashian: (Worldwide)': 'kourtney', 'Kendall Jenner: (Worldwide)': 'kendall', 'Kylie Jenner: (Worldwide)': 'kylie'}, inplace=True)\ntrends.head()"
 },
 {
  "nbid": "NB_3271432",
  "file": "NB_3271432,df12-1.ipynb",
  "row_id": 4,
  "target_code": "df12['Ndate'       ] = pd.to_datetime(df12['Date'      ], format='%d %m %Y'          )\ndf12['month'       ] = df12['Ndate'       ].dt.month\ndf12.head()",
  "generation": "df12[\"Ndate\"] = pd.to_datetime(df12[\"Date\"], format='%d %m %Y')\ndf12[\"month\"] = df12[\"Ndate\"].dt.month\ndf12.head()"
 },
 {
  "nbid": "NB_3276661",
  "file": "NB_3276661,df_train-1.ipynb",
  "row_id": 24,
  "target_code": "df_train['Sex'     ] = df_train['Sex'     ].map(lambda x: 1 if x=='male'       else 0)\ndf_train['Embarked'          ] = df_train['Embarked'          ].map({'S'   :1, 'C'    :2, 'Q'    :3})\ndf_train.head()",
  "generation": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf_train['Sex'] = le.fit_transform(df_train['Sex'])\ndf_train['Embarked'] = le.fit_transform(df_train['Embarked'].astype(str))\ndf_train.head()"
 },
 {
  "nbid": "NB_3276661",
  "file": "NB_3276661,test-1.ipynb",
  "row_id": 28,
  "target_code": "test = test.drop(['PassengerId'             , 'Name'       , 'Ticket'         ,'Cabin'       ], axis=1)\ntest.head()",
  "generation": "test = test[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\ntest.head()"
 },
 {
  "nbid": "NB_3276661",
  "file": "NB_3276661,test-1.ipynb",
  "row_id": 33,
  "target_code": "test['Sex'     ] = test['Sex'     ].map(lambda x: 1 if x=='male'       else 0)\ntest['Embarked'          ] = test['Embarked'          ].map({'S'   :1, 'C'    :2, 'Q'    :3})\ntest.head()",
  "generation": "test['Sex'] = test['Sex'].map({'female': 0, 'male': 1}).astype(int)\ntest['Embarked'] = test['Embarked'].map({'S': 1, 'C': 2, 'Q': 3})\ntest.head()"
 },
 {
  "nbid": "NB_3286055",
  "file": "NB_3286055,df,covariates-1.ipynb",
  "row_id": 13,
  "target_code": "df[\"budget\"        ] = df[\"budget\"        ].apply(lambda x: np.log10(1+x))\ndf[\"popularity\"            ] = df[\"popularity\"            ].apply(lambda x: np.log10(1+x))\ndf[\"runtime\"         ] = df[\"runtime\"         ].apply(lambda x: np.log10(1+x))\ndf[\"vote_count\"            ] = df[\"vote_count\"            ].apply(lambda x: np.log10(1+x))\ndf[\"revenue\"         ] = df[\"revenue\"         ].apply(lambda x: np.log10(1+x))\ndf.head()",
  "generation": "df[['budget', 'popularity', 'runtime', 'vote_count', 'revenue']] = df[['budget', 'popularity', 'runtime', 'vote_count', 'revenue']].replace(0, 0.0001)\ndf[['budget', 'popularity', 'runtime', 'vote_count', 'revenue']] = np.log10(df[['budget', 'popularity', 'runtime', 'vote_count', 'revenue']])\ndf.head()"
 },
 {
  "nbid": "NB_3289035",
  "file": "NB_3289035,star_wars-1.ipynb",
  "row_id": 8,
  "target_code": "seen_cols = ['seen_1'        , 'seen_2'         , 'seen_3'         , 'seen_4'         , 'seen_5'         , 'seen_6'         ]\nseen_map = {\n        \"Star Wars: Episode I  The Phantom Menace\"                                                  : True,\n        \"Star Wars: Episode II  Attack of the Clones\"                                                     : True,\n        \"Star Wars: Episode III  Revenge of the Sith\"                                                     : True,\n        \"Star Wars: Episode IV  A New Hope\"                                           : True,\n        \"Star Wars: Episode V The Empire Strikes Back\"                                                      : True,\n        \"Star Wars: Episode VI Return of the Jedi\"                                                  : True,\n        np.NaN: False\n    }\nfor col in seen_cols:\n    star_wars[col] = bool_map(star_wars[col], seen_map)\nstar_wars.head() ",
  "generation": "bool_map = lambda x, m: x.map(m)\n\nfor col in star_wars.columns[1:3]:\n    star_wars[col] = bool_map(star_wars[col], yn_map)\n\nfor col in star_wars.columns[3:9]:\n    star_wars[col] = bool_map(star_wars[col], {np.nan: False, \"Star Wars: Episode I The Phantom Menace\": True, \"Star Wars: Episode II Attack of the Clones\": True, \"Star Wars: Episode III Revenge of the Sith\": True, \"Star Wars: Episode IV A New Hope\": True, \"Star Wars: Episode V The Empire Strikes Back\": True, \"Star Wars: Episode VI Return of the Jedi\": True})\n\nstar_wars.head()"
 },
 {
  "nbid": "NB_575260",
  "file": "NB_575260,sms,y_c-1.ipynb",
  "row_id": 5,
  "target_code": "sms = sms.drop(['Unnamed: 2'            , 'Unnamed: 3'             , 'Unnamed: 4'             ], axis=1)\nsms.columns = ['label'       , 'texts'        ]\nprint(sms.shape)\nsms.head()",
  "generation": "sms = sms.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\nsms = sms.rename(columns={'Unnamed: 0': 'label', 'v2': 'texts'})\nsms"
 },
 {
  "nbid": "NB_578192",
  "file": "NB_578192,df-1.ipynb",
  "row_id": 20,
  "target_code": "num_df = df.rename(columns={\"cp\"    : \"chest pain\"             , \"trestbps\"           : \"resting blood pressure\"                         , \"chol\"       :\"cholesterol\"             , \"fbs\"      :\"fasting blood sugar\"                     , \"thalach\"          :\"max heart rate\"                , \"exang\"        :\"exercise angina\"                 , \"ca\"     :\"major vessels\"               })\nsex_dict = {0: \"female\"         , 1:\"male\"      }\ndf[\"sex\"     ] = df[\"sex\"     ].map(sex_dict)\ncp_dict = {0: \"asymptomatic\"               , 1:\"atypical angina\"                 , 2:\"non-anginal pain\"                  , 3:\"typical angina\"                }\ndf[\"cp\"    ] = df[\"cp\"    ].map(cp_dict)\nrestecg_dict = {0: \"left ventricular hypertrophy\"                               , 1:\"normal\"        , 2:\"ST-T wave abnormality\"                       }\ndf[\"restecg\"         ] = df[\"restecg\"         ].map(restecg_dict)\ntarget_dict = {0: \"disease\"          , 1:\"no disease\"            }\ndf[\"target\"        ] = df[\"target\"        ].map(target_dict)\ndf = df.rename(columns={\"cp\"    : \"chest pain\"             , \"trestbps\"           : \"resting blood pressure\"                         , \"chol\"       :\"cholesterol\"             , \"fbs\"      :\"fasting blood sugar\"                     , \"thalach\"          :\"max heart rate\"                , \"exang\"        :\"exercise angina\"                 , \"ca\"     :\"major vessels\"               })\ndf",
  "generation": "df = df.rename(columns={'cp': 'chest pain', 'trestbps': 'resting blood pressure', 'chol': 'cholesterol', 'fbs': 'fasting blood sugar', 'restecg': 'restecg', 'thalach': 'max heart rate', 'exang': 'exercise angina', 'oldpeak': 'oldpeak', 'slope': 'slope', 'ca': 'major vessels', 'thal': 'thal', 'target': 'target'})\ndf['sex'] = df['sex'].replace({0: 'female', 1: 'male'})\ndf['chest pain'] = df['chest pain'].replace({1: 'typical angina', 2: 'atypical angina', 3: 'non-anginal pain', 4: 'asymptomatic'})\ndf['fasting blood sugar'] = df['fasting blood sugar'].replace({0: 'normal', 1: 'prediabetic/diabetic'})\ndf['restecg'] = df['restecg'].replace({0: 'normal', 1: 'left ventricular hypertrophy', 2: 'ST-T wave abnormality'})\ndf['exercise angina'] = df['exercise angina'].replace({0: 'no', 1: 'yes'})\ndf['target'] = df['target'].replace({0: 'no disease', 1: 'disease'})\ndf.head()"
 },
 {
  "nbid": "NB_582724",
  "file": "NB_582724,rdf-1.ipynb",
  "row_id": 15,
  "target_code": "rdf.columns = ['Scenario'          , 'Year'       , 'ISO'      , 'Gini'       , 'Region'         ]\nrdf = rdf.set_index('Scenario'          )\nrdf.head()",
  "generation": "rdf = rdf.set_index(['scenario', 'year', 'iso']).sort_index()\nrdf.head()"
 },
 {
  "nbid": "NB_582886",
  "file": "NB_582886,crime_norm-1.ipynb",
  "row_id": 52,
  "target_code": "col2 = ['\uac15\uac04\uac80\uac70\uc728'       , '\uac15\ub3c4\uac80\uac70\uc728'        , '\uc0b4\uc778\uac80\uac70\uc728'        , '\uc808\ub3c4\uac80\uac70\uc728'        , '\ud3ed\ub825\uac80\uac70\uc728'        ]\ncrime_norm['\uac80\uac70'    ] = np.sum(crime_norm[col], axis = 1)\ncrime_norm.head()",
  "generation": "crime_norm['\uac80\uac70'] = np.sum(crime_norm[col2], axis = 1)\ncrime_norm.head()"
 },
 {
  "nbid": "NB_615694",
  "file": "NB_615694,data-1.ipynb",
  "row_id": 34,
  "target_code": "data['tidy_tweet'            ] = data['tidy_tweet'            ].str.replace(\"[^a-zA-Z#]\"            , \" \"    ) \ndata.head(10)",
  "generation": "import string\ndef remove_punctuations(text):\n    for punctuation in string.punctuation:\n        text = text.replace(punctuation, '')\n    return text\n\ndata['tidy_tweet'] = data['tidy_tweet'].apply(remove_punctuations)\ndata.head()"
 },
 {
  "nbid": "NB_615903",
  "file": "NB_615903,qter_empl-1.ipynb",
  "row_id": 9,
  "target_code": "qter_empl = qter_empl.loc[qter_empl.index[qter_empl.index <= \"2019M08\"          ], :]\nqter_empl = qter_empl[qter_empl.columns[qter_empl.columns.str.contains(\"TOTAL,SAL,TOTAL,THS\"                     )]]\nqter_empl.columns = qter_empl.columns.str.replace(\"TOTAL,SAL,TOTAL,THS,\"                      , \"\"   )\nqter_empl = qter_empl.applymap(parsing_function)\nqter_empl",
  "generation": "qter_empl = qter_empl.loc['TOTAL','SAL','TOTAL',:].dropna(axis=1)\nqter_empl = qter_empl.loc[:,~qter_empl.columns.duplicated()]\nqter_empl = qter_empl/1000\nqter_empl"
 },
 {
  "nbid": "NB_6934069",
  "file": "NB_6934069,df-1.ipynb",
  "row_id": 1,
  "target_code": "df=df.iloc[:, 1:]\ndf.head(3)",
  "generation": "df = df.drop(['User ID'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_6934069",
  "file": "NB_6934069,df-1.ipynb",
  "row_id": 2,
  "target_code": "df['Gender'        ] = np.where(df['Gender'        ]=='Male'      , 1, 0)\ndf.head(3)",
  "generation": "df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)\ndf"
 },
 {
  "nbid": "NB_7074600",
  "file": "NB_7074600,small,target-1.ipynb",
  "row_id": 2,
  "target_code": "small['S'   ]=small['X'   ]+small['Y'   ]+small['Z'   ]\nsmall.head(10)",
  "generation": "model = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(small[features], small[target])\nsmall['S'] = model.predict(small[features])\nsmall = small[['Unnamed: 0', 'ppn', 'sizeGB', 'Y_time', 'S']]\nsmall.head()"
 },
 {
  "nbid": "NB_7082729",
  "file": "NB_7082729,data-1.ipynb",
  "row_id": 4,
  "target_code": "data = data.loc[:,~data.columns.str.contains('^Unnamed'          )]\ndata.head()",
  "generation": "data = data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\ndata.head()"
 },
 {
  "nbid": "NB_7082729",
  "file": "NB_7082729,data-1.ipynb",
  "row_id": 5,
  "target_code": "data = data.rename(columns = {'v1'    :'label'       ,'v2'    :'sms_message'             }) \ndata.head()",
  "generation": "data = data.rename(columns={'v1': 'label', 'v2': 'sms_message'})\ndata.head()"
 },
 {
  "nbid": "NB_7082729",
  "file": "NB_7082729,data-1.ipynb",
  "row_id": 6,
  "target_code": "data['label'       ] = data.label.map({'ham'     :0,'spam'      :1})\ndata.head()",
  "generation": "data['label'] = data['label'].map({'ham': 0, 'spam': 1})\ndata.head()"
 },
 {
  "nbid": "NB_2155586",
  "file": "NB_2155586,df-1.ipynb",
  "row_id": 3,
  "target_code": "df = df.drop(\"Address\"         , axis=1)\ndf.head()",
  "generation": "df.drop('Address', axis=1, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_2165279",
  "file": "NB_2165279,dtf-1.ipynb",
  "row_id": 3,
  "target_code": "dtf = dtf.drop(['Province/State'                ,'Lat'     ,'Long'      ], axis=1).groupby(\"Country/Region\"                ).sum().T\ndtf.index = pd.to_datetime(dtf.index, infer_datetime_format=True)\ndtf = pd.DataFrame(index=dtf.index, data=dtf[\"United Kingdom\"                ].values, columns=[\"total\"       ])\ndtf[\"new\"     ] = dtf[\"total\"       ] - dtf[\"total\"       ].shift(1)\ndtf[\"new\"     ] = dtf[\"new\"     ].fillna(method='bfill'       )\ndtf.head()",
  "generation": "dtf = dtf.groupby(\"Country/Region\").sum().T\ndtf.index = pd.to_datetime(dtf.index)\ndtf[\"total\"] = dtf.sum(axis=1)\ndtf[\"new\"] = dtf[\"total\"].diff().fillna(0)\ndtf.head()"
 },
 {
  "nbid": "NB_2169332",
  "file": "NB_2169332,titanic-1.ipynb",
  "row_id": 6,
  "target_code": "titanic = titanic.assign(Sex = titanic.Sex.map({\"female\"        :0, \"male\"       :1}))\ntitanic.head()",
  "generation": "titanic['Sex'] = titanic['Sex'].map({'male': 1, 'female': 0})\ntitanic"
 },
 {
  "nbid": "NB_2178698",
  "file": "NB_2178698,df,df_bull-1.ipynb",
  "row_id": 6,
  "target_code": "df['MaxUp'       ] = ((df['High'      ] - df['Open'      ])/df['Open'      ]*100).round(3)\ndf['MaxDown'         ] = ((df['Low'     ] - df['Open'      ])/df['Open'      ]*100).round(3)\ndf.head()",
  "generation": "df['MaxUp'] = df['High'].sub(df['Open']).div(df['Open']).round(3)\ndf['MaxDown'] = df['Low'].sub(df['Open']).div(df['Open']).round(3)\ndf.head()"
 },
 {
  "nbid": "NB_2178754",
  "file": "NB_2178754,df-1.ipynb",
  "row_id": 41,
  "target_code": "df['Total Spent Mins'                  ] = df[\"Total day minutes\"                   ]+df[\"Total eve minutes\"                   ]+                            df[\"Total night minutes\"                     ]+df[\"Total intl minutes\"                    ]\ndf.head()",
  "generation": "df[\"Total Spent Mins\"] = df[\"Total day minutes\"] + df[\"Total eve minutes\"] + df[\"Total night minutes\"] + df[\"Total intl minutes\"]\nprint(df.head())"
 },
 {
  "nbid": "NB_2189275",
  "file": "NB_2189275,data-1.ipynb",
  "row_id": 17,
  "target_code": "data['Time'      ] = pd.to_datetime(data['Time'      ])\ndata = data.set_index('Time'      )\ndata",
  "generation": "data = data.set_index('Time')\ndata.columns = pd.MultiIndex.from_product([data.columns,['']])\ndata.index.name = 'Unnamed: 0_level_0'\ndata.reset_index(inplace=True)\ndata.head()"
 },
 {
  "nbid": "NB_2190108",
  "file": "NB_2190108,Y_pp-1.ipynb",
  "row_id": 54,
  "target_code": "Y_pp['pred_class_thresh10'                     ] = [1 if x >= 0.10 else 0 for x in Y_pp.class_1_pp.values]\nY_pp.head(10)",
  "generation": "Y_pp['pred_class_thresh10'] = [1 if x >= 0.1 else 0 for x in Y_pp['class_1_pp']]\nconfusion_matrix(y_test, Y_pp['pred_class_thresh10'])"
 },
 {
  "nbid": "NB_2190271",
  "file": "NB_2190271,df2-1.ipynb",
  "row_id": 16,
  "target_code": "t1=df2[\"Time [s]\"          ][0]\np_prima=16\ntp_prima=df2[\"Time [s]\"          ][1]\ndf2[\"Sp\"    ]= aceleracion(t1,df2[\"Time [s]\"          ])\ndf2[\"Eff\"     ]=eficiencia(df2[\"Sp\"    ], df2[\"Procesadores\"              ])\ndf2[\"Cp\"    ]=costo(df2[\"Procesadores\"              ], df2[\"Time [s]\"          ])\ndf2.head()",
  "generation": "df.loc[0,\"Time [s]\"]=430\ndf['Eff_r'] = eficienciaRelativa(p_prima, df['Sp_r'], df['Procesadores'])\ndf.head()"
 },
 {
  "nbid": "NB_2195536",
  "file": "NB_2195536,df-1.ipynb",
  "row_id": 4,
  "target_code": "i=0\nwhile i < 55:\n    test_na=list_state[i]\n    state_calculate=df[df.state==test_na].mean()\n    df[df.state==test_na]=df.fillna(state_calculate)\n    i=i+1\ndf.fillna(0, inplace= True)\ndf=df.round(0)\ndf.head(20)",
  "generation": "for state in list_state:\n    mean_permit=df[df['state']==state]['permit'].mean()\n    mean_permit_recheck=df[df['state']==state]['permit_recheck'].mean()\n    mean_handgun=df[df['state']==state]['handgun'].mean()\n    mean_long_gun=df[df['state']==state]['long_gun'].mean()\n    mean_other=df[df['state']==state]['other'].mean()\n    mean_multiple=df[df['state']==state]['multiple'].mean()\n    mean_admin=df[df['state']==state]['admin'].mean()\n    mean_prepawn_handgun=df[df['state']==state]['prepawn_handgun'].mean()\n    mean_returned_other=df[df['state']==state]['returned_other'].mean()\n    mean_rentals_handgun=df[df['state']==state]['rentals_handgun'].mean()\n    mean_rentals_long_gun=df[df['state']==state]['rentals_long_gun'].mean()\n    mean_private_sale_handgun=df[df['state']==state]['private_sale_handgun'].mean()\n    mean_private_sale_long_gun=df[df['state']==state]['private_sale_long_gun'].mean()\n    mean_private_sale_other=df[df['state']==state]['private_sale_other'].mean()\n    mean_return_to_seller_handgun=df[df['state']==state]['return_to_seller_handgun'].mean()\n    mean_return_to_seller_long_gun=df[df['state']==state]['return_to_seller_long_gun'].mean()\n    mean_return_to_seller_other=df[df['state']==state]['return_to_seller_other'].mean()\n    df.loc[df['state']==state,'permit'] = df.loc[df['state']==state,'permit'].fillna(mean_permit)\n    df.loc[df['state']==state,'permit_recheck'] = df.loc[df['state']==state,'permit_recheck'].fillna(mean_permit_recheck)\n    df.loc[df['state']==state,'handgun'] = df.loc[df['state']==state,'handgun'].fillna(mean_handgun)\n    df.loc[df['state']==state,'long_gun'] = df.loc[df['state']==state,'long_gun'].fillna(mean_long_gun)\n    df.loc[df['state']==state,'other'] = df.loc[df['state']==state,'other'].fillna(mean_other)\n    df.loc[df['state']==state,'multiple'] = df.loc[df['state']==state,'multiple'].fillna(mean_multiple)\n    df.loc[df['state']==state,'admin'] = df.loc[df['state']==state,'admin'].fillna(mean_admin)\n    df.loc[df['state']==state,'prepawn_handgun'] = df.loc[df['state']==state,'prepawn_handgun'].fillna(mean_prepawn_handgun)\n    df.loc[df['state']==state,'returned_other'] = df.loc[df['state']==state,'returned_other'].fillna(mean_returned_other)\n    df.loc[df['state']==state,'rentals_handgun'] = df.loc[df['state']==state,'rentals_handgun'].fillna(mean_rentals_handgun)\n    df.loc[df['state']==state,'rentals_long_gun'] = df.loc[df['state']==state,'rentals_long_gun'].fillna(mean_rentals_long_gun)\n    df.loc[df['state']==state,'private_sale_handgun'] = df.loc[df['state']==state,'private_sale_handgun'].fillna(mean_private_sale_handgun)\n    df.loc[df['state']==state,'private_sale_long_gun'] = df.loc[df['state']==state,'private_sale_long_gun'].fillna(mean_private_sale_long_gun)\n    df.loc[df['state']==state,'private_sale_other'] = df.loc[df['state']==state,'private_sale_other'].fillna(mean_private_sale_other)\n    df.loc[df['state']==state,'return_to_seller_handgun'] = df.loc[df['state']==state,'return_to_seller_handgun'].fillna(mean_return_to_seller_handgun)\n    df.loc[df['state']==state,'return_to_seller_long_gun'] = df.loc[df['state']==state,'return_to_seller_long_gun'].fillna(mean_return_to_seller_long_gun)\n    df.loc[df['state']==state,'return_to_seller_other'] = df.loc[df['state']==state,'return_to_seller_other'].fillna(mean_return_to_seller_other)\ndf.fillna(0,inplace=True)\ndf.round(0)"
 },
 {
  "nbid": "NB_2199759",
  "file": "NB_2199759,above_40-1.ipynb",
  "row_id": 54,
  "target_code": "above_40 = above_40.count().to_frame()\nabove_40 = above_40.reset_index()\nabove_40 = above_40.rename(columns = {\"index\"       : \"Region\"         , 0:\"Above 40\"          })\nabove_40",
  "generation": "above_40 = explore_data[explore_data > 40]\nabove_40 = above_40.stack().reset_index()\nabove_40.columns = ['Unnamed: 0', 'Region', 'Above 40']\nabove_40 = above_40.groupby('Region').sum().reset_index()\nabove_40 = above_40[['Region', 'Above 40']]\nabove_40.head()"
 },
 {
  "nbid": "NB_2199759",
  "file": "NB_2199759,geny-1.ipynb",
  "row_id": 46,
  "target_code": "geny = geny.count().to_frame()\ngeny = geny.reset_index()\ngeny = geny.rename(columns = {\"index\"       : \"Region\"         , 0:\"NO.Y\"      })\ngeny",
  "generation": "geny = explore_data[(26 <= explore_data.index) & (explore_data.index <= 40)]\ngeny = geny.stack().reset_index()\ngeny.columns = ['Region', 'Generation', 'NO.Y']\ngeny = geny.groupby(['Region']).sum().reset_index()\ngeny = geny.sort_values(by=['NO.Y'], ascending=False)\ngeny = geny.head()\ngeny = geny.reset_index(drop=True)\ngeny"
 },
 {
  "nbid": "NB_2199759",
  "file": "NB_2199759,genz-1.ipynb",
  "row_id": 38,
  "target_code": "genz = genz.count().to_frame()\ngenz = genz.reset_index()\ngenz = genz.rename(columns = {\"index\"       : \"Region\"         , 0:\"NO.Z\"      })\ngenz",
  "generation": "explore_data['NO.Z'] = explore_data[explore_data < 26].count(axis=1)\ngenz = explore_data[['Region', 'NO.Z']].groupby('Region').sum().reset_index()\ngenz"
 },
 {
  "nbid": "NB_2200344",
  "file": "NB_2200344,crime-1.ipynb",
  "row_id": 45,
  "target_code": "crime['\uac15\uac04\uac80\uac70\uc728'       ]=crime['\uac15\uac04 \uac80\uac70'       ] / crime['\uac15\uac04 \ubc1c\uc0dd'       ] * 100\ncrime['\uac15\ub3c4\uac80\uac70\uc728'       ]=crime['\uac15\ub3c4 \uac80\uac70'       ] / crime['\uac15\ub3c4 \ubc1c\uc0dd'       ] * 100\ncrime['\uc0b4\uc778\uac80\uac70\uc728'       ]=crime['\uc0b4\uc778 \uac80\uac70'       ] / crime['\uc0b4\uc778 \ubc1c\uc0dd'       ] * 100\ncrime['\uc808\ub3c4\uac80\uac70\uc728'       ]=crime['\uc808\ub3c4 \uac80\uac70'       ] / crime['\uc808\ub3c4 \ubc1c\uc0dd'       ] * 100\ncrime['\ud3ed\ub825\uac80\uac70\uc728'       ]=crime['\ud3ed\ub825 \uac80\uac70'       ] / crime['\ud3ed\ub825 \ubc1c\uc0dd'       ] * 100\ncrime.head()",
  "generation": "crime['\uac15\uac04\uac80\uac70\uc728'] = crime['\uac15\uac04 \uac80\uac70'] / crime['\uac15\uac04 \ubc1c\uc0dd'] * 100\ncrime['\uac15\ub3c4\uac80\uac70\uc728'] = crime['\uac15\ub3c4 \uac80\uac70'] / crime['\uac15\ub3c4 \ubc1c\uc0dd'] * 100\ncrime['\uc0b4\uc778\uac80\uac70\uc728'] = crime['\uc0b4\uc778 \uac80\uac70'] / crime['\uc0b4\uc778 \ubc1c\uc0dd'] * 100\ncrime['\uc808\ub3c4\uac80\uac70\uc728'] = crime['\uc808\ub3c4 \uac80\uac70'] / crime['\uc808\ub3c4 \ubc1c\uc0dd'] * 100\ncrime['\ud3ed\ub825\uac80\uac70\uc728'] = crime['\ud3ed\ub825 \uac80\uac70'] / crime['\ud3ed\ub825 \ubc1c\uc0dd'] * 100\ncrime.drop(['\uac15\uac04 \uac80\uac70', '\uac15\ub3c4 \uac80\uac70', '\uc0b4\uc778 \uac80\uac70', '\uc808\ub3c4 \uac80\uac70', '\ud3ed\ub825 \uac80\uac70'], axis=1, inplace=True)\ncrime.head()"
 },
 {
  "nbid": "NB_2203133",
  "file": "NB_2203133,data_features_encoded-1.ipynb",
  "row_id": 21,
  "target_code": "nfeature_names = ['Amount'        ,'ProviderId'            , 'ProductCategory'                  ]\ndata_features_encoded=data_features_encoded[nfeature_names]\ndata_features_encoded.head()",
  "generation": "data_features = df[feature_names]\nd = defaultdict(LabelEncoder)\ndf_encoded = data_features[['ProviderId', 'ProductCategory']]\ndf_encoded = df_encoded.apply(lambda x: d[x.name].fit_transform(x))\ndata_features_encoded = pd.concat([data_features['Amount'], df_encoded], axis=1)\ndata_features_encoded"
 },
 {
  "nbid": "NB_2212020",
  "file": "NB_2212020,df-1.ipynb",
  "row_id": 25,
  "target_code": "d = {'No'    : False, 'Yes'      : True}\ndf = df.replace({'Voice mail plan'                 : d})\ndf.head()",
  "generation": "df['International plan'] = df['International plan'].replace({False: 'No', True: 'Yes'})\ndf.head()"
 },
 {
  "nbid": "NB_2213612",
  "file": "NB_2213612,data-1.ipynb",
  "row_id": 9,
  "target_code": "data = data.rename(columns={\"Rating (Num)\"              : \"Rating\"         , \"Schedule Pacing\"                  : \"Pace\"       })\ndata.head()",
  "generation": "data = data.rename(columns={'Rating (Num)': 'Rating', 'Schedule Pacing': 'Pace'})\ndata['NPS Category'] = pd.cut(data['Rating'], bins=[-1,6.5,8.5,10], labels=['Detractor', 'Passive', 'Promoter']) \ndata = data[['ID', 'Location', 'Track', 'Week', 'Rating', 'Pace', 'NPS Category']]\ndata"
 },
 {
  "nbid": "NB_2213651",
  "file": "NB_2213651,df-1.ipynb",
  "row_id": 47,
  "target_code": "df['highway-L/100km'                 ] = 235/df[\"highway-mpg\"             ]\ndf.head()",
  "generation": "df['highway-L/100km'] = 235/df[\"highway-mpg\"]\ndf.head()"
 },
 {
  "nbid": "NB_2215015",
  "file": "NB_2215015,test_data-1.ipynb",
  "row_id": 60,
  "target_code": "test_data['due_date'          ] = pd.to_datetime(test_data['due_date'          ])\ntest_data['effective_date'                ] = pd.to_datetime(test_data['effective_date'                ])\ntest_data.head()",
  "generation": "test_data['effective_date'] = pd.to_datetime(test_data['effective_date'])\ntest_data['due_date'] = pd.to_datetime(test_data['due_date'])\ntest_data.head()"
 },
 {
  "nbid": "NB_2215015",
  "file": "NB_2215015,test_data-1.ipynb",
  "row_id": 61,
  "target_code": "test_data['dayofweek'           ] = test_data['effective_date'                ].dt.dayofweek\ntest_data['weekend'         ] = test_data['dayofweek'           ].apply(lambda x: 1 if (x>3)  else 0)\ntest_data.head()",
  "generation": "test_data['dayofweek'] = test_data['effective_date'].dt.dayofweek\ntest_data['weekend'  ] = test_data['dayofweek'    ].apply(lambda x: 1 if (x>3)  else 0)\ntest_data.head()"
 },
 {
  "nbid": "NB_2222546",
  "file": "NB_2222546,data-1.ipynb",
  "row_id": 25,
  "target_code": "data = data.sample(frac=1).reset_index(drop=True)\ndata.head()",
  "generation": "titanic_data = titanic_data.dropna(subset=['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Title', 'Sex_male', 'Embarked_Q', 'Embarked_S'])\nd = {'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Other': 4}\ntitanic_data[\"Title\"] = titanic_data[\"Title\"].apply(lambda x: d[x])\ntitanic_data.head()"
 },
 {
  "nbid": "NB_2222546",
  "file": "NB_2222546,data-1.ipynb",
  "row_id": 26,
  "target_code": "data[\"Family Size\"             ] = data[\"SibSp\"       ] + data[\"Parch\"       ]\ndata.drop([\"SibSp\"       , \"Parch\"        ], axis = 1, inplace = True)\ndata.head()",
  "generation": "data['Family Size'] = data['SibSp'] + data['Parch'] + 1\ndata.drop(['SibSp', 'Parch'], axis=1, inplace=True)\ndata.head()"
 },
 {
  "nbid": "NB_2246933",
  "file": "NB_2246933,data-1.ipynb",
  "row_id": 2,
  "target_code": "data = data.drop(columns = ['PatientId'           ,'AppointmentID'               ,'Neighbourhood'               ,'Scholarship'             ])\ndata.head()",
  "generation": "data = data.drop(['PatientId', 'AppointmentID', 'Neighbourhood', 'Scholarship'], axis=1)\ndata.head()"
 },
 {
  "nbid": "NB_2246933",
  "file": "NB_2246933,data-1.ipynb",
  "row_id": 3,
  "target_code": "data['ScheduledDay'              ] = pd.to_datetime(data['ScheduledDay'              ], infer_datetime_format=True).dt.date\ndata['AppointmentDay'                ] = pd.to_datetime(data['AppointmentDay'                ], infer_datetime_format=True).dt.date\ndata['Difference'            ] = data['AppointmentDay'                ] - data['ScheduledDay'              ]\ndata.head()",
  "generation": "data['ScheduledDay'] = pd.to_datetime(data['ScheduledDay']).dt.date\ndata['AppointmentDay'] = pd.to_datetime(data['AppointmentDay']).dt.date\ndata['Difference'] = data['AppointmentDay'] - data['ScheduledDay']\ndata.head()"
 },
 {
  "nbid": "NB_2248784",
  "file": "NB_2248784,X-1.ipynb",
  "row_id": 17,
  "target_code": "X=pd.get_dummies(X)\nX.head()",
  "generation": "X = X[['Id', 'MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'SaleType_ConLw', 'SaleType_New', 'SaleType_Oth', 'SaleType_WD', 'SaleCondition_Abnorml', 'SaleCondition_AdjLand', 'SaleCondition_Alloca', 'SaleCondition_Family', 'SaleCondition_Normal', 'SaleCondition_Partial']]\nX.head()"
 },
 {
  "nbid": "NB_2250365",
  "file": "NB_2250365,star_wars-1.ipynb",
  "row_id": 4,
  "target_code": "import numpy\nmovie_mapping = {\n    \"Star Wars: Episode I  The Phantom Menace\"                                              : True,\n    numpy.nan: False,\n    \"Star Wars: Episode II  Attack of the Clones\"                                                 : True,\n    \"Star Wars: Episode III  Revenge of the Sith\"                                                 : True,\n    \"Star Wars: Episode IV  A New Hope\"                                       : True,\n    \"Star Wars: Episode V The Empire Strikes Back\"                                                  : True,\n    \"Star Wars: Episode VI Return of the Jedi\"                                              : True\n}\nfor col in star_wars.columns[3:9]:\n    star_wars[col] = star_wars[col].map(movie_mapping)\nstar_wars = star_wars.rename(columns={\n        \"Which of the following Star Wars films have you seen? Please select all that apply.\"                                                                                             : \"seen_1\"         ,\n        \"Unnamed: 4\"                    : \"seen_2\"         ,\n        \"Unnamed: 5\"                    : \"seen_3\"         ,\n        \"Unnamed: 6\"                    : \"seen_4\"         ,\n        \"Unnamed: 7\"                    : \"seen_5\"         ,\n        \"Unnamed: 8\"                    : \"seen_6\"         \n        })\nstar_wars.head()",
  "generation": "star_wars.iloc[:,2:4] = star_wars.iloc[:,2:4].fillna(False)\nstar_wars.iloc[:,2:4] = star_wars.iloc[:,2:4].replace({True: True, False: False, \"Yes\": True, \"No\": False})\nstar_wars.iloc[:,5:10] = star_wars.iloc[:,5:10].replace({\"Very favorably\": True, \"Somewhat favorably\": True, \"Neither favorably nor unfavorably (neutral)\": False, \"Unfamiliar (N/A)\": False, \"Somewhat unfavorably\": False, \"Very unfavorably\": False})\nstar_wars.head()"
 },
 {
  "nbid": "NB_2258579",
  "file": "NB_2258579,df-1.ipynb",
  "row_id": 11,
  "target_code": "df['due_date'          ] = pd.to_datetime(df['due_date'          ])\ndf['effective_date'                ] = pd.to_datetime(df['effective_date'                ])\ndf.head()",
  "generation": "df['effective_date'] = pd.to_datetime(df['effective_date'])\ndf['due_date'] = pd.to_datetime(df['due_date'])\ndf['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_546036",
  "file": "NB_546036,cntry_df-1.ipynb",
  "row_id": 14,
  "target_code": "cntry_df['health'        ]=(cntry_df['health'        ]*cntry_df['gdpp'      ])/100\ncntry_df['imports'         ]=(cntry_df['imports'         ]*cntry_df['gdpp'      ])/100\ncntry_df['income'        ]=(cntry_df['income'        ]*cntry_df['gdpp'      ])/100\ncntry_df.head()",
  "generation": "cntry_df['health'] = cntry_df['health']*cntry_df['gdpp']/100\ncntry_df['imports'] = cntry_df['imports']*cntry_df['gdpp']/100\ncntry_df['exports'] = cntry_df['exports']*cntry_df['gdpp']/100\ncntry_df.rename(columns={'health':'health_per_gdpp', 'imports':'imports_per_gdpp', 'exports':'exports_per_gdpp'}, inplace=True)\ncntry_df"
 },
 {
  "nbid": "NB_551534",
  "file": "NB_551534,Data,Data_y-1.ipynb",
  "row_id": 4,
  "target_code": "diagnosis_coder = {'M'   :1, 'B'    :0}\nData.diagnosis = Data.diagnosis.map(diagnosis_coder)\nData.head()",
  "generation": "Data['diagnosis'] = Data['diagnosis'].map({'M':1,'B':0})"
 },
 {
  "nbid": "NB_552561",
  "file": "NB_552561,grouped_pivot,row_labels,col_labels-1.ipynb",
  "row_id": 130,
  "target_code": "grouped_pivot = grouped_pivot.fillna(0)\ngrouped_pivot",
  "generation": "grouped_pivot = df_group_two.pivot(index='body-style', columns = 'drive-wheels', values='price')\ngrouped_pivot.fillna(0, inplace=True)\ngrouped_pivot"
 },
 {
  "nbid": "NB_558233",
  "file": "NB_558233,crime_norm-1.ipynb",
  "row_id": 50,
  "target_code": "col2 = ['\uac15\uac04\uac80\uac70\uc728'       , '\uac15\ub3c4\uac80\uac70\uc728'        , '\uc0b4\uc778\uac80\uac70\uc728'        , '\uc808\ub3c4\uac80\uac70\uc728'        , '\ud3ed\ub825\uac80\uac70\uc728'        ]\ncrime_norm['\uac80\uac70'    ] = np.sum(crime_norm[col2], axis = 1)\ncrime_norm.head()",
  "generation": "crime_norm['\uac80\uac70'] = np.sum(crime_norm[col2], axis = 1)\ncrime_norm"
 },
 {
  "nbid": "NB_560635",
  "file": "NB_560635,df_sharks-1.ipynb",
  "row_id": 18,
  "target_code": "irrelevant = ['Date'      ,'Type'      ,'Activity'          ,'Name'      ,'Sex '      ,'Age'     , 'Injury'         ,\n               'Time'                     ,'Species '          , 'Unnamed: 14'              ,'Unnamed: 15'             ]\ndf_sharks = df_sharks.drop(irrelevant,axis=1)\ndf_sharks = df_sharks[(df_sharks['Country'         ] == 'USA'      ) & (df_sharks['Area'      ] == 'California'             )]\ndf_sharks = df_sharks.dropna(subset=[\"Location\"          ])\ndf = df_sharks.reset_index()\ndf_sharks.head()",
  "generation": "df_sharks = df_sharks.dropna(subset=['Location'])\ndf_sharks = df_sharks[df_sharks['Country'] == 'USA']\ndf_sharks = df_sharks[df_sharks['Area'] == 'California']\ndf_sharks = df_sharks.drop(['Name', 'Sex', 'Age', 'Activity'], axis=1)\ndf_sharks = df_sharks[df_sharks['Fatal (Y/N)'].isin(['Y', 'N'])]\ndf_sharks = df_sharks[['Year', 'Country', 'Area', 'Location', 'Fatal (Y/N)']]\ndf_sharks.head()"
 },
 {
  "nbid": "NB_568848",
  "file": "NB_568848,crime_norm-1.ipynb",
  "row_id": 49,
  "target_code": "col = ['\uac15\uac04'    , '\uac15\ub3c4'     , '\uc0b4\uc778'     , '\uc808\ub3c4'     , '\ud3ed\ub825'     ]\ncrime_norm['\ubc94\uc8c4'    ]= np.sum(crime_norm[col], axis=1)\ncrime_norm.head()",
  "generation": "crime_norm['\ubc94\uc8c4'] = np.sum(crime_norm[col], axis=1)\ncrime_norm = crime_norm[['\uac15\uac04', '\uac15\ub3c4', '\uc0b4\uc778', '\uc808\ub3c4', '\ud3ed\ub825', '\uac15\uac04\uac80\uac70\uc728', '\uac15\ub3c4\uac80\uac70\uc728', '\uc0b4\uc778\uac80\uac70\uc728', '\uc808\ub3c4\uac80\uac70\uc728', '\ud3ed\ub825\uac80\uac70\uc728', '\uc778\uad6c\uc218', 'CCTV', '\ubc94\uc8c4', '\uad6c\ubcc4']]\ncrime_norm.head()"
 },
 {
  "nbid": "NB_568848",
  "file": "NB_568848,crime_norm-1.ipynb",
  "row_id": 50,
  "target_code": "col2 = ['\uac15\uac04\uac80\uac70\uc728'       , '\uac15\ub3c4\uac80\uac70\uc728'        , '\uc0b4\uc778\uac80\uac70\uc728'        , '\uc808\ub3c4\uac80\uac70\uc728'        , '\ud3ed\ub825\uac80\uac70\uc728'        ]\ncrime_norm['\uac80\uac70'    ]= np.sum(crime_norm[col2], axis=1)\ncrime_norm.head()",
  "generation": "crime_norm['\uac80\uac70'] = np.sum(crime_norm[col2], axis=1)"
 },
 {
  "nbid": "NB_593675",
  "file": "NB_593675,data-1.ipynb",
  "row_id": 8,
  "target_code": "data['Datetime'          ] = pd.to_datetime(data['Datetime'          ], format='%d-%m-%Y %H:%M'                )\ndata.head()",
  "generation": "data['Datetime'] = pd.to_datetime(data['Datetime'])\ndata.head()"
 },
 {
  "nbid": "NB_593675",
  "file": "NB_593675,data-1.ipynb",
  "row_id": 11,
  "target_code": "data = data.set_index(data['Datetime'          ])\ndata.head()",
  "generation": "data = data.set_index('Datetime')\ndata.index = pd.to_datetime(data.index, format='%d-%m-%Y %H:%M')\ndata = data.reset_index()\ndata.columns = ['Datetime', 'ID', 'Count']\ndata.head()"
 },
 {
  "nbid": "NB_6662310",
  "file": "NB_6662310,d-1.ipynb",
  "row_id": 2,
  "target_code": "d.experience = d.experience.fillna(\"zero\"      )\nd",
  "generation": "d['experience'] = d['experience'].apply(lambda x: 'zero' if x=='nan' else w2n.word_to_num(x))\nd.head()"
 },
 {
  "nbid": "NB_6671296",
  "file": "NB_6671296,df-1.ipynb",
  "row_id": 3,
  "target_code": "df = df.join(pd.get_dummies(df.prestige))\ndf.head()",
  "generation": "df[['1', '2', '3', '4']] = pd.get_dummies(df['prestige'])\ndf"
 },
 {
  "nbid": "NB_6686583",
  "file": "NB_6686583,df-1.ipynb",
  "row_id": 3,
  "target_code": "df.experiene = df.experiene.fillna('zero'      )\ndf",
  "generation": "df['experiene'] = df['experiene'].apply(lambda x: w2n.word_to_num(x) if isinstance(x, str) else x)\ndf['experiene'] = df['experiene'].fillna(0)\ndf['experiene'] = df['experiene'].apply(lambda x: 'zero' if x == 0 else w2n.num_to_word(x))\ndf.head()"
 },
 {
  "nbid": "NB_2268892",
  "file": "NB_2268892,f,budgetyear,profityear,imdbyear,mlensyear,yearlabels-1.ipynb",
  "row_id": 7,
  "target_code": "f['ReleseDate'            ]=pd.to_datetime(f['Release'         ])\nf.head(2)",
  "generation": "f['Release Date'] = pd.to_datetime(f['Release Date'])\nf = f.rename(columns={'Release Date': 'ReleseDate'})\nf['Day'] = f['ReleseDate'].dt.day_name()\nf = f[['Unnamed: 0', 'Day', 'Director', 'Genre', 'Movie Title', 'ReleseDate', 'Studio', 'Adjusted Gross ($mill)', 'Budget ($mill)', 'Gross ($mill)', 'IMDb Rating', 'MovieLens Rating', 'Overseas ($mill)', 'Overseas%', 'Profit ($mill)', 'Profit%', 'Runtime (min)', 'US ($mill)', 'Gross % US']]\nf = f.rename(columns={'Adjusted Gross ($mill)': 'Adj_GrossM', 'Budget ($mill)': 'BudgetM', 'Gross ($mill)': 'GrossM', 'IMDb Rating': 'IMDb', 'MovieLens Rating': 'MovieLens', 'Overseas ($mill)': 'OverseasM', 'Profit ($mill)': 'ProfitM', 'Runtime (min)': 'Runtime', 'US ($mill)': 'USM'})\nf"
 },
 {
  "nbid": "NB_2273142",
  "file": "NB_2273142,df_5-1.ipynb",
  "row_id": 37,
  "target_code": "df_5 = df_5.dropna(subset=['attacker_outcome'                  , 'attacker_size'                , 'defender_size'                ])\ndf_5",
  "generation": "df_5 = df_5.dropna()\ndf_5.head()"
 },
 {
  "nbid": "NB_2273142",
  "file": "NB_2273142,df_5-1.ipynb",
  "row_id": 38,
  "target_code": "df_5['differece_size'                ] = df_5['attacker_size'               ] - df_5['defender_size'               ]\ndf_5.head()",
  "generation": "df_5['differece_size'] = df_5['attacker_size'] - df_5['defender_size']\ndf_5.head()"
 },
 {
  "nbid": "NB_2280649",
  "file": "NB_2280649,titanic_data,X-1.ipynb",
  "row_id": 4,
  "target_code": "titanic_data = titanic_data.drop(['Cabin'       , 'Ticket'         ], axis=1)\ntitanic_data.head(5)",
  "generation": "titanic_data.drop(['SibSp', 'ParCh', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntitanic_data.head()"
 },
 {
  "nbid": "NB_2298979",
  "file": "NB_2298979,star_wars-1.ipynb",
  "row_id": 4,
  "target_code": "star_wars = star_wars[star_wars['RespondentID'              ].notnull()]\nstar_wars.head(10)",
  "generation": "star_wars = star_wars.dropna(subset=['RespondentID'])\nstar_wars['seen_1'] = star_wars['seen_1'].map({'Star Wars: Episode I  The Phantom Menace': True, np.nan: False})\nstar_wars['seen_2'] = star_wars['seen_2'].map({'Star Wars: Episode II  Attack of the Clones': True, np.nan: False})\nstar_wars['seen_3'] = star_wars['seen_3'].map({'Star Wars: Episode III  Revenge of the Sith': True, np.nan: False})\nstar_wars['seen_4'] = star_wars['seen_4'].map({'Star Wars: Episode IV  A New Hope': True, np.nan: False})\nstar_wars['seen_5'] = star_wars['seen_5'].map({'Star Wars: Episode V The Empire Strikes Back': True, np.nan: False})\nstar_wars['seen_6'] = star_wars['seen_6'].map({'Star Wars: Episode VI Return of the Jedi': True, np.nan: False})\nstar_wars = star_wars.rename(columns={'Which of the following Star Wars films have you seen? Please select all that apply.': 'seen_1',\n                                      'Unnamed: 4': 'seen_2',\n                                      'Unnamed: 5': 'seen_3',\n                                      'Unnamed: 6': 'seen_4',\n                                      'Unnamed: 7': 'seen_5',\n                                      'Unnamed: 8': 'seen_6'})"
 },
 {
  "nbid": "NB_2298979",
  "file": "NB_2298979,star_wars-1.ipynb",
  "row_id": 5,
  "target_code": "to_bool = {\n    'Yes'         : True,\n    'No'        : False\n}\nstar_wars['Have you seen any of the 6 films in the Star Wars franchise?'                                                              ] = star_wars['Have you seen any of the 6 films in the Star Wars franchise?'                                                              ].map(to_bool)\nstar_wars['Do you consider yourself to be a fan of the Star Wars film franchise?'                                                                       ] = star_wars['Do you consider yourself to be a fan of the Star Wars film franchise?'                                                                       ].map(to_bool)\nstar_wars.head(10)",
  "generation": "yes_no_map = {\"Yes\": True, \"No\": False}\nstar_wars['Have you seen any of the 6 films in the Star Wars franchise?'] = star_wars['Have you seen any of the 6 films in the Star Wars franchise?'].map(yes_no_map)\nstar_wars['Do you consider yourself to be a fan of the Star Wars film franchise?'] = star_wars['Do you consider yourself to be a fan of the Star Wars film franchise?'].map(yes_no_map)\nstar_wars.head(2)"
 },
 {
  "nbid": "NB_2298979",
  "file": "NB_2298979,star_wars-1.ipynb",
  "row_id": 6,
  "target_code": "import numpy as np\nto_bool = {\n    'Star Wars: Episode I  The Phantom Menace'                                              : True,\n    'Star Wars: Episode II  Attack of the Clones'                                                 : True,\n    'Star Wars: Episode III  Revenge of the Sith'                                                 : True,\n    'Star Wars: Episode IV  A New Hope'                                       : True,\n    'Star Wars: Episode V The Empire Strikes Back'                                                  : True,\n    'Star Wars: Episode VI Return of the Jedi'                                              : True,\n    np.NaN: False\n}\nfor key in star_wars.columns[3:9]:\n    star_wars[key] = star_wars[key].map(to_bool)\nstar_wars.head(10)",
  "generation": "star_wars['Have you seen any of the 6 films in the Star Wars franchise?'] = star_wars['Have you seen any of the 6 films in the Star Wars franchise?'].fillna(False)\nstar_wars['Do you consider yourself to be a fan of the Star Wars film franchise?'] = star_wars['Do you consider yourself to be a fan of the Star Wars film franchise?'].fillna(False)\nstar_wars.iloc[:, 2:8] = star_wars.iloc[:, 2:8].replace({'Yes': True, 'No': False, np.nan: False})\nstar_wars.head(2)"
 },
 {
  "nbid": "NB_2298979",
  "file": "NB_2298979,star_wars-1.ipynb",
  "row_id": 7,
  "target_code": "rename = {\n    'Which of the following Star Wars films have you seen? Please select all that apply.'                                                                                         : 'seen_1'         ,\n    'Unnamed: 4'                : 'seen_2'         ,\n    'Unnamed: 5'                : 'seen_3'         ,\n    'Unnamed: 6'                : 'seen_4'         ,\n    'Unnamed: 7'                : 'seen_5'         ,\n    'Unnamed: 8'                : 'seen_6'         ,\n}\nstar_wars = star_wars.rename(columns=rename)\nstar_wars.head(5)",
  "generation": "star_wars.rename(columns={\n    'Which of the following Star Wars films have you seen? Please select all that apply.': 'seen_1',\n    'Unnamed: 4': 'seen_2',\n    'Unnamed: 5': 'seen_3',\n    'Unnamed: 6': 'seen_4',\n    'Unnamed: 7': 'seen_5',\n    'Unnamed: 8': 'seen_6'\n}, inplace=True)\n\nfor key in star_wars.columns[2:4]:\n    star_wars[key] = star_wars[key].map(to_bool)\n\nfor key in star_wars.columns[3:9]:\n    star_wars[key] = star_wars[key].map(to_bool)\n\nstar_wars.head(10)"
 },
 {
  "nbid": "NB_2313300",
  "file": "NB_2313300,dg-1.ipynb",
  "row_id": 8,
  "target_code": "dg['Ndate'       ] = pd.to_datetime(dg['Date'      ], format='%d %m %Y'          )\ndg['month'       ] = dg['Ndate'       ].dt.month\ndg.head()",
  "generation": "dg['Ndate'] = pd.to_datetime(dg['Date'], format='%d %m %Y')\ndg['month'] = dg['Ndate'].dt.month\ndg.head()"
 },
 {
  "nbid": "NB_2316476",
  "file": "NB_2316476,star_wars,bar1-1.ipynb",
  "row_id": 1,
  "target_code": "boo = {\"Yes\"     :True, \"No\"     :False}\nstar_wars[star_wars.columns[1]] = star_wars[star_wars.columns[1]].map(boo)\nstar_wars[star_wars.columns[2]] = star_wars[star_wars.columns[2]].map(boo)\nbools = {\"Star Wars: Episode I  The Phantom Menace\"                                          :True,\n         \"Star Wars: Episode II  Attack of the Clones\"                                                      :True,\n         \"Star Wars: Episode III  Revenge of the Sith\"                                                      :True,\n         \"Star Wars: Episode IV  A New Hope\"                                            :True,\n         \"Star Wars: Episode V The Empire Strikes Back\"                                                       :True,\n         \"Star Wars: Episode VI Return of the Jedi\"                                                   :True,\n         np.nan: False}\nfor c in range(3,9):\n    star_wars[star_wars.columns[c]] = star_wars[star_wars.columns[c]].map(bools)\nstar_wars = star_wars.rename(columns={\"Which of the following Star Wars films have you seen? Please select all that apply.\"                                                                                     :\"seen_1\"        ,\n                                     \"Unnamed: 4\"                                                 : \"seen_2\"         , \"Unnamed: 5\"             : \"seen_3\"         , \"Unnamed: 6\"             :\"seen_4\"        ,\n                                     \"Unnamed: 7\"                                                 :\"seen_5\"        , \"Unnamed: 8\"             : \"seen_6\"         })\nstar_wars.head(10)",
  "generation": "star_wars.iloc[:,2:10] = star_wars.iloc[:,2:10].apply(lambda x: x.notnull())\nstar_wars.head(2)"
 },
 {
  "nbid": "NB_2323355",
  "file": "NB_2323355,nutr_facts-1.ipynb",
  "row_id": 5,
  "target_code": "df = nutr_facts.drop(['greek_or_not'              ], axis=1) \nstd_sz = 150\nfor r in range(len(df)): \n    row = df.iloc[r]\n    sz = row['serving_size'              ]\n    df.iloc[r] = row.div(sz).mul(std_sz)\nnutr_facts = pd.concat([df, nutr_facts['greek_or_not'              ]], axis=1)\nnutr_facts",
  "generation": "nutr_facts = nutr_facts.drop(columns=['Unnamed: 1_level_1', 'Unnamed: 2_level_1', 'Unnamed: 3_level_1', 'Unnamed: 4_level_1', 'Unnamed: 5_level_1', 'Unnamed: 6_level_1', 'Unnamed: 7_level_1', 'Unnamed: 8_level_1', 'Unnamed: 9_level_1', 'Unnamed: 10_level_1'])\nnutr_facts = nutr_facts.div(nutr_facts['serving_size'], axis=0)\nnutr_facts = nutr_facts.drop(columns=['serving_size'])\nnutr_facts.head()"
 },
 {
  "nbid": "NB_2323607",
  "file": "NB_2323607,titanic_df-1.ipynb",
  "row_id": 7,
  "target_code": "titanic_df['Cabin'       ] = titanic_df['Cabin'       ].fillna('UNKNOW'        )\ntitanic_df.head()",
  "generation": "titanic_df['Cabin'].fillna('UNKNOW', inplace=True)\ntitanic_df.head()"
 },
 {
  "nbid": "NB_2323607",
  "file": "NB_2323607,titanic_df-1.ipynb",
  "row_id": 13,
  "target_code": "titanic_df = pd.get_dummies(titanic_df, columns=['Sex'     ])\ntitanic_df.head()",
  "generation": "titanic_df['Cabin'] = titanic_df['Cabin'].fillna('UNKNOW')\ntitanic_df['Sex_female'] = pd.get_dummies(titanic_df['Sex'])['female']\ntitanic_df['Sex_male'] = pd.get_dummies(titanic_df['Sex'])['male']\ntitanic_df.drop('Sex', axis=1, inplace=True)\ntitanic_df.head()"
 },
 {
  "nbid": "NB_2326653",
  "file": "NB_2326653,mobile_df-1.ipynb",
  "row_id": 22,
  "target_code": "mobile_df=mobile_df.loc[:,('subreddit'           ,'content'         )]\nmobile_df.head()",
  "generation": "mobile_df = pd.concat([android_df,iphone_df])\nmobile_df['selftext'].fillna('', inplace=True)\nmobile_df['content']=mobile_df['title'] +' '+ mobile_df['selftext']\nmobile_df['subreddit'] = mobile_df['subreddit'].map({'Android':1,'iphone':0})\nmobile_df = mobile_df[['subreddit', 'content']]\nmobile_df.head()"
 },
 {
  "nbid": "NB_2326801",
  "file": "NB_2326801,df-1.ipynb",
  "row_id": 76,
  "target_code": "df[\"highway-mpg\"             ] = 235/df[\"highway-mpg\"             ]\ndf.rename(columns={\"highway-mpg\"             : \"highway-L/100km\"                  }, inplace=True)\ndf.head()",
  "generation": "df['highway-L/100km'] = 235/df[\"highway-mpg\"]\ndf = df.rename(columns={'city-L/100km': 'city-L/100km', 'highway-L/100km': 'highway-L/100km'})\ndf.head()"
 },
 {
  "nbid": "NB_2328670",
  "file": "NB_2328670,df-1.ipynb",
  "row_id": 2,
  "target_code": "df = df.iloc[:50,:2]\ndf['Date'      ] = pd.to_datetime(df['Date'      ])\ndf.head()",
  "generation": "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')\ndf = df[['Date', 'Temp']]\ndf.head()"
 },
 {
  "nbid": "NB_2334602",
  "file": "NB_2334602,df,day_of_week_ct_df-1.ipynb",
  "row_id": 12,
  "target_code": "df = df.rename(columns={'CVLEGEND'          : 'EVENTDESC'            })\ndf.head()",
  "generation": "df.rename(columns={'CVLEGEND': 'EVENTDESC'}, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_2334602",
  "file": "NB_2334602,df,day_of_week_ct_df-1.ipynb",
  "row_id": 22,
  "target_code": "df = df.drop(\"Block_Location\"                , axis = 1)\ndf.head()",
  "generation": "df = df.drop(columns=['Block_Location'])\ndf = df.rename(columns={'LAT': 'LATITUDE', 'LON': 'LONGITUDE'})\ndf.head()"
 },
 {
  "nbid": "NB_2334602",
  "file": "NB_2334602,df,day_of_week_ct_df-1.ipynb",
  "row_id": 27,
  "target_code": "df = df.drop([\"City\"      ,\"State\"       ], axis = 1)\ndf.head()",
  "generation": "df = df.drop([\"City\", \"State\"], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_2334602",
  "file": "NB_2334602,df,day_of_week_ct_df-1.ipynb",
  "row_id": 45,
  "target_code": "df['EVENTDT'         ] = pd.to_datetime(df['EVENTDT'         ]).dt.date\ndf.head()",
  "generation": "df['EVENTDT'] = pd.to_datetime(df['EVENTDT']).dt.date\ndf.head()"
 },
 {
  "nbid": "NB_2338700",
  "file": "NB_2338700,persons-1.ipynb",
  "row_id": 20,
  "target_code": "persons = persons[['Height'        , 'Weight'         , 'Gender'         ]]\npersons.head()",
  "generation": "persons = persons[['Height', 'Weight', 'Gender']]\npersons['Gender'] = persons['Gender'].replace({'Male': 1, 'Female': 0})\npersons = persons[['Unnamed: 0', 'Height', 'Weight', 'Gender']]\npersons.head()"
 },
 {
  "nbid": "NB_2339522",
  "file": "NB_2339522,trends-1.ipynb",
  "row_id": 3,
  "target_code": "trends = trends.rename(columns = {\"Month\"        : \"month\"        , \"Kim Kardashian: (Worldwide)\"                               : \"kim\"      , \"Khlo\u00e9 Kardashian: (Worldwide)\"                                 : \"khloe\"        , \"Kourtney Kardashian: (Worldwide)\"                                    : \"kourtney\"           , \"Kendall Jenner: (Worldwide)\"                               : \"kendall\"          , \"Kylie Jenner: (Worldwide)\"                             : \"kylie\"         })\ntrends.head()",
  "generation": "trends.columns=['month','kim','khloe','kourtney','kendall','kylie']\ntrends.head()"
 },
 {
  "nbid": "NB_2339522",
  "file": "NB_2339522,trends-1.ipynb",
  "row_id": 7,
  "target_code": "for i in trends.columns:\n    if \"<\"     in trends[i].to_string():\n        trends[i] = trends[i].str.replace(\"<\"   , \" \"    )\n        trends[i] = pd.to_numeric(trends[i])\ntrends.info()\ntrends.head()",
  "generation": "for col in trends.columns[2:]:\n    trends[col] = trends[col].str.replace('<', '').astype(int)\n    \ntrends.head()"
 },
 {
  "nbid": "NB_2340536",
  "file": "NB_2340536,df-1.ipynb",
  "row_id": 9,
  "target_code": "df['GroundTruth'             ] = df.Capacity * df.SurveyPercentage\ndf.head()",
  "generation": "df['GroundTruth'] = df['Capacity'] * df['SurveyPercentage']"
 },
 {
  "nbid": "NB_2340536",
  "file": "NB_2340536,df2-1.ipynb",
  "row_id": 15,
  "target_code": "df2['MaxCount'          ] = maxlist\ndf2['AverageCount'              ] = averagelist\ndf2['MedianCount'             ] = medianlist\ndf2['ModeCount'           ] = modelist\ntry:\n    df2 = df2.drop('Count'       , axis=1)\nexcept ValueError:\n    pass\ndf2.head()",
  "generation": "df2['MaxCount'   ]=df2.groupby(['room_id'])['Count'].transform(max)\ndf2['AverageCount'   ]=df2.groupby(['room_id'])['Count'].transform('mean')\ndf2['MedianCount'   ]=df2.groupby(['room_id'])['Count'].transform('median')\ndf2['ModeCount'   ]=df2.groupby(['room_id'])['Count'].transform(lambda x: x.value_counts().index[0])\ndf2 = df2.drop(['Count'], axis=1)\ndf2.head()"
 },
 {
  "nbid": "NB_2340536",
  "file": "NB_2340536,df2-1.ipynb",
  "row_id": 16,
  "target_code": "df2['SurveyPercentage'                  ] = df2['SurveyPercentage'                  ].apply(lambda x: x*100)\ndf2.head()",
  "generation": "df2['SurveyPercentage'] = df2['SurveyPercentage']*100\ndf2.head()"
 },
 {
  "nbid": "NB_2347643",
  "file": "NB_2347643,star_wars,females-1.ipynb",
  "row_id": 7,
  "target_code": "star_wars = star_wars[pd.notnull(star_wars['RespondentID'              ])]\nstar_wars.head(3)",
  "generation": "star_wars = star_wars[pd.notnull(star_wars['RespondentID'])]"
 },
 {
  "nbid": "NB_2347643",
  "file": "NB_2347643,star_wars,females-1.ipynb",
  "row_id": 16,
  "target_code": "star_wars = star_wars.rename(columns={\n    \"Which of the following Star Wars films have you seen? Please select all that apply.\"                                                                                         : \"seen_1\"         ,\n    \"Unnamed: 4\"                : \"seen_2\"          ,\n    \"Unnamed: 5\"                : \"seen_3\"         ,\n    \"Unnamed: 6\"                : \"seen_4\"         ,\n    \"Unnamed: 7\"                : \"seen_5\"         ,\n    \"Unnamed: 8\"                : \"seen_6\"         \n})\nstar_wars.head()",
  "generation": "movie_mapping = {\n    \"Which of the following Star Wars films have you seen? Please select all that apply.\": \"seen_1\",\n    \"Unnamed: 4\": \"seen_2\",\n    \"Unnamed: 5\": \"seen_3\",\n    \"Unnamed: 6\": \"seen_4\",\n    \"Unnamed: 7\": \"seen_5\",\n    \"Unnamed: 8\": \"seen_6\"\n}\n\nstar_wars = star_wars.rename(columns=movie_mapping)\n\nmovie_mapping = {\n    \"Star Wars: Episode I  The Phantom Menace\": True,\n    \"Star Wars: Episode II  Attack of the Clones\": True,\n    \"Star Wars: Episode III  Revenge of the Sith\": True,\n    \"Star Wars: Episode IV  A New Hope\": True,\n    \"Star Wars: Episode V The Empire Strikes Back\": True,\n    \"Star Wars: Episode VI Return of the Jedi\": True,\n    np.nan: False\n}\n\nfor col in star_wars.columns[3:9]:\n    star_wars[col] = star_wars[col].map(movie_mapping)\n    \nstar_wars.head()"
 },
 {
  "nbid": "NB_2348874",
  "file": "NB_2348874,trends-1.ipynb",
  "row_id": 11,
  "target_code": "trends =  trends.set_index(\"month\"       )\ntrends.head()",
  "generation": "trends.set_index('month', inplace=True)\ntrends.head()"
 },
 {
  "nbid": "NB_2350140",
  "file": "NB_2350140,trends-1.ipynb",
  "row_id": 9,
  "target_code": "trends['month'       ] = pd.to_datetime(trends['month'       ])\nprint(trends.info())\ntrends.head()",
  "generation": "trends['month'] = pd.to_datetime(trends['month'], format='%Y-%m')\ntrends.head()"
 },
 {
  "nbid": "NB_2350691",
  "file": "NB_2350691,trends-1.ipynb",
  "row_id": 7,
  "target_code": "for column in trends.columns:\n    if '<'     in trends[column].to_string():\n        trends[column] = trends[column].str.replace('<'   ,\"\"  )\n        trends[column] = pd.to_numeric(trends[column])\ntrends.info()\ntrends.head()",
  "generation": "cols = ['khloe', 'kourtney', 'kendall']\nfor col in cols:\n    trends[col] = trends[col].str.replace('<', '').astype(int)\ntrends = trends.rename(columns={'Month': 'month'})\ntrends = trends.rename(columns=lambda x: x.lower())\ntrends.head()"
 },
 {
  "nbid": "NB_2356154",
  "file": "NB_2356154,amounts-1.ipynb",
  "row_id": 15,
  "target_code": "amounts = amounts.groupby('province'          )['menus.amountMax'                 ].mean().nlargest(10).to_frame()\namounts.head()",
  "generation": "amounts = amounts.groupby('province')['menus.amountMax'].max().reset_index()\namounts = amounts.sort_values(by='menus.amountMax', ascending=False).head(5)\namounts = amounts[['province', 'menus.amountMax']]\namounts.set_index('province', inplace=True)\namounts.columns = ['']\namounts.head()"
 },
 {
  "nbid": "NB_2359831",
  "file": "NB_2359831,dados,y-1.ipynb",
  "row_id": 1,
  "target_code": "map = {\n    'expected_hours'                     : 'horas_esperadas'                  ,\n    'price'            : 'preco'        ,\n    'unfinished'                 : 'nao_finalizado'                 \n}\ndados = dados.rename(columns = map)\ndados.head()",
  "generation": "dados = dados.rename(columns={'unfinished': 'nao_finalizado', 'expected_hours': 'horas_esperadas', 'price': 'preco'})\ndados.head()"
 },
 {
  "nbid": "NB_2359831",
  "file": "NB_2359831,dados,y-1.ipynb",
  "row_id": 2,
  "target_code": "swap = {\n    0 : 1,\n    1 : 0\n}\ndados['finalizado'            ] = dados.nao_finalizado.map(swap)\ndados.head()",
  "generation": "dados['finalizado'] = 0\ndados.head()"
 },
 {
  "nbid": "NB_2361032",
  "file": "NB_2361032,trends-1.ipynb",
  "row_id": 7,
  "target_code": "for kardashian in trends.columns:\n    if '<'     in trends[kardashian].to_string():\n        trends[kardashian] = trends[kardashian].str.replace('<'   , ''   )\n        trends[kardashian] = pd.to_numeric(trends[kardashian])\ntrends.info()\ntrends.head()",
  "generation": "trends['khloe'] = trends['khloe'].str.replace('<', '').astype(int)\ntrends['kourtney'] = trends['kourtney'].str.replace('<', '').astype(int)\ntrends['kendall'] = trends['kendall'].str.replace('<', '').astype(int)\n\ntrends.columns = ['month', 'kim', 'khloe', 'kourtney', 'kendall', 'kylie']\ntrends.head()"
 },
 {
  "nbid": "NB_2362162",
  "file": "NB_2362162,tmp-1.ipynb",
  "row_id": 6,
  "target_code": "tmp =pd.get_dummies(tmp, columns=[\"Ethnicity\"           ])\ntmp = tmp.rename(index=str, columns={\"Gender_ Male\"              : \"Gender_Male\"              ,\"Ethnicity_African American\"                            :\"Ethnicity_African_American\"                            })\ntmp = tmp.drop([\"Ethnicity_Caucasian\"                     ,\"Student_No\"            ,\"Married_No\"            ,\"Gender_Female\"               ], axis=1)\ntmp.head()",
  "generation": "data = pd.get_dummies(data, columns=[\"Gender\",\"Student\",\"Married\"])\ndata = data.drop(['Ethnicity_African American'], axis=1)\ndata = data.rename(columns={'Gender_ Male': 'Gender_Male', 'Student_No': 'Student_Yes', 'Married_No': 'Married_Yes', 'Ethnicity_Asian': 'Ethnicity_African_American'})\ndata.head()"
 },
 {
  "nbid": "NB_2362670",
  "file": "NB_2362670,ndf_test-1.ipynb",
  "row_id": 21,
  "target_code": "dum_sex=pd.get_dummies(df_train.Sex)\nprint(dum_sex.corrwith(df_train.Survived))\nndf_train=ndf_train.join(pd.get_dummies(df_train.Sex,drop_first=True))\nndf_test=ndf_test.join(pd.get_dummies(df_test.Sex,drop_first=True))\nndf_test.head()",
  "generation": "df_train.Age[kid] = df_train.Age[kid].fillna(6)\ndf_test.Age[kid] = df_test.Age[kid].fillna(6)\nbins = [(- 1), 0.5, 3.5, 9]\nndf_train=pd.DataFrame(df_train[\"Pclass\"].copy())\nndf_test=pd.DataFrame(df_test[\"Pclass\"].copy())\nmost_fre=df_train.Embarked.value_counts().max()\nndf_train[\"Embarked\"]=df_train.Embarked.fillna(most_fre)\nndf_train.head()\ndum_emb=pd.get_dummies(df_train.Embarked)\nprint(dum_emb.corrwith(df_train.Survived))\nndf_train=ndf_train.join(pd.get_dummies(df_train.Embarked))\nndf_test=ndf_test.join(pd.get_dummies(df_test.Embarked))\nndf_test.head()\nndf_train=ndf_train.join(pd.get_dummies(pd.cut(df_train.Fare,bins,labels=[\"Fare_low\",\"Fare_mid\",\"Fare_high\"])))\nndf_test=ndf_test.join(pd.get_dummies(pd.cut(df_test.Fare,bins,labels=[\"Fare_low\",\"Fare_mid\",\"Fare_high\"])))\nndf_test.head()\nndf_train=ndf_train.join(pd.get_dummies(df_train.Sex))\nndf_test=ndf_test.join(pd.get_dummies(df_test.Sex))\nndf_test.head()"
 },
 {
  "nbid": "NB_2362670",
  "file": "NB_2362670,ndf_test-1.ipynb",
  "row_id": 24,
  "target_code": "ndf_train=ndf_train.join(pd.get_dummies(pd.cut(df_train.SibSp,bins,labels=[\"S0\"    ,\"S1-2\"      ,\"S>2\"     ])))\nndf_test=ndf_test.join(pd.get_dummies(pd.cut(df_test.SibSp,bins,labels=[\"S0\"    ,\"S1-2\"      ,\"S>2\"     ])))\nndf_test.head()",
  "generation": "ndf_train['S0'] = ndf_train['S'] == 0\nndf_train['S1-2'] = ndf_train['S'].isin([1, 2])\nndf_train['S>2'] = ndf_train['S'] > 2\n\nndf_test['S0'] = ndf_test['S'] == 0\nndf_test['S1-2'] = ndf_test['S'].isin([1, 2])\nndf_test['S>2'] = ndf_test['S'] > 2\n\nndf_train.drop('S', axis=1, inplace=True)\nndf_test.drop('S', axis=1, inplace=True)"
 },
 {
  "nbid": "NB_2362670",
  "file": "NB_2362670,ndf_train,ndf_test-1.ipynb",
  "row_id": 15,
  "target_code": "dum_emb=pd.get_dummies(df_train.Embarked)\nprint(dum_emb.corrwith(df_train.Survived))\nndf_train=ndf_train.join(pd.get_dummies(df_train.Embarked))\nndf_test=ndf_test.join(pd.get_dummies(df_test.Embarked))\nndf_test.head()",
  "generation": "ndf_train=pd.get_dummies(df_train, columns=['Embarked'])\nndf_test=pd.get_dummies(df_test, columns=['Embarked'])\nndf_train.head()"
 },
 {
  "nbid": "NB_2365200",
  "file": "NB_2365200,df,result-1.ipynb",
  "row_id": 6,
  "target_code": "l = LabelEncoder()\ndf[\"i_o\"     ] = l.fit_transform(df[\"i_o\"     ])\ndf[\"crashed\"         ] = l.fit_transform(df[\"crashed\"         ])\ndf.head()",
  "generation": "le = LabelEncoder()\ndf['cpu_utilization'] = le.fit_transform(df['cpu_utilization'])\ndf['crashed'] = le.fit_transform(df['crashed'])\ndf.head()"
 },
 {
  "nbid": "NB_2365854",
  "file": "NB_2365854,mails,spam_mails-1.ipynb",
  "row_id": 3,
  "target_code": "mails=mails.drop([\"Unnamed: 2\"            ,\"Unnamed: 3\"            ,\"Unnamed: 4\"            ],axis=1)\nmails=mails.rename(columns={\"v1\"    :\"label\"       ,\"v2\"    :\"message\"         })\nmails.label=mails.label.replace({\"ham\"     :\"0\"   ,\"spam\"      :\"1\"   })\nmails",
  "generation": "mails = mails.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\nmails = mails.rename(columns={'v1': 'label', 'v2': 'message'})\nmails['label'] = mails['label'].map({'ham': 0, 'spam': 1})\nmails.head()"
 },
 {
  "nbid": "NB_2368381",
  "file": "NB_2368381,a-1.ipynb",
  "row_id": 4,
  "target_code": "a = a.loc[a['\uc5c5\uc885\uc911\ubd84\ub958'       ] == '\ub300\uc911\ubaa9\uc695\ud0d5/\ud734\uac8c'           ]\na",
  "generation": "a = pd.read_csv('bbb.csv')\na = a.set_index(\"\uad11\uc5ed\uc2dc\ub3c4\")\ndel a[\"Unnamed: 9\"]\na = a.loc['\uc11c\uc6b8\ud2b9\ubcc4\uc2dc']\na = a.set_index(\"\uc2dc\uad70\uad6c\")\na.head()"
 },
 {
  "nbid": "NB_498300",
  "file": "NB_498300,suicide_sum-1.ipynb",
  "row_id": 18,
  "target_code": "ind = [2, 3, 4, 1, 5, 6]\nsuicide_sum[\"index_col\"           ] = ind\nsuicide_sum = suicide_sum.sort_values(\"index_col\"           )['suicides_sum'              ]\nsuicide_sum",
  "generation": "suicide_sum = suicide_sum.reindex(suicide_sum['suicides_sum'].sort_values(by='55-74 years', ascending=False).index)\nsuicide_sum"
 },
 {
  "nbid": "NB_507439",
  "file": "NB_507439,country_df-1.ipynb",
  "row_id": 8,
  "target_code": "country_df['Cum Cases'           ] = country_df.groupby(\"countriesAndTerritories\"                         )['cases'       ].cumsum()\ncountry_df['Cum Deaths'            ] = country_df.groupby(\"countriesAndTerritories\"                         )['deaths'        ].cumsum()\ncountry_df.head(3)",
  "generation": "country_df['Cum Cases'] = country_df.groupby('countriesAndTerritories')['cases'].cumsum()\ncountry_df['Cum Deaths'] = country_df.groupby('countriesAndTerritories')['deaths'].cumsum()\ncountry_df.head(3)"
 },
 {
  "nbid": "NB_507618",
  "file": "NB_507618,df-1.ipynb",
  "row_id": 2,
  "target_code": "for i in ['price'       , 'sqft_living'              , 'sqft_lot'           , 'waterfront'             , 'view'       , 'condition'            , 'grade'        , 'sqft_above'             , 'sqft_basement'                ,\n          'yr_built'                    , 'yr_renovated'               , 'zipcode'          , 'sqft_living15'                , 'sqft_lot15'             ]:\n    df[i] = df[i].astype('int'     ) \ndf.head(2)",
  "generation": "df[['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade']] = df[['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade']].astype(int)\ndf.head(3)"
 },
 {
  "nbid": "NB_523497",
  "file": "NB_523497,data-1.ipynb",
  "row_id": 12,
  "target_code": "data['Public Score'              ] = NewScore\ndata = data.drop(columns = \"Score\"        )\ndata.head()",
  "generation": "data['Average Score'] = (data['Score'] + data['Metascore']) / 2\ndata['Public Score'] = data['Score'] * 10\ndata = data.drop(['Score'], axis=1)"
 },
 {
  "nbid": "NB_532075",
  "file": "NB_532075,df-1.ipynb",
  "row_id": 1,
  "target_code": "df=df.rename(columns={\n    \"TOWN\"          :\"CIUDAD\"        ,\n    \"CRIM\"          :\"INDICE_CRIMEN\"               ,\n    \"ZN\"        :\"PCT_ZONA_RESIDENCIAL\"                      ,\n    \"INDUS\"           :\"PCT_ZONA_INDUSTRIAL\"                     ,\n    \"CHAS\"          :\"RIO_CHARLES\"             ,\n    \"NOX\"         :\"OXIDO_NITROSO_PPM\"                   ,\n    \"RM\"        :\"N_HABITACIONES_MEDIO\"                      ,\n    \"AGE\"         :\"PTC_CASAS_405\"               ,\n    \"DIS_EMPLEO\"                :\"DISTANCIA_CENTRO_EMPLEO\"                         ,\n    \"RAD\"         :\"DIS_AUTOPISTAS\"                ,\n    \"TAX\"         :\"CARGA_FISCAL\"              ,\n    \"PTRATIO\"             :\"RATIO_PROFESORES\"                  ,\n    \"B\"       :\"PCT_NEGRA\"           ,\n    \"MEDV\"          :\"VALOR_MEDIANO\"               ,\n    \"LSTAT\"           :\"PTC_CLASE_BAJA\"                \n}\n)\ndf.head()",
  "generation": "df = df.rename(columns={'TOWN': 'CIUDAD', 'MEDV': 'VALOR_MEDIANO', 'CRIM': 'INDICE_CRIMEN', 'ZN': 'PCT_ZONA_RESIDENCIAL', 'INDUS': 'PCT_ZONA_INDUSTRIAL', 'CHAS': 'RIO_CHARLES', 'NOX': 'OXIDO_NITROSO_PPM', 'RM': 'N_HABITACIONES_MEDIO', 'AGE': 'PTC_CASAS_405', 'DIS': 'DIS_AUTOPISTAS', 'RAD': 'CARGA_FISCAL', 'TAX': 'PTC_NEGRA', 'PTRATIO': 'RATIO_PROFESORES', 'B': 'PCT_CLASE_BAJA', 'LSTAT': 'PTC_CLASE_BAJA'})"
 },
 {
  "nbid": "NB_536120",
  "file": "NB_536120,df-1.ipynb",
  "row_id": 4,
  "target_code": "df = df[df['parent-location'                 ]=='Cook County, Illinois'                       ]\ndf.head()",
  "generation": "df = df[df['name'].str.contains('Cook County, Illinois')]\ndf.head()"
 },
 {
  "nbid": "NB_61226",
  "file": "NB_61226,df,pop_min-1.ipynb",
  "row_id": 4,
  "target_code": "df = df[['STNAME'        , 'CTYNAME'          ,'CENSUS2010POP'               ]]\ndf.head()",
  "generation": "df = df[['STNAME', 'CTYNAME', 'CENSUS2010POP']]\ndf.head()"
 },
 {
  "nbid": "NB_2379289",
  "file": "NB_2379289,X_train-1.ipynb",
  "row_id": 11,
  "target_code": "stddev = X_train.std()\nmean = X_train.mean()\nX_train = (X_train - mean)/stddev\nX_train.head()",
  "generation": "pca = PCA()\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\nX_train_pca_df = pd.DataFrame(X_train_pca)\nX_test_pca_df = pd.DataFrame(X_test_pca)\nX_train_pca_df.head()"
 },
 {
  "nbid": "NB_2383563",
  "file": "NB_2383563,df-1.ipynb",
  "row_id": 8,
  "target_code": "df = df.drop(['Name'      ,'PassengerId'             ,'Ticket'        , 'Cabin'        ], axis = 1)\ndftest = dftest.drop(['Name'      ,'PassengerId'             ,'Ticket'        , 'Cabin'        ], axis = 1)\ndf.head()",
  "generation": "dftest.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1, inplace=True)\ndftest.head()"
 },
 {
  "nbid": "NB_2400321",
  "file": "NB_2400321,cars,max_price-1.ipynb",
  "row_id": 2,
  "target_code": "cars['Km_new'        ] = cars.Km.str[:-3]\nN = np.sum(cars.Km == '\u767e\u516c\u91cc\u5185'       )\nRatio = N/cars.shape[0]\ncars.Km_new.replace('\u767e'   ,'0.005'       , inplace=True)\ncars['Km_new'        ] = cars.Km_new.str[:-3]\ncars.Km_new = cars.Km_new.astype('float'       )\ncars.head()",
  "generation": "cars['Km_new'] = cars.Km.str.replace('\u4e07\u516c\u91cc','').astype('float')\ncars.head(5)"
 },
 {
  "nbid": "NB_2400321",
  "file": "NB_2400321,cars,max_price-1.ipynb",
  "row_id": 3,
  "target_code": "cars['New_price_new'               ] = cars.New_price.str[:-1]\nN = np.sum(cars.New_price == '\u6682\u65e0'     )\nRatio = N/cars.shape[0]\nRatio\ncars = cars.loc[cars.New_price != '\u6682\u65e0'     ,:]\ncars.New_price_new = cars.New_price_new.astype('float'       )\ncars.head()\ncars.describe()",
  "generation": "# \u5220\u9664\u201c\u4e07\u201d\u5b57\ncars['New_price_new'] = cars.New_price.str[:-1]\n# \u5c06\u201c\u6682\u65e0\u201d\u66ff\u6362\u4e3a0\ncars.New_price_new.replace('\u6682\u65e0', '0', inplace=True)\n# \u6570\u636e\u7c7b\u578b\u8f6c\u6362\ncars.New_price_new = cars.New_price_new.astype('float')\n# \u6570\u636e\u96c6\u7684\u6982\u89c8\u4fe1\u606f\ncars[['Sec_price','year','month','diff_months','Km_new','New_price_new']].describe()"
 },
 {
  "nbid": "NB_2406715",
  "file": "NB_2406715,Default,y-1.ipynb",
  "row_id": 5,
  "target_code": "Default['default2'          ] = Default['default'         ].factorize()[0]\nX = Default.balance.values.reshape(-1,1) \ny = Default.default2\nDefault.head()",
  "generation": "Default['default2'] = Default['default'].replace({'No':0,'Yes':1})"
 },
 {
  "nbid": "NB_2415658",
  "file": "NB_2415658,test-1.ipynb",
  "row_id": 54,
  "target_code": "import pandas as pd\ntest=pd.concat([test,a,b,c], axis=1)\ntest.head()",
  "generation": "test = pd.read_csv('test.csv')\na = pd.get_dummies(test['Sex'])\nb = pd.get_dummies(test['Embarked'])\nc = pd.get_dummies(test['Pclass'])\ntest = pd.concat([test, a, b, c], axis=1)\ntest.drop(['Sex', 'Embarked', 'Pclass'], axis=1, inplace=True)\ntest.head()"
 },
 {
  "nbid": "NB_2425016",
  "file": "NB_2425016,data,X-1.ipynb",
  "row_id": 57,
  "target_code": "temp2 = pd.get_dummies(data['COUNTRY_NAME'              ], prefix = 'country'          , drop_first = True)\ndata = pd.concat([data, temp2], axis = 1)\ndata.head()",
  "generation": "ohe = OneHotEncoder()\ncountry_ohe = ohe.fit_transform(data['COUNTRY_NAME'].values.reshape(-1,1)).toarray()\ncountry_ohe_df = pd.DataFrame(country_ohe, columns=['country_'+str(int(i)) for i in range(country_ohe.shape[1])])\ndata = pd.concat([data, country_ohe_df], axis=1)\ndata.head()"
 },
 {
  "nbid": "NB_2425016",
  "file": "NB_2425016,data,X-1.ipynb",
  "row_id": 14,
  "target_code": "data = data.drop(['COUNTRY_NAME'              ], axis = 1)\ndata.head()",
  "generation": "data = data.drop(['COUNTRY_NAME'], axis = 1)\ntemp2 = pd.get_dummies(data['QUARTER'], prefix = 'quarter', drop_first = True)\ndata = pd.concat([data, temp2], axis = 1)\ndata = data.drop(['QUARTER'], axis = 1)\ndata.head()"
 },
 {
  "nbid": "NB_2430313",
  "file": "NB_2430313,uberdf-1.ipynb",
  "row_id": 7,
  "target_code": "uberdf = uberdf.drop(['Driver id'           ], axis=1)\nuberdf.head()",
  "generation": "uberdf = uberdf.drop(['Unnamed: 0', 'Driver id'], axis=1)\nuberdf.head()"
 },
 {
  "nbid": "NB_2433481",
  "file": "NB_2433481,food_rating-1.ipynb",
  "row_id": 31,
  "target_code": "food_rating = food_rating.replace(0,np.nan)\nmissing_data = food_rating.isnull()\nfor column in missing_data.columns.values.tolist():\n    print(column)\n    print (missing_data[column].value_counts())\n    print(\"\"  )  \nfood_rating.head(7)",
  "generation": "food_rating[['Tip Count', 'Likes']] = food_rating.groupby('Category')[['Tip Count', 'Likes']].transform(lambda x: x.fillna(x.mean()))\nfood_rating.head()"
 },
 {
  "nbid": "NB_2433481",
  "file": "NB_2433481,food_rating-1.ipynb",
  "row_id": 32,
  "target_code": "for column in ('Rating'        , 'Likes'        , 'Tip Count'            ):\n    has_val = food_rating[column].notnull()\n    mean = food_rating[has_val].groupby('Category'          )[column].mean()\n    food_rating.loc[~has_val, column] = food_rating['Category'          ].map(mean)\nfood_rating.head(7)",
  "generation": "food_rating[['Rating', 'Tip Count', 'Likes']] = food_rating.groupby('Category')[['Rating', 'Tip Count', 'Likes']].transform(lambda x: x.fillna(x.mean()))\nfood_rating = food_rating.replace(np.nan, 0)"
 },
 {
  "nbid": "NB_2433481",
  "file": "NB_2433481,food_rating-1.ipynb",
  "row_id": 34,
  "target_code": "food_rating = food_rating.replace(np.nan,0)\nfood_rating.head(7)",
  "generation": "# Replace 0 values with NaN\nfood_rating = food_rating.replace(0,np.nan)\n\n# Find the mean value of rows with the same category\nfor column in ('Rating', 'Likes', 'Tip Count'):\n    has_val = food_rating[column].notnull()\n    mean = food_rating[has_val].groupby('Category')[column].mean()\n    food_rating.loc[~has_val, column] = food_rating['Category'].map(mean)\n\n# Replace NaN values with 0\nfood_rating = food_rating.fillna(0)\n\nfood_rating"
 },
 {
  "nbid": "NB_2434217",
  "file": "NB_2434217,sdf,x-1.ipynb",
  "row_id": 6,
  "target_code": "sdf['income_log'            ] = sdf['income'        ].apply(math.log10)\nsdf.head(5)",
  "generation": "sdf['income_log'] = sdf['income'].apply(lambda x: math.log(x,10))\nsdf.head(3)"
 },
 {
  "nbid": "NB_2453114",
  "file": "NB_2453114,data-1.ipynb",
  "row_id": 13,
  "target_code": "data = data.drop(('Mjob'      ), axis=1)\ndata = data.drop(('Fjob'      ), axis=1)\ndata = data.drop(('reason'        ), axis=1)\ndata = data.drop(('guardian'          ), axis=1)\ndata = pd.concat((data, data_nonbinary), axis=1)\ndata = pd.DataFrame(data, dtype = int)\ndata.head()",
  "generation": "nonbinary_columns = ['Mjob', 'Fjob', 'reason', 'guardian']\ndata_nonbinary = pd.get_dummies(data[nonbinary_columns])\ndata = pd.concat([data, data_nonbinary], axis=1)\ndata.drop(nonbinary_columns, axis=1, inplace=True)\ndata.head()"
 },
 {
  "nbid": "NB_2454088",
  "file": "NB_2454088,glass,y-1.ipynb",
  "row_id": 44,
  "target_code": "glass['household_pred_class'                      ] = np.where(glass.household_pred >= 0.5, 1, 0)\nglass.head()",
  "generation": "from sklearn.linear_model import LinearRegression\n\nX = glass[['al']]\ny = glass['ri']\n\nlr = LinearRegression()\nlr.fit(X, y)\n\nglass['ri_pred'] = lr.predict(X)\n\nglass.head()"
 },
 {
  "nbid": "NB_2454354",
  "file": "NB_2454354,data-1.ipynb",
  "row_id": 8,
  "target_code": "data.coffees = pd.to_numeric(data.coffees, errors = \"coerce\"         )\ndata.head()",
  "generation": "data['coffees'] = pd.to_numeric(data['coffees'], errors='coerce')\ndata.head()"
 },
 {
  "nbid": "NB_2458881",
  "file": "NB_2458881,X-1.ipynb",
  "row_id": 5,
  "target_code": "X = pd.get_dummies(X)\nX.head()",
  "generation": "import pandas as pd\nfrom sklearn import tree\ntitanic_data = pd.read_csv('train.csv')\nY = titanic_data['Survived']\nX = titanic_data.drop(['PassengerId','Survived', 'Name', 'Ticket', 'Cabin'], axis=1)\nX = pd.get_dummies(X)\nX = X.fillna(X.mean())\nX.head()"
 },
 {
  "nbid": "NB_2464029",
  "file": "NB_2464029,df-1.ipynb",
  "row_id": 2,
  "target_code": "df.index=pd.to_datetime(df.index,format='%Y'    )\ndf.head()",
  "generation": "df['DATE'] = pd.to_datetime(df['DATE'], format='%Y')\ndf = df.set_index('DATE')\ndf.index.name = None\ndf.columns = pd.MultiIndex.from_product([['TAVG'], ['']])\ndf.index = df.index.strftime('%Y-%m-%d')\ndf.head()"
 },
 {
  "nbid": "NB_2479182",
  "file": "NB_2479182,traindf-1.ipynb",
  "row_id": 8,
  "target_code": "traindf = traindf.drop([\"\u8a69\u540d\"    ], axis=1)\ntestdf = testdf.drop([\"\u8a69\u540d\"    ], axis=1)\ntraindf",
  "generation": "traindf.drop(['\u8a69\u540d'], axis=1, inplace=True)\ntraindf.head()"
 },
 {
  "nbid": "NB_402453",
  "file": "NB_402453,patient_conversations-1.ipynb",
  "row_id": 6,
  "target_code": "patient_conversations= patient_conversations[[\"TRANS_CONV_TEXT\"                 ,\"Patient_Tag\"             ]]\npatient_conversations.head(2)",
  "generation": "patient_conversations = patient_conversations[['TRANS_CONV_TEXT', 'Patient_Tag']]"
 },
 {
  "nbid": "NB_402453",
  "file": "NB_402453,patient_conversations-1.ipynb",
  "row_id": 21,
  "target_code": "patient_conversations['Text'      ] = patient_conversations['Text'      ].apply(stopwords)\npatient_conversations.head(10)",
  "generation": "vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(patient_conversations['TRANS_CONV_TEXT'])\ndf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\ndf['Pat_Tag'] = patient_conversations['Patient_Tag']\ndf = df.rename(columns={'TRANS_CONV_TEXT': 'Text'})\ndf"
 },
 {
  "nbid": "NB_479770",
  "file": "NB_479770,df-1.ipynb",
  "row_id": 16,
  "target_code": "df = df.drop(['Serial No.'            ], axis=1)\ndf.head(5)",
  "generation": "df = df.drop(columns=['Serial No.'])\ndf.head()"
 },
 {
  "nbid": "NB_487327",
  "file": "NB_487327,euData-1.ipynb",
  "row_id": 7,
  "target_code": "euData = euData.iloc[0:28]\neuData = euData[['geo\\\\time'           ,'2016'      ]]\neuData.columns = ['Country'         ,'Unemployment'              ]\neuData.head()",
  "generation": "euData = euData.drop(['geo\\\\time'], axis=1)\neuData = euData.drop([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\neuData = euData.transpose()\neuData.columns = ['Unemployment']\neuData = euData.reset_index()\neuData = euData.rename(columns={'index': 'Year'})\neuData = euData.drop([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\neuData = euData.reset_index(drop=True)\neuData.head()"
 },
 {
  "nbid": "NB_489605",
  "file": "NB_489605,df_clean-1.ipynb",
  "row_id": 17,
  "target_code": "df_clean[['registration year/month'                         ]] = df_clean[['registration year/month'                         ]].applymap(cleanYear)\ndf_clean[['manufacture year/month'                        ]] = df_clean[['manufacture year/month'                        ]].applymap(cleanYear)\ndf_clean['makes'       ] = df_clean['car make'          ].apply(getMakes)\ndf_clean['model code'            ] = df_clean['model code'            ].apply(cleanModelCode)\ndf_clean['milleage'          ] = df_clean['milleage'          ].apply(getIntFromString)\ndf_clean['engine size (cc)'                  ] = df_clean['engine size (cc)'                  ].apply(getIntFromString)\ndf_clean['car make'          ] = df_clean['car make'          ].apply(removeSpaces)\ndf_clean.head(100)",
  "generation": "df_clean['makes'] = df_clean['car make']\ndf_clean['makes'] = df_clean['makes'].str.split(' ').str[0]\ndf_clean['milleage'] = df_clean['milleage'].str.replace(',', '').astype(int)\ndf_clean['engine size (cc)'] = df_clean['engine size (cc)'].astype(int)\ndf_clean['registration year/month'] = df_clean['registration year/month'].str.split('/').str[0].astype(int)\ndf_clean['manufacture year/month'] = df_clean['manufacture year/month'].str.split('/').str[0].astype(int)\ndf_clean = df_clean.drop('car make', axis=1)\ndf_clean.head()"
 },
 {
  "nbid": "NB_512633",
  "file": "NB_512633,data_preprocess,x_scaled-1.ipynb",
  "row_id": 59,
  "target_code": "data_preprocess = pd.get_dummies(data_preprocess, columns=[\"x80\"     ])\ndata_preprocess.head()",
  "generation": "# One hot encoding\ndata_preprocess = pd.get_dummies(data_preprocess, columns=['x80'], prefix='x80')\n# Normalization\nmin_max_scaler = preprocessing.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(data_preprocess)\ndata_preprocess = pd.DataFrame(np_scaled, columns=data_preprocess.columns)\n# VIF and linear analysis\nX = data_preprocess.drop(['Financial_Distress'], axis=1)\ny = data_preprocess['Financial_Distress']\nvif = pd.DataFrame()\nvif['VIF Factor'] = [sm.stats.outliers_influence.variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['features'] = X.columns\nvif = vif.sort_values('VIF Factor').reset_index(drop=True)\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\nmodel.summary()"
 },
 {
  "nbid": "NB_512633",
  "file": "NB_512633,data_preprocess,x_scaled-1.ipynb",
  "row_id": 60,
  "target_code": "x = data_preprocess.values \nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndata_preprocess = pd.DataFrame(x_scaled, columns=data_preprocess.columns)\ndata_preprocess.head(3)",
  "generation": "min_max_scaler = preprocessing.MinMaxScaler()\ndata_preprocess[data_preprocess.columns] = min_max_scaler.fit_transform(data_preprocess[data_preprocess.columns])\ndata_preprocess.head()"
 },
 {
  "nbid": "NB_512828",
  "file": "NB_512828,training_data-1.ipynb",
  "row_id": 29,
  "target_code": "training_data = training_data.drop(['Date'      , 'Adj Close'            ], axis = 1)\ntraining_data.head()",
  "generation": "scaler = MinMaxScaler()\ntraining_data = training_data.drop(['Date', 'Adj Close'], axis=1)\ntraining_data = scaler.fit_transform(training_data)\ntraining_data = pd.DataFrame(training_data, columns=['Open', 'High', 'Low', 'Close', 'Volume'])\ntraining_data.head()"
 },
 {
  "nbid": "NB_513814",
  "file": "NB_513814,df,df_normalizado-1.ipynb",
  "row_id": 14,
  "target_code": "scaler = StandardScaler()\nscaler.fit(df.drop(['FTR'     ],axis=1))\ndados_normalizados = scaler.transform(df.drop(['FTR'     ],axis=1))\ndf_normalizado = pd.DataFrame(dados_normalizados, columns=df.columns[1:])\ndf = df[['FTR'     ]]\ndf = pd.concat([df, df_normalizado], axis=1, sort=False)\ndf.head()",
  "generation": "scaler = StandardScaler()\ndf[['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR', 'HTHG', 'HTAG']] = scaler.fit_transform(df[['HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR', 'HTHG', 'HTAG']])\ndf.head()"
 },
 {
  "nbid": "NB_6291283",
  "file": "NB_6291283,df-1.ipynb",
  "row_id": 5,
  "target_code": "df.bedrooms=df.bedrooms.fillna(median_bedrooms)\ndf",
  "generation": "df['bedrooms'] = df['bedrooms'].fillna(median_bedrooms)\ndf.head()"
 },
 {
  "nbid": "NB_6488297",
  "file": "NB_6488297,sentence_df-1.ipynb",
  "row_id": 5,
  "target_code": "sentence_df['sentence'          ] = sentence_df['sentence'          ].apply(lambda x: ' '    .join(RegexpTokenizer(r'\\w+'      ).tokenize(x)))\nsentence_df.head()",
  "generation": "sentence_df['sentence'] = sentence_df['sentence'].str.replace('[^\\w\\s]','').str.replace('\\s+',' ')"
 },
 {
  "nbid": "NB_6500794",
  "file": "NB_6500794,data-1.ipynb",
  "row_id": 7,
  "target_code": "data['smoker'        ] = data['smoker'        ].replace(['Yes'     ], 1)\ndata['smoker'        ] = data['smoker'        ].replace(['No'    ], 0)\ndata.head()",
  "generation": "data['smoker'] = pd.get_dummies(data['smoker'], drop_first=True)\ndata.head()"
 },
 {
  "nbid": "NB_6535823",
  "file": "NB_6535823,query-1.ipynb",
  "row_id": 14,
  "target_code": "s = pandas.DataFrame(predict_data_num, columns=features_num)\nquery = pandas.concat([s, query], axis=1, sort=False)\nquery.head()",
  "generation": "query['age'] = pandas.DataFrame(predict_data_num)\nquery.head()"
 },
 {
  "nbid": "NB_2495730",
  "file": "NB_2495730,df_train-1.ipynb",
  "row_id": 32,
  "target_code": "df_train['author'        ]=df_train['author'        ].astype('category'          )\ndf_train['author2'         ]=df_train['author'        ].cat.codes\ndf_train.head(n=3)\nmid = df_train['author2'         ]\ndf_train.drop(labels=['author2'         ], axis=1,inplace = True)\ndf_train.insert(0, 'author2'          , mid)\ndf_train.head()",
  "generation": "df_train['author2'] = df_train['author'].apply(lambda x: 0 if x == 'EAP' else (1 if x == 'HPL' else 2))\ndf_train = df_train.rename(columns={'id': 'id', 'text': 'text', 'author': 'author', 'cleaned_text': 'cleaned_text', 'cleaned_text_string': 'cleaned_text_string', 'length': 'length', 'num_words': 'num_words', 'num_unique_words': 'num_unique_words', 'num_punctuations': 'num_punctuations', 'num_words_upper': 'num_words_upper', 'num_words_title': 'num_words_title', 'mean_word_len': 'mean_word_len', 'num_stopwords': 'num_stopwords', 'lexical_diversity': 'lexical_diversity', 'w2v_array': 'w2v_array', 'mws_index': 'mws_index', 'eap_index': 'eap_index', 'hpl_index': 'hpl_index', 'author2': 'author2'})\ndf_train = df_train[['Unnamed: 0', 'author2', 'id', 'text', 'author', 'cleaned_text', 'cleaned_text_string', 'length', 'num_words', 'num_unique_words', 'num_punctuations', 'w2v_feature_93', 'w2v_feature_94', 'w2v_feature_95', 'w2v_feature_96', 'w2v_feature_97', 'w2v_feature_98', 'w2v_feature_99', 'mws_index', 'eap_index', 'hpl_index']]\ndf_train.head()"
 },
 {
  "nbid": "NB_2504809",
  "file": "NB_2504809,pizza_prices-1.ipynb",
  "row_id": 29,
  "target_code": "pizza_prices['Mean'      ] =(pizza_prices['menus.amountMax'                 ] + pizza_prices['menus.amountMin'                 ])/2\npizza_prices = pizza_prices.loc[:, ['province'          , 'menus.amountMax'                  , 'menus.amountMin'                  , 'Mean'       ] ]\npizza_prices",
  "generation": "pizza_mean = pizza_new_data.drop_duplicates(['id'])\npizza_prices = pd.DataFrame(pizza_mean.groupby(['province'])[['menus.amountMax', 'menus.amountMin']].mean().reset_index())\npizza_prices['Mean'] = pizza_prices[['menus.amountMax', 'menus.amountMin']].mean(axis=1)\npizza_prices.drop(['menus.amountMax', 'menus.amountMin'], axis=1, inplace=True)\npizza_prices.head()"
 },
 {
  "nbid": "NB_2507550",
  "file": "NB_2507550,df_subset-1.ipynb",
  "row_id": 26,
  "target_code": "df_subset = pd.concat([df_subset,df_subset.person_qualities.str.get_dummies(' '   )],axis=1)\ndf_subset = pd.concat([df_subset,df_subset.person_skills.str.get_dummies(' '   )],axis=1)\ndf_subset",
  "generation": "df_subset['person_gender'] = df_subset['person_gender'].replace({'female': 1, 'male': 0})\ndf_subset = pd.concat([df_subset, df_subset['person_skills'].str.get_dummies(sep=' ')], axis=1)\ndf_subset = pd.concat([df_subset, df_subset['person_experience'].str.get_dummies(sep=' ')], axis=1)\ndf_subset = pd.concat([df_subset, df_subset['person_education'].str.get_dummies(sep=' ')], axis=1)\ndf_subset = pd.concat([df_subset, df_subset['person_qualities'].str.get_dummies(sep=' ')], axis=1)\ndf_subset = df_subset.drop(['person_skills', 'person_experience', 'person_education', 'person_qualities'], axis=1)\ndf_subset = df_subset.rename(columns={'female': 'femaleConfirmed', 'male': 'maleConfirmed'})\ndf_subset"
 },
 {
  "nbid": "NB_2511570",
  "file": "NB_2511570,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.drop(columns=['suicides_no'             , 'population'             , 'country-year'               , 'HDI for year'               , 'generation'             , ' gdp_for_year ($) '                     , 'gdp_per_capita ($)'                     ])\ndf.head()",
  "generation": "df = df[['country', 'year', 'sex', 'age', 'suicides/100k pop']]\ndf.head()"
 },
 {
  "nbid": "NB_2515639",
  "file": "NB_2515639,uber_df-1.ipynb",
  "row_id": 8,
  "target_code": "uber_df['Request Day'             ] = pd.DatetimeIndex(uber_df['Request timestamp'                   ]).day_name()\nuber_df['Request Hour'              ] = pd.DatetimeIndex(uber_df['Request timestamp'                   ]).hour\nuber_df.head()",
  "generation": "uber_df['Request Day' ] = uber_df['Request timestamp'].dt.day_name()\nuber_df['Request Hour'] = uber_df['Request timestamp'].dt.hour\nuber_df.head()"
 },
 {
  "nbid": "NB_2515694",
  "file": "NB_2515694,df-1.ipynb",
  "row_id": 75,
  "target_code": "df[\"highway-mpg\"             ] = 235/df[\"highway-mpg\"             ]\ndf.rename(columns={\"highway-mpg\"              : \"highway-L/100km\"                  }, inplace = True)\ndf.head()",
  "generation": "df['highway-L/100km'] = 235/df[\"highway-mpg\"]\ndf = df.rename(columns={'city-L/100km': 'city-L/100km', 'highway-L/100km': 'highway-L/100km'})"
 },
 {
  "nbid": "NB_2516752",
  "file": "NB_2516752,covid-1.ipynb",
  "row_id": 18,
  "target_code": "state = \"RJ\"     \nsituation = \"last_available_confirmed\"                           \ndata_state =  data.loc[data.state == state, :]\ndata_state = data_state.groupby('date'      ).sum()[situation].reset_index()\ndata_state = data_state.drop(columns=['date'      ])\ndata_state.columns = [state]\ncovid = pd.concat([covid, data_state], ignore_index=False, axis=1)\ncovid",
  "generation": "data_state = pd.DataFrame({'SP': [1, 1, 1, 2, 2], 'RJ': [1, 2, 2, 3, 3]})\ncovid = pd.concat([covid, data_state], ignore_index=False, axis=1)\ncovid.head(10)"
 },
 {
  "nbid": "NB_2516752",
  "file": "NB_2516752,data_state-1.ipynb",
  "row_id": 14,
  "target_code": "data_state = data_state.groupby('date'      ).sum()[situation].reset_index()\ndata_state",
  "generation": "data_state = data_state[['date', 'last_available_confirmed']]\ndata_state = data_state.groupby(['date']).sum().reset_index()\ndata_state.head()"
 },
 {
  "nbid": "NB_2530710",
  "file": "NB_2530710,data-1.ipynb",
  "row_id": 3,
  "target_code": "selected_cols = ['YR--MODAHRMN'              , 'TEMP'       , 'SPD'      ]\ndata = data[selected_cols]\nname_conversion = {'YR--MODAHRMN'              : 'TIME'       , 'SPD'      : 'SPEED'        }\ndata = data.rename(columns=name_conversion)\ndata['Celsius'         ] = (data['TEMP'      ] - 32) / 1.8\ndata.head(3)",
  "generation": "data = data[['YR--MODAHRMN', 'TEMP', 'SPD']]\ndata = data.rename(columns={'YR--MODAHRMN': 'TIME', 'SPD': 'SPEED'})\ndata['Celsius'] = (data['TEMP'] - 32) * 5/9\ndata = data[['TIME', 'TEMP', 'SPEED', 'Celsius']]\ndata.head(3)"
 },
 {
  "nbid": "NB_2532752",
  "file": "NB_2532752,marketing_data,f2,f3,f4-1.ipynb",
  "row_id": 1,
  "target_code": "marketing_data.drop(columns=['y'   ])\nmarketing_data['job'     ] = marketing_data['job'     ].astype('category'          )\nmarketing_data['job'     ] = marketing_data['job'     ].cat.codes\nmarketing_data['marital'         ] = marketing_data['marital'         ].astype('category'          )\nmarketing_data['marital'         ] = marketing_data['marital'         ].cat.codes\nmarketing_data.head(5)",
  "generation": "# Convert categorical variables to numerical\nmarketing_data['job'] = pd.factorize(marketing_data['job'])[0]\nmarketing_data['marital'] = pd.factorize(marketing_data['marital'])[0]\nmarketing_data['education'] = pd.factorize(marketing_data['education'])[0]\nmarketing_data['default'] = pd.factorize(marketing_data['default'])[0]\nmarketing_data['housing'] = pd.factorize(marketing_data['housing'])[0]\nmarketing_data['loan'] = pd.factorize(marketing_data['loan'])[0]\nmarketing_data['contact'] = pd.factorize(marketing_data['contact'])[0]\nmarketing_data['month'] = pd.factorize(marketing_data['month'])[0]\nmarketing_data['poutcome'] = pd.factorize(marketing_data['poutcome'])[0]\n\n# Scale numerical variables\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nmarketing_data[['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']] = scaler.fit_transform(marketing_data[['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']])\n\n# Cluster data using KMeans\nkmeans = KMeans(n_clusters=5, random_state=0)\nkmeans.fit(marketing_data)\nmarketing_data['cluster'] = kmeans.labels_\n\n# Replace numerical values with original categorical values\nmarketing_data['job'] = pd.Categorical.from_codes(marketing_data['job'], categories=['admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown'])\nmarketing_data['marital'] = pd.Categorical.from_codes(marketing_data['marital'], categories=['divorced','married','single'])\nmarketing_data['education'] = pd.Categorical.from_codes(marketing_data['education'], categories=['primary','secondary','tertiary','unknown'])\nmarketing_data['default'] = pd.Categorical.from_codes(marketing_data['default'], categories=['no','yes'])\nmarketing_data['housing'] = pd.Categorical.from_codes(marketing_data['housing'], categories=['no','yes'])\nmarketing_data['loan'] = pd.Categorical.from_codes(marketing_data['loan'], categories=['no','yes'])\nmarketing_data['contact'] = pd.Categorical.from_codes(marketing_data['contact'], categories=['cellular','telephone','unknown'])\nmarketing_data['month'] = pd.Categorical.from_codes(marketing_data['month'], categories=['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'])\nmarketing_data['poutcome'] = pd.Categorical.from_codes(marketing_data['poutcome'], categories=['failure','other','success','unknown'])\n\n# Drop the cluster column\nmarketing_data.drop('cluster', axis=1, inplace=True)\n\nmarketing_data.head()"
 },
 {
  "nbid": "NB_2536308",
  "file": "NB_2536308,df,women-1.ipynb",
  "row_id": 3,
  "target_code": "gender_contin_cols = ['gender'        , 'continent'            ]\ngender_contin_dummies = pd.get_dummies(df[gender_contin_cols])\ndf = pd.concat([df[df.columns.difference(gender_contin_cols)], gender_contin_dummies], axis=1)\ndf.head()",
  "generation": "df = pd.get_dummies(df, columns=['gender', 'continent'])\ndf = MinMaxScaler().fit_transform(df)\ndf = pd.DataFrame(df, columns=['Unnamed: 0', '10k', '20k', '25k', '30k', '35k', '40k', '5k', 'age', 'half', 'official', 'overall', 'pace', 'gender_F', 'gender_M', 'continent_Africa', 'continent_Asia', 'continent_Europe', 'continent_N. America', 'continent_Oceania', 'continent_S. America'])\ndf.index = df.index.astype(float)\ndf.head()"
 },
 {
  "nbid": "NB_2537498",
  "file": "NB_2537498,PTCAudit,array,X,Y,fit,PTCAudit_New-1.ipynb",
  "row_id": 3,
  "target_code": "PTCAudit = PTCAudit[PTCAudit.columns.difference(['Disp_Cd'         , 'DISP_CD'          ])]\nPTCAudit.head()",
  "generation": "pca = PCA(n_components=1)\nPTCAudit_pca = pca.fit_transform(PTCAudit)\nPTCAudit_pca = pd.DataFrame(PTCAudit_pca, columns=['PCA'])\nPTCAudit_pca = pd.concat([PTCAudit_pca, PTCAudit.drop(['PCA'], axis=1)], axis=1)\nPTCAudit_pca.head()"
 },
 {
  "nbid": "NB_2538481",
  "file": "NB_2538481,analyst_data,y-1.ipynb",
  "row_id": 6,
  "target_code": "analyst_data['Variability'             ] = analyst_data[['Q1'    ,'Q2'    ,'Q3'    ,'Q4'    ,'Q5'    ,'Q6'    ,'Q7'    ,'Q8'    ]].var(axis=1)\nanalyst_data.head()",
  "generation": "analyst_data['Variability'] = analyst_data.iloc[:, 1:9].std(axis=1)\nanalyst_data.head()"
 },
 {
  "nbid": "NB_2555024",
  "file": "NB_2555024,data-1.ipynb",
  "row_id": 31,
  "target_code": "data['patient_dob'             ]=pd.to_datetime(data['patient_dob'             ])\ndata['appointment_date'                  ]=pd.to_datetime(data['appointment_date'                  ])\ndata.head()",
  "generation": "data[\"patient_dob\"] = pd.to_datetime(data[\"patient_dob\"])\ndata[\"appointment_date\"] = pd.to_datetime(data[\"appointment_date\"])\ndata.head()"
 },
 {
  "nbid": "NB_2555024",
  "file": "NB_2555024,data-1.ipynb",
  "row_id": 37,
  "target_code": "data['prescribed_medicines'                      ]=data['prescribed_medicines'                      ].fillna('no prescription'                 )\ndata['diagnosis'           ]=data['diagnosis'           ].fillna('no diagnosis'              )\ndata.head()",
  "generation": "data['prescribed_medicines'].fillna('no prescription', inplace=True)\ndata['diagnosis'].fillna('no diagnosis', inplace=True)\ndata=data.drop(['id'],axis=1)\ndata=data.drop(['patient_name'],axis=1)\ndata=data.drop(['patient_email'],axis=1)\ndata=data.drop(['doctor_phone'],axis=1)\ndata=data.drop(['patient_nhs_number'],axis=1)\ndata=data.drop(['Unnamed: 0'],axis=1)\ndata.head()"
 },
 {
  "nbid": "NB_2556741",
  "file": "NB_2556741,drs,age_labels,drs_sub-1.ipynb",
  "row_id": 7,
  "target_code": "drs = drs.join(drs['Gender_Age_Marital_Status'                           ].str.split('_'   , 2, expand=True).rename(columns={0: 'Gender'         , 1: 'Age'      , 2: 'Marital_Status'                 }))\ndrs = drs.drop(columns=['Gender_Age_Marital_Status'                           ])\ndrs.head()",
  "generation": "city_category_le.fit(drs.City_Category)\nage_le.fit(drs.Age)\ndrs_sub.City_Category_Label = city_category_le.transform(drs_sub.City_Category)\ndrs_sub.Age_Label = age_le.transform(drs_sub.Age)\ndrs_sub = pd.get_dummies(drs_sub, columns=['Age_Label', 'City_Category_Label'])\ndrs_sub = drs_sub.drop(['Age', 'City_Category'], axis=1)\ndrs_sub.head()"
 },
 {
  "nbid": "NB_2556741",
  "file": "NB_2556741,drs,age_labels,drs_sub-1.ipynb",
  "row_id": 12,
  "target_code": "new_purchase_values = round(drs_grouped_by_product.mean(),2).to_frame().reset_index()\ndrs = drs.merge(new_purchase_values, how='left'      , on='Product_ID'            ).drop(columns=['Purchase_x'            ]).rename(columns={'Purchase_y'            :'Purchase'          })\ndrs",
  "generation": "city_category_le.fit(drs['City_Category'])\ndrs_sub['City_Category_Label'] = city_category_le.transform(drs['City_Category'])\nage_le.fit(drs['Age'])\ndrs_sub['Age_Label'] = age_le.transform(drs['Age'])\ndrs['Purchase'] = drs.Purchase.astype(float)/100\ndrs = drs.join(drs['Gender_Age_Marital_Status'].str.split('_', 2, expand=True).rename(columns={0: 'Gender', 1: 'Age', 2: 'Marital_Status'}))\ndrs = drs.drop(columns=['Gender_Age_Marital_Status'])\ndrs = drs[['User_ID', 'Product_ID', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3', 'Gender', 'Age', 'Marital_Status', 'Purchase']]\ndrs.head()"
 },
 {
  "nbid": "NB_2559117",
  "file": "NB_2559117,global_warming-1.ipynb",
  "row_id": 5,
  "target_code": "global_warming = global_warming.drop(columns =['Country Code'              , 'Indicator Name'                 , 'Indicator Code'                 ]).set_index('Country Name'              )\nglobal_warming.head()",
  "generation": "global_warming = global_warming.drop(['Country Code', 'Indicator Name', 'Indicator Code'], axis=1).set_index('Country Name')\nplt.figure(figsize=(10,10))\nsns.heatmap(global_warming, cmap='coolwarm')"
 },
 {
  "nbid": "NB_2559273",
  "file": "NB_2559273,insurance_data-1.ipynb",
  "row_id": 9,
  "target_code": "insurance_data[\"claim_amount\"              ]=pd.to_numeric(insurance_data.claim_amount.str[1:].replace(','   , ''   , regex=True))\ninsurance_data.head()",
  "generation": "insurance_data['claim_amount'] = insurance_data['claim_amount'].str.replace('$','').astype(float)\ninsurance_data.head()"
 },
 {
  "nbid": "NB_2567115",
  "file": "NB_2567115,X-1.ipynb",
  "row_id": 7,
  "target_code": "for col in X.columns:\n    if isinstance(X[col].iloc[0], np.number):\n        X[col] = X[col].fillna(X[col].mean())\n    else:\n        X[col] = X[col].fillna(X[col].mode().values[0])\nX.head()",
  "generation": "X = X.fillna(X.mean())\nX['Cabin'] = X['Cabin'].fillna('B96 B98')\nX['Embarked'] = X['Embarked'].fillna('S')\nX.head()"
 },
 {
  "nbid": "NB_2576081",
  "file": "NB_2576081,y_train_pred_final-1.ipynb",
  "row_id": 35,
  "target_code": "y_train_pred_final['Predicted'           ] = y_train_pred_final.Diabetics_prob.map(lambda x :1 if x>0.5 else 0)\ny_train_pred_final.head()",
  "generation": "y_train_pred_final['Predicted'] = y_train_pred_final.Diabetics_prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()"
 },
 {
  "nbid": "NB_409497",
  "file": "NB_409497,df-1.ipynb",
  "row_id": 22,
  "target_code": "volume = df['x'   ]*df['y'   ]*df['z'   ]\ndf = df.assign(volume = df['x'   ]*df['y'   ]*df['z'   ])\ndf.head()",
  "generation": "df['volume'] = df['x']*df['y']*df['z']\ndf = df.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_409964",
  "file": "NB_409964,caracteristicas,ocurr_verd,ocurr_falsas-1.ipynb",
  "row_id": 70,
  "target_code": "caracteristicas = caracteristicas[['ocurrencias_verd'                  , 'ocurrencias_falsas'                     , 'ocurrencias'              , 'veracidad'            , 'long_prom_char'                 , 'long_prom_char_v'                   , 'long_prom_char_f'                   , 'long_prom_word'                 , 'long_prom_word_v'                   , 'long_prom_word_f'                   ]]\ncaracteristicas",
  "generation": "caracteristicas = caracteristicas[['ocurrencias_verd', 'ocurrencias_falsas', 'ocurrencias', 'veracidad', 'long_prom_char', 'long_prom_char_v', 'long_prom_char_f', 'long_prom_word', 'long_prom_word_v', 'long_prom_word_f']]\ncaracteristicas"
 },
 {
  "nbid": "NB_409964",
  "file": "NB_409964,tweets_por_keyword-1.ipynb",
  "row_id": 18,
  "target_code": "tweets_por_keyword['about_disaster_false'                      ] = tweets_por_keyword['count'       ] - tweets_por_keyword['about_disaster_true'                     ]\ntweets_por_keyword['veracity'          ] =  tweets_por_keyword['about_disaster_true'                     ]/tweets_por_keyword['count'       ]\ntweets_por_keyword = tweets_por_keyword[['about_disaster_true'                     , 'about_disaster_false'                       , 'count'        , 'veracity'           , 'char_length_mean'                   , 'word_length_mean'                   ]]\ntweets_por_keyword.sort_values(by='veracity'          , ascending=False, inplace=True)\ntweets_por_keyword.head(10)",
  "generation": "tweets_por_keyword = tweets_final.groupby(['keyword'])\\\n                    .agg({'about_disaster':['sum', lambda x: len(x)-sum(x)], 'char_length':['mean'], 'word_length':['mean']})\ntweets_por_keyword.columns = ['about_disaster_true', 'about_disaster_false', 'count', 'veracity', 'char_length_mean', 'word_length_mean']\ntweets_por_keyword.reset_index(inplace=True)\ntweets_por_keyword.head()"
 },
 {
  "nbid": "NB_410853",
  "file": "NB_410853,bank-1.ipynb",
  "row_id": 13,
  "target_code": "bank=bank.replace(replaceStruct)\nbank.head(10)",
  "generation": "bank.replace(replaceStruct, inplace=True)\nbank.head()"
 },
 {
  "nbid": "NB_424754",
  "file": "NB_424754,drug_use-1.ipynb",
  "row_id": 72,
  "target_code": "drug_use = drug_use.replace('-'   , np.nan)\ndrug_use.head()",
  "generation": "drug_use.replace('-', np.nan, inplace=True)"
 },
 {
  "nbid": "NB_426792",
  "file": "NB_426792,y_train_pred_final-1.ipynb",
  "row_id": 145,
  "target_code": "numbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()",
  "generation": "cutoffs = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\ncols = ['Converted', 'Conversion_Prob', 'LeadNumber']\nfor i in cutoffs:\n    y_train_pred_final[i] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()"
 },
 {
  "nbid": "NB_432938",
  "file": "NB_432938,df-1.ipynb",
  "row_id": 19,
  "target_code": "df.drop(one_hot_features,axis=1,inplace=True)\ndf = pd.concat([df, one_hot_encoded] ,axis=1)\ndf",
  "generation": "df = pd.read_csv('/Users/Aziz/Desktop/Machine-Learning-Final-Project/y_test_predicted.csv')\ndf.head()"
 },
 {
  "nbid": "NB_6376821",
  "file": "NB_6376821,data-1.ipynb",
  "row_id": 4,
  "target_code": "data = data[[\"Review Text\"             ,\"Rating\"        ]]\ndata.head(3)",
  "generation": "data = data[[\"Review Text\", \"Rating\"]]\ndata.head()"
 },
 {
  "nbid": "NB_6376821",
  "file": "NB_6376821,data-1.ipynb",
  "row_id": 5,
  "target_code": "data[\"Rating\"        ] = (data[\"Rating\"        ] >= 4).astype(int)\ndata.head(3)",
  "generation": "data['sentiment'] = data['Rating'].apply(lambda rating : 1 if rating > 3 else 0) \ndata = data.drop(['Rating'], axis=1) \ndata"
 },
 {
  "nbid": "NB_6398009",
  "file": "NB_6398009,data-1.ipynb",
  "row_id": 2,
  "target_code": "data = data.replace(np.nan, 'Unknown'          , regex = True)\ndata.head()",
  "generation": "data.fillna(value={'director': 'Unknown', 'cast': 'Unknown'}, inplace=True)\ndata.head()"
 },
 {
  "nbid": "NB_6398009",
  "file": "NB_6398009,data-1.ipynb",
  "row_id": 5,
  "target_code": "data['features'          ] = important_features(data)\ndata.head()",
  "generation": "data['features'] = data['cast'] + ' ' + data['listed_in'] + ' ' + data['director']\ndata.head()"
 },
 {
  "nbid": "NB_2588051",
  "file": "NB_2588051,data-1.ipynb",
  "row_id": 5,
  "target_code": "data = data.sample(frac=1).reset_index(drop=True)   \ndata.head()",
  "generation": "data = data.sample(frac=1).reset_index(drop=True)\ndata.tail()"
 },
 {
  "nbid": "NB_2593028",
  "file": "NB_2593028,df-1.ipynb",
  "row_id": 22,
  "target_code": "df = df[['bgr'     ,'wc'    ,'rc'    ]]\ndf.head()",
  "generation": "df = df[['bgr', 'wc', 'rc']]\ndf.head()"
 },
 {
  "nbid": "NB_2602919",
  "file": "NB_2602919,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.drop('Theoretical_Power_Curve (KWh)'                               , axis = 1)\ndf.head()",
  "generation": "df = df.drop(columns=['Theoretical_Power_Curve (KWh)'])\ndf.head()"
 },
 {
  "nbid": "NB_2606788",
  "file": "NB_2606788,dataset-1.ipynb",
  "row_id": 8,
  "target_code": "dataset = dataset.dropna(axis=1)\ndataset.head()",
  "generation": "dataset = dataset.replace('?', np.nan)\ndataset = dataset.apply(lambda x:x.fillna(x.value_counts().index[0]))\ndataset = dataset.apply(le.fit_transform)\ndataset.head()"
 },
 {
  "nbid": "NB_2606788",
  "file": "NB_2606788,dataset-1.ipynb",
  "row_id": 9,
  "target_code": "dataset = dataset.drop('Histological Type'                   ,1)\ndataset.head()",
  "generation": "dataset.iloc[:, 1:5] = dataset.iloc[:, 1:5].apply(le.fit_transform)\ndataset = dataset.dropna(axis=1) \ndataset"
 },
 {
  "nbid": "NB_2606788",
  "file": "NB_2606788,dataset-1.ipynb",
  "row_id": 10,
  "target_code": "dataset = dataset.drop('Age'     ,1)\ndataset.head()",
  "generation": "dataset = dataset.drop('Age', 1)\ndataset = dataset.apply(le.fit_transform)\ndataset.head()"
 },
 {
  "nbid": "NB_2608179",
  "file": "NB_2608179,df-1.ipynb",
  "row_id": 1,
  "target_code": "df['total_price'             ] = df['qty'     ] * df['unit_price'            ]\ndf.head(3)",
  "generation": "df['total_price'] = df['qty'] * df['unit_price']\ndf.head(3)"
 },
 {
  "nbid": "NB_2608179",
  "file": "NB_2608179,df-1.ipynb",
  "row_id": 2,
  "target_code": "df['qty_long'          ] = np.log(df['qty'     ])\ndf.head(3)",
  "generation": "df['qty_long'] = np.log(df['qty'])\ndf.head(3)"
 },
 {
  "nbid": "NB_2619616",
  "file": "NB_2619616,data-1.ipynb",
  "row_id": 9,
  "target_code": "data = pd.DataFrame(data,columns=['year'      ,'GDPc'      ,'GDP2009'         ])\ndata.set_index('year'      ,inplace=True)\ndata.head(10)",
  "generation": "gdp = pd.read_excel('gdplev.xls', skiprows=7, usecols=[4,6], names=['year', 'GDP'], header=None)\ngdp = gdp[gdp.year >= '2000q1']\ngdp['GDP'] = gdp['GDP'].div(gdp.iloc[0]['GDP'])\ngdp.head()"
 },
 {
  "nbid": "NB_2620264",
  "file": "NB_2620264,df,demented_women,demented_women_count,demented_men,demented_men_count,non_demented,non_demented_women,non_demented_women_count,no-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.loc[df['Delay'       ].isnull()]\ndf = df.reset_index(drop=True)\ndf = df.drop(['ID'    , 'Delay'        , 'Hand'       ], axis=1)\ndf['M/F'     ] = df['M/F'     ].replace(['F'   ,'M'   ], [0,1]) \ngroup = df['CDR'     ]\ngroup = group.apply(lambda cdr: 1 if cdr>=0.5 else 0)\ngroup_name = group.apply(lambda cdr: 'Demented'            if cdr>=0.5 else 'Nondemented'              )\ndf['Group'       ] = group\ndf['Classification'                ] = group_name\ndf.head()",
  "generation": "df = df[df['Delay'].isnull()]\ndf['Group'] = df['CDR'].apply(lambda x: 'Demented' if x>0 else 'Nondemented')\ndf['Classification'] = df['Group'].apply(lambda x: 1 if x=='Demented' else 0)\ndf = df[['M/F', 'Age', 'Educ', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF', 'Group', 'Classification']]\ndf.head()"
 },
 {
  "nbid": "NB_2636065",
  "file": "NB_2636065,diamonds-1.ipynb",
  "row_id": 6,
  "target_code": "diamonds = diamonds.drop(diamonds.columns[[0]], axis = 1)\ndiamonds.head(5)",
  "generation": "diamonds = diamonds.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])"
 },
 {
  "nbid": "NB_2640920",
  "file": "NB_2640920,conicyt-1.ipynb",
  "row_id": 3,
  "target_code": "conicyt=conicyt.rename(columns = {'a\u00f1o'     :'A\u00f1o'     , 'n_concursados'                :'Concursados'             , 'n_aprobados'              :'Aprobados'           })\nconicyt['Tasa de aprobaci\u00f3n'                    ]=np.round(conicyt['Aprobados'           ]/conicyt['Concursados'             ]*100,decimals=1)\nconicyt_postdoc=conicyt.copy()\nconicyt.head()",
  "generation": "conicyt['Tasa de aprobaci\u00f3n'] = (conicyt['n_aprobados']/conicyt['n_concursados'])*100\nconicyt = conicyt.rename(columns={'a\u00f1o': 'A\u00f1o', 'n_concursados': 'Concursados', 'n_aprobados': 'Aprobados'})\nconicyt = conicyt.reset_index(drop=True)\nconicyt.head()"
 },
 {
  "nbid": "NB_2640920",
  "file": "NB_2640920,conicyt_doctorado-1.ipynb",
  "row_id": 7,
  "target_code": "conicyt_doctorado=conicyt_doctorado.rename(columns = {'a\u00f1o'     :'A\u00f1o'     })\nconicyt_doctorado['Aprobados'           ]=conicyt_doctorado.iloc[:,1:6].sum(axis=1)\nconicyt_doctorado.head()",
  "generation": "conicyt_doctorado['Aprobados'] = conicyt_doctorado['doctorado_nacional'] + conicyt_doctorado['doctorado_extranjeros'].fillna(0) + conicyt_doctorado['doctorado_conicyt_internacional'] + conicyt_doctorado['doctorado_bpr_internacional'] + conicyt_doctorado['doctorado_becas_chile'].fillna(0)\nconicyt_doctorado.head()"
 },
 {
  "nbid": "NB_2640920",
  "file": "NB_2640920,mineduc_doctorado-1.ipynb",
  "row_id": 9,
  "target_code": "mineduc_doctorado=mineduc_doctorado.rename(columns = {'a\u00f1o'     :'A\u00f1o'     , 'matriculados'               :'Matriculados'              })\nmineduc_doctorado.head()",
  "generation": "mineduc_doctorado = mineduc_doctorado.rename(columns={'a\u00f1o': 'A\u00f1o'})\nmineduc_doctorado.head()"
 },
 {
  "nbid": "NB_2643213",
  "file": "NB_2643213,times-1.ipynb",
  "row_id": 49,
  "target_code": "times['Vit\u00f3rias_total'                ] = times['Vit\u00f3rias casa'               ] + times['Vit\u00f3rias fora'               ]\ntimes['Derrotas_total'                ] = times['Derrotas casa'               ] + times['Derrotas fora'               ]\ntimes.head()",
  "generation": "times['Vit\u00f3rias_total'] = times['Vit\u00f3rias casa'] + times['Vit\u00f3rias fora']\ntimes['Derrotas_total'] = times['Derrotas casa'] + times['Derrotas fora']\ntimes.head()"
 },
 {
  "nbid": "NB_2644045",
  "file": "NB_2644045,df_cleaned,Ytrain-1.ipynb",
  "row_id": 93,
  "target_code": "clean_date = df['watch-time-text'                 ].str.extract(r'([A-z]{3}) (\\d+), (\\d+)'                          )\ndf = df[clean_date[0].notnull()]\ndf_cleaned = df_cleaned[clean_date[0].notnull()]\nclean_date = clean_date[clean_date[0].notnull()]\nclean_date[0] = clean_date[0].map(lambda x: str(x))\nclean_date[1] = clean_date[1].map(lambda x: '0'    +str(x) if len(str(x)) == 1 else str(x)) \nclean_date[2] = clean_date[2].map(lambda x: str(x))\nclean_date = clean_date.apply(lambda x: ' '    .join(x), axis=1)\ndf_cleaned['date'      ] = pd.to_datetime(clean_date, format='%b %d %Y'          )\ndf_cleaned.head()",
  "generation": "clean_date = df_cleaned['date'].dt.strftime('%Y-%m-%d')\ndf_cleaned['date'] = clean_date\ndf_cleaned = df_cleaned[['title', 'date']]\ndf_cleaned.head()"
 },
 {
  "nbid": "NB_2651140",
  "file": "NB_2651140,new_data-1.ipynb",
  "row_id": 4,
  "target_code": "new_data.index = new_data.Date\nnew_data.drop('Date'      , axis=1, inplace=True)\nnew_data.head()",
  "generation": "new_data.set_index('Date', inplace=True)\nnew_data.head()"
 },
 {
  "nbid": "NB_2652738",
  "file": "NB_2652738,df-1.ipynb",
  "row_id": 1,
  "target_code": "df['Robbery2'          ] = np.where(df['Robbery'         ]>0, 1, 0)\ndf['Murder2'         ] = np.where(df['Murder'        ]>0, 1, 0)\nmean_rob=df['Robbery'         ].mean()\nmean_mur=df['Murder'        ].mean()\ndf['Robbery3'          ] = np.where(df['Robbery'         ]>mean_rob, 1, 0)\ndf['Murder3'         ] = np.where(df['Murder'        ]>mean_mur, 1, 0)\ndf.head(5)",
  "generation": "df['Robbery2'] = np.where(df['Robbery']>0, 1, 0)\ndf['Murder2'] = np.where(df['Murder']>0, 1, 0)\ndf['Robbery3'] = np.where(df['Robbery']>0, 'Yes', 'No')\ndf['Murder3'] = np.where(df['Murder']>0, 'Yes', 'No')\ndf.head()"
 },
 {
  "nbid": "NB_2656426",
  "file": "NB_2656426,usage,Y-1.ipynb",
  "row_id": 23,
  "target_code": "usage['tempsqr'         ] = usage['temperature'             ]**2\nusage['yearcts'         ] = usage['year'      ] + (usage['month'       ]-1)/12\nusage.head()",
  "generation": "usage['tempsqr'] = usage.temperature ** 2\nusage['yearcts'] = (usage.year - 1989.5) * (usage.year - 1989.5)"
 },
 {
  "nbid": "NB_2670052",
  "file": "NB_2670052,water_dataset-1.ipynb",
  "row_id": 2,
  "target_code": "for i in water_dataset:\n    if i not in ['mlogp'       , 'rdchi'        , 'gats1p'         , 'lc50'       ]:\n        water_dataset[i] = water_dataset[i].replace(0, water_dataset[i].mean())\nwater_dataset.head(10)",
  "generation": "water_dataset['nn'] = water_dataset['nn'].apply(lambda x: x + 1.003663)\nwater_dataset['c-040'] = water_dataset['c-040'].apply(lambda x: x + 0.35348)\nwater_dataset['h-050'] = water_dataset['h-050'].apply(lambda x: 0.937729)\nwater_dataset['tpsa'] = water_dataset['tpsa'].apply(lambda x: 48.47293)\nwater_dataset['saacc'] = water_dataset['saacc'].apply(lambda x: 58.869018000000004)\nwater_dataset.head()"
 },
 {
  "nbid": "NB_2672118",
  "file": "NB_2672118,data-1.ipynb",
  "row_id": 33,
  "target_code": "data['Margin'        ] = data['W_Votes'         ]-data['R_Votes'         ]\ndata['Margin%'         ] = (100*data['Margin'        ]/data['Total Votes'             ]).round(2)\ndata.head()",
  "generation": "data['Margin'] = data['W_Votes'] - data['R_Votes']\ndata['Margin%'] = ((data['W_Votes'] - data['R_Votes'])/data['Total Votes']*100).round(2)"
 },
 {
  "nbid": "NB_2673827",
  "file": "NB_2673827,temp,y_data-1.ipynb",
  "row_id": 3,
  "target_code": "temp = data.query(\"\u5206\u985e == '\u7dcf\u6570'\"            ).T\ntemp['\u5408\u8a08'    ] = temp['\u8853\u524d'    ] + temp['\u8853\u5f8c'    ]\ntemp = temp[2:].sort_values(by='\u8853\u524d'    )\ntemp",
  "generation": "data = pd.read_csv('span_in_hospital.csv', index_col=0, comment='#')\ntemp = data.query(\"\u5206\u985e == '\u7dcf\u6570'\")\nlabels = data.columns[2:]\nbefore_ope = temp.iloc[0,2:]\nafter_ope = temp.iloc[1,2:]\n\nfrom pylab import rcParams\nrcParams['figure.dpi'] = 150\n\ntotal = [num1 + num2 for num1,num2 in zip(before_ope,after_ope)]\n\nfig,ax = plt.subplots()\nplt.xticks(rotation=90)\nplt.title('\u7dcf\u6570')\nax.bar(labels,total,label='after_ope')\nax.bar(labels,before_ope,label='before_ope')\nax.legend()\ntemp = data.query(\"\u5206\u985e == '\u7dcf\u6570'\").T\ntemp['\u5408\u8a08'] = temp['\u8853\u524d'] + temp['\u8853\u5f8c']\ntemp = temp[2:].sort_values(by='\u5408\u8a08')\ntemp.head()"
 },
 {
  "nbid": "NB_2673827",
  "file": "NB_2673827,temp,y_data-1.ipynb",
  "row_id": 4,
  "target_code": "temp = data.query(\"\u5206\u985e == '\u7dcf\u6570'\"            ).T\ntemp['\u5408\u8a08'    ] = temp['\u8853\u524d'    ] + temp['\u8853\u5f8c'    ]\ntemp = temp[2:].sort_values(by='\u8853\u5f8c'    )\ntemp",
  "generation": "data = pd.read_csv('span_in_hospital.csv', index_col=0, comment='#')\ntemp = data.query(\"\u5206\u985e == '\u7dcf\u6570'\")\nlabels = data.columns[2:]\nbefore_ope = temp.iloc[0,2:]\nafter_ope = temp.iloc[1,2:]\n\nfrom pylab import rcParams\nrcParams['figure.dpi'] = 150\n\ntotal = [num1 + num2 for num1,num2 in zip(before_ope,after_ope)]\n\nfig,ax = plt.subplots()\nplt.xticks(rotation=90)\nplt.title('\u7dcf\u6570')\nax.bar(labels,total,label='after_ope')\nax.bar(labels,before_ope,label='before_ope')\nax.legend()\n\ntemp = data.query(\"\u5206\u985e == '\u7dcf\u6570'\").T\ntemp['\u5408\u8a08'] = temp['\u8853\u524d'] + temp['\u8853\u5f8c']\ntemp = temp[2:].sort_values(by='\u5408\u8a08')\ntemp\n\ntemp = data.query(\"\u5206\u985e == '\u7dcf\u6570'\").T\ntemp['\u5408\u8a08'] = temp['\u8853\u524d'] + temp['\u8853\u5f8c']\ntemp = temp[2:].sort_values(by='\u8853\u524d')\ntemp"
 },
 {
  "nbid": "NB_2680973",
  "file": "NB_2680973,daily_train-1.ipynb",
  "row_id": 13,
  "target_code": "daily_train['ds'    ] = daily_train.index\ndaily_train['y'   ] = daily_train.Count\ndaily_train.drop(['Count'       ], axis=1, inplace=True)\ndaily_train.head()",
  "generation": "daily_train['ds'] = daily_train.index\ndaily_train['y'] = daily_train['Count']\ndaily_train.drop(['Count', 'Datetime'], axis=1, inplace=True)\ndaily_train.head()"
 },
 {
  "nbid": "NB_2686995",
  "file": "NB_2686995,df-1.ipynb",
  "row_id": 3,
  "target_code": "from io import StringIO\ncol=['current_character'                   ,'current_dialogue'                  ]\ndf=df[col]\ndf.head()",
  "generation": "df = df[['current_character', 'current_dialogue']]\ndf.head()"
 },
 {
  "nbid": "NB_2686995",
  "file": "NB_2686995,df-1.ipynb",
  "row_id": 5,
  "target_code": "df=df[pd.notnull(df[\"current_character\"                   ])]\ndf=df[pd.notnull(df[\"current_dialogue\"                  ])]\ndf[\"current_character\"                   ]=df[\"current_character\"                   ].str.strip()\ndf[\"current_dialogue\"                  ]=df[\"current_dialogue\"                  ].str.strip()\ndf.head()",
  "generation": "df.rename(columns={'current_character': 'current_character', 'current_dialogue': 'current_dialogue'}, inplace=True)\ndf['current_character'] = df['current_character'].str.upper()\ndf.head()"
 },
 {
  "nbid": "NB_2686995",
  "file": "NB_2686995,df-1.ipynb",
  "row_id": 7,
  "target_code": "df['category_id'             ] = df['current_character'                   ].factorize()[0]\ndf.head(10)",
  "generation": "df['category_id'] = df['current_character'].factorize()[0]\ncategory_id_df = df[['current_character', 'category_id']].drop_duplicates().sort_values('category_id')\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'current_character']].values)\ndf.head()"
 },
 {
  "nbid": "NB_2693377",
  "file": "NB_2693377,df-1.ipynb",
  "row_id": 17,
  "target_code": "df['survived2'           ] = df['survived'          ]\ndf['survived2'           ] = df['survived2'           ].map({0:'Died'      ,1:'Survived'          })\ndf.head()",
  "generation": "df['survived2'] = df['survived'].apply(lambda x: 'Survived' if x == 1 else 'Died')\ndf.head()"
 },
 {
  "nbid": "NB_2694797",
  "file": "NB_2694797,df-1.ipynb",
  "row_id": 3,
  "target_code": "dates = pd.date_range(start=\"May-2009\"          , end=\"June-2019\"           , freq='M'   )\ndf = df.set_index(dates)\ndf.head()",
  "generation": "df['Unnamed: 0'] = pd.to_datetime(df['Unnamed: 0'])\ndf.set_index('Unnamed: 0', inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_452314",
  "file": "NB_452314,heart-1.ipynb",
  "row_id": 24,
  "target_code": "dummyVars = ['cp'    , 'restecg'          , 'slope'        , 'thal'       ]  \nheart = pd.get_dummies(heart, columns = dummyVars)\nheart.head()",
  "generation": "heart = pd.get_dummies(heart, columns=['restecg', 'slope', 'thal'])"
 },
 {
  "nbid": "NB_456157",
  "file": "NB_456157,data-1.ipynb",
  "row_id": 18,
  "target_code": "data.Month = pd.to_datetime(data.Month)\ndata.set_index('Month'       , inplace=True)\ndata.head()",
  "generation": "data['Month'] = pd.to_datetime(data['Month'])\ndata = data.set_index('Month')\ndata.head()"
 },
 {
  "nbid": "NB_6148888",
  "file": "NB_6148888,books-1.ipynb",
  "row_id": 10,
  "target_code": "books['metadata'          ]= books.apply(create_metadata,axis=1)\nbooks['metadata'          ]= books['metadata'          ].fillna(''  )\nbooks.head()",
  "generation": "metadata = books[['authors', 'tag_name', 'language_code']].apply(lambda x: ' '.join(x), axis=1)\nbooks['metadata'] = metadata\nbooks"
 },
 {
  "nbid": "NB_6170778",
  "file": "NB_6170778,computers,y-1.ipynb",
  "row_id": 7,
  "target_code": "computers['cd'    ] = computers['cd'    ].map({'no'    :0,'yes'     :1})\ncomputers.head()",
  "generation": "computers['multi'] = computers['multi'].map({'no': 0, 'yes': 1})\ncomputers['premium'] = computers['premium'].map({'no': 0, 'yes': 1})\ncomputers.head()"
 },
 {
  "nbid": "NB_6170778",
  "file": "NB_6170778,computers,y-1.ipynb",
  "row_id": 9,
  "target_code": "computers['multi'       ] = computers['multi'       ].map({'no'    :0,'yes'     :1})\ncomputers.head()",
  "generation": "computers['cd'] = computers['cd'].map({'no':0,'yes':1})\ncomputers['premium'] = computers['premium'].map({'no':0,'yes':1})\ncomputers.head()"
 },
 {
  "nbid": "NB_6252784",
  "file": "NB_6252784,data-1.ipynb",
  "row_id": 2,
  "target_code": "data[\"Variance\"          ] = (data[\"Variance\"          ]-data[\"Variance\"          ].min()) / (data[\"Variance\"          ].max()-data[\"Variance\"          ].min())\ndata[\"Skewness\"          ] = (data[\"Skewness\"          ]-data[\"Skewness\"          ].min()) / (data[\"Skewness\"          ].max()-data[\"Skewness\"          ].min())\ndata[\"Curtosis\"          ] = (data[\"Curtosis\"          ]-data[\"Curtosis\"          ].min()) / (data[\"Curtosis\"          ].max()-data[\"Curtosis\"          ].min())\ndata[\"Entropy\"         ] = (data[\"Entropy\"         ]-data[\"Entropy\"         ].min()) / (data[\"Entropy\"         ].max()-data[\"Entropy\"         ].min())\ndata.head()",
  "generation": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndata[['Variance', 'Skewness', 'Curtosis', 'Entropy']] = scaler.fit_transform(data[['Variance', 'Skewness', 'Curtosis', 'Entropy']])\ndata.head()"
 },
 {
  "nbid": "NB_2706123",
  "file": "NB_2706123,iris,iris_scaled-1.ipynb",
  "row_id": 4,
  "target_code": "species = iris['Species'         ]\niris = iris.drop('Species'         , 1)\ncolnames = list(iris.columns.values)\niris_scaled = pd.DataFrame(preprocessing.StandardScaler().fit_transform(iris))\niris_scaled.columns = colnames\niris_scaled.head()",
  "generation": "scaler = preprocessing.StandardScaler()\nscaled_data = scaler.fit_transform(iris.iloc[:, :-1])\niris_scaled = pd.DataFrame(scaled_data, columns=iris.columns[:-1])\niris_scaled.head()"
 },
 {
  "nbid": "NB_2706403",
  "file": "NB_2706403,airbnb_df-1.ipynb",
  "row_id": 15,
  "target_code": "airbnb_df = airbnb_df.drop([\"property_type\"               , \"room_type\"            , \"amenities\"             ], axis = 1)\nairbnb_df.head()",
  "generation": "airbnb_df = pd.read_csv(\"/Users/basillatif/Desktop/AirBnb Project/aribnb_final_clean.csv\")\ndummy_prop = pd.get_dummies(airbnb_df['property_type'])\nroom_dummy = pd.get_dummies(airbnb_df['room_type'])\ntarget = pd.DataFrame(airbnb_df, columns=['price'])\nairbnb_df = pd.concat([airbnb_df[['id', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'price', 'security_deposit', 'cleaning_fee', 'guests_included', 'extra_people', 'last_review', 'availability_365', 'number_of_reviews']], dummy_prop, room_dummy], axis = 1)\nairbnb_df"
 },
 {
  "nbid": "NB_2708716",
  "file": "NB_2708716,df2-1.ipynb",
  "row_id": 52,
  "target_code": "df2 = df2.drop('Hcy'     , axis=1)\ndf2.head()",
  "generation": "df2[['AGE_Years', 'SEX', 'AA_Interp', 'Alanine_P', 'Arginine_P', 'Aspartic_acid_P', 'Citrulline_P', 'Glutamine_P', 'Glutamic_acid_P', 'Glycine_P', 'Methionine_P', 'Ornithine_P', 'Phenylalanine_P', 'Proline_P', 'Serine_P', 'Taurine_P', 'Threonine_P', 'Tyrosine_P', 'Valine_P', 'Cysteine_P', 'ACCESSION']] = df2[['AGE_Years', 'SEX', 'AA_Interp', 'Alanine_P', 'Arginine_P', 'Aspartic_acid_P', 'Citrulline_P', 'Glutamine_P', 'Glutamic_acid_P', 'Glycine_P', 'Methionine_P', 'Ornithine_P', 'Phenylalanine_P', 'Proline_P', 'Serine_P', 'Taurine_P', 'Threonine_P', 'Tyrosine_P', 'Valine_P', 'Cysteine_P', 'ACCESSION']].apply(pd.to_numeric, errors='coerce')\ndf2 = df2.rename(columns={'Alanine_P': 'Ala', 'Arginine_P': 'Arg', 'Aspartic_acid_P': 'Asp', 'Citrulline_P': 'Cit', 'Glutamine_P': 'Gln', 'Glutamic_acid_P': 'Glu', 'Glycine_P': 'Gly', 'Methionine_P': 'Met', 'Phenylalanine_P': 'Phe', 'Proline_P': 'Pro', 'Serine_P': 'Ser', 'Taurine_P': 'Tau', 'Threonine_P': 'Thr', 'Tyrosine_P': 'Tyr', 'Valine_P': 'Val', 'Cysteine_P': 'Cys'})"
 },
 {
  "nbid": "NB_2715763",
  "file": "NB_2715763,dados,y-1.ipynb",
  "row_id": 3,
  "target_code": "a_renomenar ={\n    'expected_hours'                    :'hora_esperadas'                ,\n    'price'           :'preco'       ,\n    'unfinished'                : 'finalizado'             \n}\ndados = dados.rename(columns = a_renomenar)\ndados.head()",
  "generation": "dados = dados.rename(columns={\"Unnamed: 0\":\"finalizado\", \"unfinished\":\"hora_esperadas\", \"expected_hours\":\"preco\"})\ndados.head(10)"
 },
 {
  "nbid": "NB_2715763",
  "file": "NB_2715763,dados,y-1.ipynb",
  "row_id": 5,
  "target_code": "troca = {\n    0 : 1,\n    1 : 0\n}\ndados['finalizado'            ] = dados.finalizado.map(troca)\ndados.head()",
  "generation": "dados['finalizado'] = dados['finalizado'].map({1:0, 0:1})\ndados.head()"
 },
 {
  "nbid": "NB_2717647",
  "file": "NB_2717647,df-1.ipynb",
  "row_id": 5,
  "target_code": "df['appointment_day'                 ] = np.where((df['appointment_day'                 ] - df['scheduled_day'               ]).dt.days < 0, df['scheduled_day'               ], df['appointment_day'                 ])\ndf['awaiting_time'               ] = df['appointment_day'                 ] - df['scheduled_day'               ]\ndf['awaiting_time'               ] = df['awaiting_time'               ].dt.days\ndf.head()",
  "generation": "df['awaiting_time'] = (df['appointment_day'] - df['scheduled_day']).dt.days\ndf.head()"
 },
 {
  "nbid": "NB_2721063",
  "file": "NB_2721063,set1,train_targets,test_data,test_targets-1.ipynb",
  "row_id": 11,
  "target_code": "set1 = set1[['Rate'      , 'shift_5'          , 'shift_4'          , 'shift_3'          , 'shift_2'          , 'shift_1'          ]]\nset1",
  "generation": "set1 = set1[['Rate', 'shift_5', 'shift_4', 'shift_3', 'shift_2', 'shift_1', 'Date']]\nset1.head()"
 },
 {
  "nbid": "NB_2725389",
  "file": "NB_2725389,train-1.ipynb",
  "row_id": 3,
  "target_code": "drop = [\"Ticket\"        ,\"Fare\"      ,\"Age\"     ,\"PassengerId\"             ,\"Pclass\"        ]\ntrain = train.drop(drop,axis =1)\ntrain.head()",
  "generation": "test = test[['Sex', 'SibSp', 'Parch', 'Embarked']]\ntest['target'] = 0\ntest.head()"
 },
 {
  "nbid": "NB_2734297",
  "file": "NB_2734297,media,y-1.ipynb",
  "row_id": 3,
  "target_code": "media = media.drop('Unnamed: 7'            , axis=1)\nmedia.head()",
  "generation": "media = media.drop(columns=['Unnamed: 7'])\nmedia.head()"
 },
 {
  "nbid": "NB_2734297",
  "file": "NB_2734297,media,y-1.ipynb",
  "row_id": 4,
  "target_code": "media['Date'      ] = pd.to_datetime(media['Date'      ]).dt.date\nmedia.head()",
  "generation": "media['Date'] = pd.to_datetime(media['Date'])\nmedia.head()"
 },
 {
  "nbid": "NB_2734297",
  "file": "NB_2734297,media,y-1.ipynb",
  "row_id": 6,
  "target_code": "media['Day'     ] = media['Day'     ].astype(str)\nmedia['Day'     ] = media['Day'     ].map(lambda x: x[0:2])\nmedia['Day'     ] = media['Day'     ].astype(int)\nmedia.head()",
  "generation": "media['Day'] = [i+1 for i in range(len(media))]\nmedia.head()"
 },
 {
  "nbid": "NB_2734297",
  "file": "NB_2734297,media,y-1.ipynb",
  "row_id": 17,
  "target_code": "media['Lag_views'           ] = np.roll(media['Views_show'            ], 1)\nmedia.Lag_views.replace(108961, 0, inplace=True)\nmedia.head()",
  "generation": "media['Weekend'] = np.where(media['Weekday']<5, 0, 1)\nmedia['Lag_views'] = np.roll(media['Views_show'], 1)\nmedia['Lag_views'].replace(108961, 0, inplace=True)\nmedia.head()"
 },
 {
  "nbid": "NB_2737441",
  "file": "NB_2737441,data_dc,animal_type_df-1.ipynb",
  "row_id": 68,
  "target_code": "ohe_enc = OneHotEncoder()\nanimal_type_ohe = ohe_enc.fit_transform(data_dc[['animal_type'             ]]).toarray()\nanimal_type_columns = ohe_enc.categories_[0]\nanimal_type_df = pd.DataFrame(animal_type_ohe, columns=animal_type_columns)\nprint(animal_type_df.head())\ndata_dc = pd.concat([data_dc,  animal_type_df], axis=1)\ndata_dc.drop(['animal_type'             ], axis=1, inplace=True)\ndata_dc.head(10)",
  "generation": "one_hot_enc = OneHotEncoder(sparse=False)\nanimal_type_one_hot = one_hot_enc.fit_transform(data_dc[['animal_type']])\nanimal_type_one_hot_df = pd.DataFrame(animal_type_one_hot, columns=['Cat', 'Dog'])\ndata_dc = pd.concat([data_dc, animal_type_one_hot_df], axis=1)\ndata_dc.drop(['animal_type'], axis=1, inplace=True)\ndata_dc.head()"
 },
 {
  "nbid": "NB_2737441",
  "file": "NB_2737441,data_dc,animal_type_df-1.ipynb",
  "row_id": 73,
  "target_code": "ohe_enc = OneHotEncoder()\nsex_type_ohe = ohe_enc.fit_transform(data_dc[['sex_type'          ]]).toarray()\nsex_type_columns = ohe_enc.categories_[0]\nsex_type_df = pd.DataFrame(sex_type_ohe, columns=sex_type_columns)\nprint(sex_type_df.head())\ndata_dc = pd.concat([data_dc,  sex_type_df], axis=1)\ndata_dc.drop(['sex_type'          ], axis=1, inplace=True)\ndata_dc.head(10)",
  "generation": "# \u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 OneHotEncoder \u0434\u043b\u044f \u0441\u0442\u043e\u043b\u0431\u0446\u0430 'sex_type'\nohe_enc = OneHotEncoder()\nsex_type_ohe = ohe_enc.fit_transform(data_dc[['sex_type']]).toarray()\nsex_type_columns = ohe_enc.categories_[0]\nsex_type_df = pd.DataFrame(sex_type_ohe, columns=sex_type_columns)\nprint(sex_type_df.head())\n\n# \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435 \u0432 \u043e\u0434\u043d\u0443 \u0442\u0430\u0431\u043b\u0438\u0446\u0443 'data_dc' \u0438 'sex_type'\ndata_dc = pd.concat([data_dc,  sex_type_df], axis=1)\n\n# \u0423\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u0430 'sex_type'\ndata_dc.drop(['sex_type'], axis=1, inplace=True)\n\ndata_dc.head(10)"
 },
 {
  "nbid": "NB_2737441",
  "file": "NB_2737441,data_dc,animal_type_df-1.ipynb",
  "row_id": 74,
  "target_code": "ohe_enc = OneHotEncoder()\nspay_neuter_ohe = ohe_enc.fit_transform(data_dc[['spay/neuter'             ]]).toarray()\nspay_neuter_columns = ['neuter_no'           , 'neuter_yes'             ]\nspay_neuter_df = pd.DataFrame(spay_neuter_ohe, columns=spay_neuter_columns)\nprint(spay_neuter_df.head(5))\ndata_dc = pd.concat([data_dc,  spay_neuter_df], axis=1)\ndata_dc.drop(['spay/neuter'             ], axis=1, inplace=True)\ndata_dc.head()",
  "generation": "ohe_enc = OneHotEncoder()\nneuter_ohe = ohe_enc.fit_transform(data_dc[['spay/neuter']]).toarray()\nneuter_columns = ohe_enc.categories_[0]\nneuter_df = pd.DataFrame(neuter_ohe, columns=neuter_columns)\ndata_dc = pd.concat([data_dc, neuter_df], axis=1)\ndata_dc.drop(['spay/neuter'], axis=1, inplace=True)\ndata_dc.head(10)"
 },
 {
  "nbid": "NB_2738031",
  "file": "NB_2738031,insurance,y-1.ipynb",
  "row_id": 1,
  "target_code": "from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import normalize\nonehotencoder = OneHotEncoder(handle_unknown='ignore'        )\ncategorical = insurance[['sex'     ,'smoker'        ,'region'        ]].copy()\ninsurance.drop(['sex'     , 'smoker'         , 'region'         ], axis=1, inplace=True)\ndummies = pd.get_dummies(categorical[:])\ninsurance = pd.concat([dummies, insurance], sort = 'true'       , axis = 1)\ninsurance.head()",
  "generation": "cat_data = insurance.select_dtypes(include=['object'])\ncat_data.head()\ninsurance.drop(['sex', 'smoker', 'region'], axis=1, inplace=True)\ncat_data = pd.get_dummies(cat_data, drop_first=True)\ninsurance = pd.concat([cat_data, insurance], axis=1)\ninsurance.head()"
 },
 {
  "nbid": "NB_2742198",
  "file": "NB_2742198,df_data-1.ipynb",
  "row_id": 2,
  "target_code": "cat_means = df_data.groupby(\"Category\"          ).mean().reset_index().rename(index=str, columns={\"% of Spending\"               : \"Cat Mean % of Spending\"                         })\ncat_means['Category Rank'               ]=cat_means['Cat Mean % of Spending'                        ].rank(ascending=0,method='dense'       ).astype(int)\ncat_means['row'     ]=((cat_means['Category Rank'               ].astype(int)-1)/3 + 1).astype(int)\ncat_means['col'     ]=(cat_means['Category Rank'               ]%3).replace(0, 3).astype(int)\ndf_data = df_data.merge(cat_means, on='Category'          )\ndf_data",
  "generation": "df_data[\"Cat Mean % of Spending\"] = df_data.groupby(\"Category\")[\"% of Spending\"].transform(\"mean\")\ndf_data[\"Category Rank\"        ] = df_data.groupby(\"Category\")[\"% of Spending\"].rank(ascending=False, method=\"dense\")\ndf_data[\"row\"                  ] = df_data[\"Category Rank\"].apply(lambda x: (x-1)//3 + 1)\ndf_data[\"col\"                  ] = df_data[\"Category Rank\"].apply(lambda x: (x-1)%3 + 1)\ndf_data.head()"
 },
 {
  "nbid": "NB_2750156",
  "file": "NB_2750156,df_cleaned_dummies-1.ipynb",
  "row_id": 50,
  "target_code": "waterfront_columns = pd.DataFrame({'waterfront'            : ['no'    ,'yes'     ,'unknown'         ]})\ndf_cleaned_dummies = pd.concat([df_cleaned_dummies, pd.get_dummies(df_cleaned['waterfront'            ], columns=waterfront_columns)], axis=1)\ndf_cleaned_dummies.head()",
  "generation": "df_cleaned_dummies = pd.concat([df_cleaned, pd.get_dummies(df_cleaned['zipcode'], prefix='zip')], axis=1)\ndf_cleaned_dummies = pd.concat([df_cleaned_dummies, pd.get_dummies(df_cleaned_dummies['waterfront'], prefix='waterfront')], axis=1)\ndf_cleaned_dummies.drop(columns=['zipcode', 'waterfront'], inplace=True)\ndf_cleaned_dummies.head(2)"
 },
 {
  "nbid": "NB_2759657",
  "file": "NB_2759657,train-1.ipynb",
  "row_id": 6,
  "target_code": "train[\"datetime-year\"               ] = train[\"datetime\"          ].dt.year\ntrain[\"datetime-month\"                ] = train[\"datetime\"          ].dt.month\ntrain[\"datetime-day\"              ] = train[\"datetime\"          ].dt.day\ntrain[\"datetime-hour\"               ] = train[\"datetime\"          ].dt.hour\ntrain[\"datetime-minute\"                 ] = train[\"datetime\"          ].dt.minute\ntrain[\"datetime-second\"                 ] = train[\"datetime\"          ].dt.second\ntrain[\"datetime-dayofweek\"                    ] = train[\"datetime\"          ].dt.dayofweek\nprint(train.shape)\ntrain.head()",
  "generation": "train['datetime-year'      ] = train.loc[:,'datetime'].dt.year\ntrain['datetime-month'     ] = train.loc[:,'datetime'].dt.month\ntrain['datetime-day'       ] = train.loc[:,'datetime'].dt.day\ntrain['datetime-hour'      ] = train.loc[:,'datetime'].dt.hour\ntrain['datetime-minute'    ] = train.loc[:,'datetime'].dt.minute\ntrain['datetime-second'    ] = train.loc[:,'datetime'].dt.second\ntrain['datetime-dayofweek' ] = train.loc[:,'datetime'].dt.dayofweek\ntrain.head()"
 },
 {
  "nbid": "NB_2771105",
  "file": "NB_2771105,avg_tem_fert,y-1.ipynb",
  "row_id": 35,
  "target_code": "avg_tem_fert = avg_tem_fert.rename(columns={\"AverageTemperature\"                    : \"temp\"       , \"Fertility rate (children per woman)\"                                      : \"fert\"       })\navg_tem_fert.head()",
  "generation": "gr_df_climat = gr_df_climat.rename(columns={'Entity': 'country'})\ndf = df.rename(columns={'Entity': 'country'})\navg_tem_country = gr_df_climat.drop(['AverageTemperatureUncertainty'], axis=1)\navg_fet_country = df.groupby('country')['Fertility rate (children per woman)'].mean()\navg_tem_fert = pd.concat([avg_tem_country, avg_fet_country], axis=1, join='inner')\navg_tem_fert = avg_tem_fert.rename(columns={'AverageTemperature': 'temp', 'Fertility rate (children per woman)': 'fert'})\navg_tem_fert.head()"
 },
 {
  "nbid": "NB_2771898",
  "file": "NB_2771898,data,y_train-1.ipynb",
  "row_id": 8,
  "target_code": "data = data.drop(['PassengerId'             , 'Name'       , 'Embarked'           , 'Ticket'         ], axis=1)\ndata.head(10)",
  "generation": "data = data.drop(['Name', 'Ticket', 'Cabin', 'PassengerId', 'Embarked'], axis=1)\ndata['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\ndata = data.fillna(-999)\ntrain = data[:train_samples]\ntest = data[train_samples:]"
 },
 {
  "nbid": "NB_2779258",
  "file": "NB_2779258,covid_data-1.ipynb",
  "row_id": 4,
  "target_code": "covid_data = covid_data.drop(covid_data[covid_data['Province/State'                ]=='Grand Princess'                ].index)\ncovid_data = covid_data.drop(covid_data[covid_data['Province/State'                ]=='Diamond Princess'                  ].index)\ncovid_data = covid_data.drop(covid_data[covid_data['Country/Region'                ]=='Diamond Princess'                  ].index)\ncovid_data = covid_data.drop(covid_data[covid_data['Country/Region'                ]=='MS Zaandam'            ].index)\ncovid_data = covid_data.reset_index()\ndel covid_data['index'       ]\ncovid_data",
  "generation": "covid_data = covid_data[covid_data['Province/State'].notna()]"
 },
 {
  "nbid": "NB_2780687",
  "file": "NB_2780687,df-1.ipynb",
  "row_id": 14,
  "target_code": "df = df.append(pd.DataFrame({'numbers'         : 100, 'floats'         : 5.75, 'names'        : 'Henry'        }, index=['z'   ,]))\ndf",
  "generation": "df['floats'] = [1.5, 2.5, 3.5, 4.5]\ndf['names'] = ['Guido', 'Felix', 'Francesc', 'Yves']\ndf.head()"
 },
 {
  "nbid": "NB_2780687",
  "file": "NB_2780687,df-1.ipynb",
  "row_id": 16,
  "target_code": "df = df.join(pd.DataFrame([1, 4, 9, 16, 25], index=['a'   , 'b'    , 'c'    , 'd'    , 'y'    ], columns=['squares'         ,]), how='outer'       )\ndf",
  "generation": "df['squares'] = df['numbers'] ** 2\ndf.loc['y'] = [None, None, None, 25]\ndf"
 },
 {
  "nbid": "NB_2783511",
  "file": "NB_2783511,uber-1.ipynb",
  "row_id": 3,
  "target_code": "uber['Request timestamp'                   ] = pd.to_datetime(uber['Request timestamp'                   ], dayfirst=True)\nuber['Drop timestamp'                ]= pd.to_datetime(uber['Drop timestamp'                ], dayfirst=True)\nuber.head()",
  "generation": "uber.loc[:, 'Request timestamp'] = pd.to_datetime(uber['Request timestamp'], dayfirst=True)\nuber.loc[:, 'Drop timestamp'   ] = pd.to_datetime(uber['Drop timestamp'   ], dayfirst=True)\nuber.head()"
 },
 {
  "nbid": "NB_2783511",
  "file": "NB_2783511,uber-1.ipynb",
  "row_id": 7,
  "target_code": "uber['Driver id'           ]=uber['Driver id'           ].fillna(0).astype(int)\nuber.head()",
  "generation": "uber['Driver id'] = uber['Driver id'].fillna(0).astype(int)\nuber.head()"
 },
 {
  "nbid": "NB_2783511",
  "file": "NB_2783511,uber-1.ipynb",
  "row_id": 10,
  "target_code": "uber=uber.drop('Drop timestamp'                , axis=1)\nuber.head()",
  "generation": "uber = uber.drop([\"Drop timestamp\"], axis=1)\nuber"
 },
 {
  "nbid": "NB_2784348",
  "file": "NB_2784348,titanicData-1.ipynb",
  "row_id": 35,
  "target_code": "titanicData = titanicData.replace(\"male\"      , 0)\ntitanicData = titanicData.replace(\"female\"        , 1)\ntitanicData.head()",
  "generation": "titanicData['Sex'] = np.where(titanicData['Sex'] == 'male', 0, 1)\ntitanicData.head()"
 },
 {
  "nbid": "NB_2786544",
  "file": "NB_2786544,df2_clean-1.ipynb",
  "row_id": 111,
  "target_code": "df2_clean['intercept'           ] = 1\ndf2_clean[['p1_notdog'           , 'p1_isdog'           ]] = pd.get_dummies(df2_clean.p1_dog)\ndf2_clean[['p2_notdog'           , 'p2_isdog'           ]] = pd.get_dummies(df2_clean.p2_dog)\ndf2_clean[['p3_notdog'           , 'p3_isdog'           ]] = pd.get_dummies(df2_clean.p3_dog)\ndf2_clean.head()",
  "generation": "df2_clean['intercept'] = 1\ndf2_clean['p1_notdog'] = ~(df2_clean['p1_dog'])\ndf2_clean['p1_isdog'] = df2_clean['p1_dog']\ndf2_clean['p2_notdog'] = ~(df2_clean['p2_dog'])\ndf2_clean['p2_isdog'] = df2_clean['p2_dog']\ndf2_clean['p3_notdog'] = ~(df2_clean['p3_dog'])\ndf2_clean['p3_isdog'] = df2_clean['p3_dog']\ndf2_clean.head()"
 },
 {
  "nbid": "NB_2789609",
  "file": "NB_2789609,df-1.ipynb",
  "row_id": 4,
  "target_code": "diabetes_map = {True: 1, False: 0}\ndf['diabetes'          ] = df['diabetes'          ].map(diabetes_map)\ndf.head()",
  "generation": "df['diabetes'] = df['diabetes'].astype(int)\ndf.head()"
 },
 {
  "nbid": "NB_2790139",
  "file": "NB_2790139,cars-1.ipynb",
  "row_id": 40,
  "target_code": "cars['group'       ] = pd.Series(cars.gear, dtype='category'          )\ncars.head()",
  "generation": "cars['group'] = pd.cut(cars['mpg'], bins=[0, 10, 20, 30], labels=['low', 'medium', 'high'])"
 },
 {
  "nbid": "NB_30814",
  "file": "NB_30814,df_lilX,y_test-1.ipynb",
  "row_id": 26,
  "target_code": "df_lilX['content'         ]=df_lilX[['title'       , 'selftext'           ]].apply(lambda row: ' '    .join(row.values.astype(str)), axis=1)\ndf_lilX['content'         ]=df_lilX['content'         ].str.replace('nan'     , ''   )\ndf_lilX.head()",
  "generation": "df_lilX['content'] = df_lilX['title'].fillna('') + ' ' + df_lilX['selftext'].fillna('')\ndf_lilX.head()"
 },
 {
  "nbid": "NB_30814",
  "file": "NB_30814,df_vegX-1.ipynb",
  "row_id": 13,
  "target_code": "df_vegX['content'         ]=df_vegX[['title'       , 'selftext'           ]].apply(lambda row: ' '    .join(row.values.astype(str)), axis=1)\ndf_vegX['content'         ]=df_vegX['content'         ].str.replace('nan'     , ''   )\ndf_vegX.head()",
  "generation": "df_vegX['content'] = df_vegX['title'].fillna('') + ' ' + df_vegX['selftext'].fillna('')\ndf_vegX = df_vegX[['Unnamed: 0', 'title', 'selftext', 'num_comments', 'content']]\ndf_vegX.head()"
 },
 {
  "nbid": "NB_344488",
  "file": "NB_344488,cars-1.ipynb",
  "row_id": 13,
  "target_code": "cars.CarName = cars.CarName.apply(lambda x:x.split()[0])\ncars.head()",
  "generation": "cars['CarName'] = cars['CarName'].apply(lambda x: x.split(' ')[0].lower().capitalize())\ncars['RiskRating'] = pd.cut(cars['price'], bins=[0, 10000, 20000, 100000], labels=['Risky', 'Moderate', 'Safe'])\ncars.head()"
 },
 {
  "nbid": "NB_347095",
  "file": "NB_347095,data,true_tweets-1.ipynb",
  "row_id": 7,
  "target_code": "from nltk.tokenize import word_tokenize                           \ndata['clean_text'            ] = data['clean_text'            ].apply(word_tokenize)\ndata.head()",
  "generation": "from nltk.tokenize import word_tokenize\n\ndata['clean_text'] = data['clean_text'].apply(word_tokenize)\ndata.head()"
 },
 {
  "nbid": "NB_347095",
  "file": "NB_347095,data,true_tweets-1.ipynb",
  "row_id": 9,
  "target_code": "data['clean_text'            ] = data['clean_text'            ].apply(filter_stopwords)                         \ndata.head()",
  "generation": "from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\ndef remove_stopwords(tweet):\n    return [word for word in tweet if word not in stop_words]\n\ndata['clean_text'] = data['clean_text'].apply(remove_stopwords)\ndata['clean_text'] = data['clean_text'].apply(lambda x: ' '.join(x))\ndata['clean_text'] = data['clean_text'].str.replace('[^\\w\\s]','')\ndata['clean_text'] = data['clean_text'].str.replace('\\d+', '')\ndata['clean_text'] = data['clean_text'].str.strip()\n\ndata['clean_text'] = data['clean_text'].str.replace('http\\S*', '')\ndata['clean_text'] = data['clean_text'].str.replace('[^a-z\\s]', '')\ndata['clean_text'] = data['clean_text'].str.lower()\n\ndata['clean_text'] = data['clean_text'].apply(word_tokenize)\ndata['clean_text'] = data['clean_text'].apply(remove_stopwords)\ndata['clean_text'] = data['clean_text'].apply(lambda x: ' '.join(x))\n\ndata['clean_text'] = data['clean_text'].str.replace('[^\\w\\s]','')\ndata['clean_text'] = data['clean_text'].str.replace('\\d+', '')\ndata['clean_text'] = data['clean_text'].str.strip()\n\ndata['clean_text'] = data['clean_text'].apply(lambda x: x.split())\ndata['clean_text'] = data['clean_text'].apply(lambda x: [item for item in x if item.isalpha()])\ndata['clean_text'] = data['clean_text'].apply(lambda x: ' '.join(x))\n\ndata.head()"
 },
 {
  "nbid": "NB_347095",
  "file": "NB_347095,data,true_tweets-1.ipynb",
  "row_id": 11,
  "target_code": "data['clean_text'            ] = data['clean_text'            ].apply(lemmatize_tweet)                        \ndata.head()",
  "generation": "from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\ndef filter_stopwords(tokens):\n    filtered_tokens = [token for token in tokens if token not in stop_words]\n    return filtered_tokens\ndata['clean_text'] = data['clean_text'].apply(filter_stopwords)\ndata.head()"
 },
 {
  "nbid": "NB_347095",
  "file": "NB_347095,data,true_tweets-1.ipynb",
  "row_id": 12,
  "target_code": "data['clean_text'            ] = data['clean_text'            ].apply(lambda text:' '   .join(text))                      \ndata.head()",
  "generation": "from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ndef filter_stopwords(tokens):\n    return [token for token in tokens if token not in stop_words]\n\ndef lemmatize_tweet(tokens):\n    return [lemmatizer.lemmatize(token) for token in tokens]\n\ndata['clean_text'] = data['clean_text'].apply(filter_stopwords)\ndata['clean_text'] = data['clean_text'].apply(lemmatize_tweet)\ndata['clean_text'] = data['clean_text'].apply(lambda x: ' '.join(x)) \ndata.head()"
 },
 {
  "nbid": "NB_351384",
  "file": "NB_351384,train_data,female_lowclass,male_lowclass,male_highclass-1.ipynb",
  "row_id": 4,
  "target_code": "train_data = train_data.drop('Cabin'       ,axis=1)\ntrain_data = train_data.drop('Name'      ,axis=1)\ntrain_data = train_data.drop('PassengerId'             ,axis=1)\ntrain_data = train_data.drop('Ticket'        , axis=1)\ntrain_data",
  "generation": "train_data = train_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\ntrain_data"
 },
 {
  "nbid": "NB_471783",
  "file": "NB_471783,genre_df-1.ipynb",
  "row_id": 73,
  "target_code": "genre_df = genre_df.sort_values(by=[\"count\"       ],ascending=False).reset_index(drop=True)\ngenre_df",
  "generation": "genre_df.sort_values(by='count', ascending=False, inplace=True)\ngenre_df.reset_index(drop=True, inplace=True)\ngenre_df.head()"
 },
 {
  "nbid": "NB_6034274",
  "file": "NB_6034274,data,Y-1.ipynb",
  "row_id": 2,
  "target_code": "data=data.iloc[:,1:]\ndata.head(2)",
  "generation": "data.drop(['Unnamed: 0', 'index'], axis=1, inplace=True)\ndata.head()"
 },
 {
  "nbid": "NB_6051950",
  "file": "NB_6051950,messages-1.ipynb",
  "row_id": 10,
  "target_code": "print('Length of initial few Messages : '                                   )\nmessages['Length'        ] = messages['message'         ].map(lambda text: len(text))\nmessages.head()",
  "generation": "messages['Length'] = messages['message'].apply(len)\nmessages.head()"
 },
 {
  "nbid": "NB_6067922",
  "file": "NB_6067922,rating_df-1.ipynb",
  "row_id": 3,
  "target_code": "rating_df['event_count'             ] = rating_df['visitorid'           ].map(rating_df.visitorid.value_counts())\nrating_df.head()",
  "generation": "rating_df['event_count'] = rating_df.groupby('visitorid')['visitorid'].transform('count')\nrating_df.head()"
 },
 {
  "nbid": "NB_6067922",
  "file": "NB_6067922,rating_df-1.ipynb",
  "row_id": 4,
  "target_code": "rating_df = rating_df.sort_values('timestamp'           )\nrating_df.head(10)",
  "generation": "rating_df = rating_df.sort_values(by=['timestamp'])\nrating_df.head()"
 },
 {
  "nbid": "NB_6125368",
  "file": "NB_6125368,existing_df-1.ipynb",
  "row_id": 6,
  "target_code": "existing_df = existing_df.fillna(15)\nexisting_df.head()",
  "generation": "There is no target code provided. Please provide the target code so I can help you transform the input dataframe to the output dataframe."
 },
 {
  "nbid": "NB_27062",
  "file": "NB_27062,df_main-1.ipynb",
  "row_id": 9,
  "target_code": "df_main['Timestamp'           ] = pd.to_datetime(df_main['TIMESTAMP'           ])\ndf_main = df_main.set_index('Timestamp'           )\ndf_main.drop([\"TIMESTAMP\"           ], axis=1, inplace=True)\ndf_main.head()",
  "generation": "df_main['TIMESTAMP'] = pd.to_datetime(df_main['TIMESTAMP'])\ndf_main.set_index('TIMESTAMP', inplace=True)\ndf_main.drop(['Entry No'], axis=1, inplace=True)\ndf_main.head()"
 },
 {
  "nbid": "NB_2799145",
  "file": "NB_2799145,df-1.ipynb",
  "row_id": 36,
  "target_code": "df['age_bins'          ] = pd.cut(x = df['age'     ], bins = [-1, 25, 50, 75, 116])\ndf.head()",
  "generation": "df['age_bins'] = pd.cut(x = df['age'], bins = [0, 25, 50, 75, 100, 150]) \ndf.head()"
 },
 {
  "nbid": "NB_2799219",
  "file": "NB_2799219,df-1.ipynb",
  "row_id": 2,
  "target_code": "df[\"Date\"      ] = pd.to_datetime(df['Date'      ])\ndf.head(5)",
  "generation": "df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')\ndf.head()"
 },
 {
  "nbid": "NB_2816842",
  "file": "NB_2816842,data,Y-1.ipynb",
  "row_id": 32,
  "target_code": "data[\"encoded_default\"                 ] = data[\"default\"         ].map({\"no\"    :0, \"yes\"      :1})\ndata.head()",
  "generation": "data['encoded_default'] = data['default'].apply(lambda x: 1 if x == 'yes' else 0)\ndata.head()"
 },
 {
  "nbid": "NB_2817930",
  "file": "NB_2817930,crosstab_cv,top10_final-1.ipynb",
  "row_id": 15,
  "target_code": "crosstab_cv['Total'       ] = crosstab_cv.sum(axis=1)\ncrosstab_cv.head()",
  "generation": "crosstab_cv['Total'] = crosstab_cv.sum(axis=1)\ncrosstab_cv = crosstab_cv[['Total'] + list(range(15, 34))]\ncrosstab_cv"
 },
 {
  "nbid": "NB_2840794",
  "file": "NB_2840794,videogame_data-1.ipynb",
  "row_id": 5,
  "target_code": "videogame_data = videogame_data.drop(['NA_Sales'          ,'EU_Sales'          ,'JP_Sales'          ,'Other_Sales'             ], axis=1)\nvideogame_data.head()",
  "generation": "videogame_data.drop(['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales'], axis=1, inplace=True)\nvideogame_data.head()"
 },
 {
  "nbid": "NB_2840794",
  "file": "NB_2840794,videogame_data-1.ipynb",
  "row_id": 7,
  "target_code": "videogame_data.columns = ['Videogame'           ,'Platform'          ,'Release Year'              ,'Genre'       ,'Publisher'           ,'Global Sales'              ,'Critic Score'              ,'Critic Count'              ,'User Score'            ,'User Count'            ,'Developer'           ,'Rating'        ]\nvideogame_data = videogame_data[['Videogame'           ,'Global Sales'              ,'Developer'           ,'Publisher'           ,'Platform'          ,'Genre'       ,'Rating'        ,'Release Year'              ,'Critic Score'              ,'Critic Count'              ,'User Score'            ,'User Count'            ]]\nvideogame_data.head()",
  "generation": "videogame_data = videogame_data.rename(columns={'Name':'Videogame', 'Global_Sales':'Global Sales', 'Developer':'Developer', 'Publisher':'Publisher', 'Platform':'Platform', 'Genre':'Genre', 'Rating':'Rating', 'Year_of_Release':'Release Year', 'Critic_Score':'Critic Score', 'Critic_Count':'Critic Count', 'User_Score':'User Score', 'User_Count':'User Count'})\nvideogame_data = videogame_data[['Videogame', 'Global Sales', 'Developer', 'Publisher', 'Platform', 'Genre', 'Rating', 'Release Year', 'Critic Score', 'Critic Count', 'User Score', 'User Count']]\nvideogame_data.head()"
 },
 {
  "nbid": "NB_2840794",
  "file": "NB_2840794,videogame_data-1.ipynb",
  "row_id": 9,
  "target_code": "videogame_data = videogame_data.dropna()\nvideogame_data.head()",
  "generation": "videogame_data = videogame_data.dropna()\nvideogame_data = videogame_data.reset_index(drop=True)"
 },
 {
  "nbid": "NB_2844332",
  "file": "NB_2844332,df,test-1.ipynb",
  "row_id": 2,
  "target_code": "df ['date'      ] =   pd.to_datetime( df [ ['year'      ,'month'       ,'day'     ] ] )\ndf.head()",
  "generation": "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\ndf.drop(['Unnamed: 0'], axis=1, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_2844332",
  "file": "NB_2844332,df,test-1.ipynb",
  "row_id": 3,
  "target_code": "df['ppm_fixed'           ] = df['ppm'     ].map(lambda x: np.nan if x < 0 else x )\ndf['ppm_fixed'           ].fillna(method='backfill'          ,inplace= True)\ndf.head()",
  "generation": "df['ppm_fixed'] = df['ppm'].fillna(0)\ndf.head()"
 },
 {
  "nbid": "NB_2850508",
  "file": "NB_2850508,df,X-1.ipynb",
  "row_id": 3,
  "target_code": "df['approved'          ] = df['Interest.Rate'               ] <= 12\ndf.head()",
  "generation": "df['approved'] = df['Interest.Rate'] <= 12\ndf.head()"
 },
 {
  "nbid": "NB_2850829",
  "file": "NB_2850829,data,pd_crosstab-1.ipynb",
  "row_id": 12,
  "target_code": "combs = []\nfor col in range(0,len(data.columns)-1):\n    for oneMore in range(col+1, len(data.columns)-1):\n        x = data.columns[col] + \"+\"     + data.columns[oneMore]\n        combs.append(x)\nfor i in combs:\n    sp = i.split(\"+\"   )\n    data[i] = data[sp[0]] + data[sp[1]]\ndata",
  "generation": "data['Feature1+Feature2'   ]= data['Feature1'] + data['Feature2']\ndata['Feature1+Feature3'   ]= data['Feature1'] + data['Feature3']\ndata['Feature1+Feature4'   ]= data['Feature1'] + data['Feature4']\ndata['Feature1+DayOfWeek'  ]= data['Feature1'] + data['DayOfWeek']\ndata['Feature2+Feature3'   ]= data['Feature2'] + data['Feature3']\ndata['Feature2+Feature4'   ]= data['Feature2'] + data['Feature4']\ndata['Feature2+DayOfWeek'  ]= data['Feature2'] + data['DayOfWeek']\ndata['Feature3+Feature4'   ]= data['Feature3'] + data['Feature4']\ndata['Feature3+DayOfWeek'  ]= data['Feature3'] + data['DayOfWeek']\ndata['Feature4+DayOfWeek'  ]= data['Feature4'] + data['DayOfWeek']\ndata.head()"
 },
 {
  "nbid": "NB_2854229",
  "file": "NB_2854229,weo-1.ipynb",
  "row_id": 41,
  "target_code": "weo = weo.drop(\"Ratio\"       , axis = 1)\nweo.head()",
  "generation": "weo = weo.drop(columns=['Ratio'])\nweo.head()"
 },
 {
  "nbid": "NB_2856984",
  "file": "NB_2856984,df-1.ipynb",
  "row_id": 11,
  "target_code": "df['scheduled_day'               ]=pd.to_datetime(df['scheduled_day'               ])         \ndf['appointment_day'                 ]=pd.to_datetime(df['appointment_day'                 ])\ndf['patient_id'            ]=df['patient_id'            ].astype(int)                   \ndf.head(2)",
  "generation": "df['ScheduledDay'] = pd.to_datetime(df['ScheduledDay']).dt.date\ndf['AppointmentDay'] = pd.to_datetime(df['AppointmentDay']).dt.date\ndf['patient_id'] = df['PatientId'].astype(int)\ndf['scheduled_day'] = pd.to_datetime(df['ScheduledDay'])\ndf['appointment_day'] = pd.to_datetime(df['AppointmentDay'])\ndf = df.drop(['PatientId', 'ScheduledDay', 'AppointmentDay'], axis=1)\ndf = df.rename(columns={'Hipertension': 'hypertension', 'Handcap': 'handicap', 'No-show': 'no_show'})\ndf.head(1)"
 },
 {
  "nbid": "NB_2858325",
  "file": "NB_2858325,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.fillna(df.mode().iloc[0])\ndf",
  "generation": "df['Recommendation_encoded'] = le.fit_transform(df['Recommendation'])"
 },
 {
  "nbid": "NB_2861068",
  "file": "NB_2861068,housing_data-1.ipynb",
  "row_id": 5,
  "target_code": "housing_data['age_of_house'              ] = current_year - pd.to_datetime(housing_data['date'      ]).dt.year\nhousing_data.head()",
  "generation": "housing_data['age_of_house'] = current_year - housing_data['yr_built']\nhousing_data = housing_data.drop('yr_renovated', axis=1)\nhousing_data = housing_data.drop('date', axis=1)\nhousing_data = housing_data[feature_cols + ['price']]\nhousing_data.head()"
 },
 {
  "nbid": "NB_2868404",
  "file": "NB_2868404,df-1.ipynb",
  "row_id": 7,
  "target_code": "temp = pd.get_dummies(df.Sentiment, prefix= 'Sentiment'            , drop_first=True)\ndf= pd.concat([df , temp], axis=1)\ndf",
  "generation": "df['Sentiment_Neutral'] = df['Sentiment'].apply(lambda x: 1 if x == 'Neutral' else 0)\ndf['Sentiment_Positive'] = df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else 0)\ndf = df.drop(['Sentiment'], axis=1)\ndf"
 },
 {
  "nbid": "NB_2869116",
  "file": "NB_2869116,dr_le_pop-1.ipynb",
  "row_id": 7,
  "target_code": "dr_le_pop = dr_le_pop.drop(['Sex'     ,'Race'      ,'Age-adjusted Death Rate'                         ], axis=1)\ndr_le_pop.head()",
  "generation": "dr_le_pop = dr_le_pop[['Year', 'Average Life Expectancy (Years)']]\ndr_le_pop.reset_index(drop=True, inplace=True)\ndr_le_pop.head()"
 },
 {
  "nbid": "NB_2870589",
  "file": "NB_2870589,movie_ratings_scraped-1.ipynb",
  "row_id": 55,
  "target_code": "movie_ratings_scraped = movie_ratings_scraped[['movie'       , 'year'       , 'imdb'       , 'metascore'            , 'votes'        ]]\nmovie_ratings_scraped.head()",
  "generation": "movie_ratings_scraped = movie_ratings_scraped[['movie', 'year', 'imdb', 'metascore', 'votes']]\nmovie_ratings_scraped.head()"
 },
 {
  "nbid": "NB_2872382",
  "file": "NB_2872382,avocado-1.ipynb",
  "row_id": 3,
  "target_code": "avocado.columns = avocado.columns.str.replace(' '   , '_'    )\navocado.head()",
  "generation": "avocado.columns=avocado.columns.str.replace(' ','_')\navocado.head()"
 },
 {
  "nbid": "NB_2872382",
  "file": "NB_2872382,avocado-1.ipynb",
  "row_id": 4,
  "target_code": "avocado['Date'      ] = pd.to_datetime(avocado['Date'      ])\navocado.head()",
  "generation": "avocado.columns = avocado.columns.str.replace(' ', '_')\navocado['Date'] = pd.to_datetime(avocado['Date'])\navocado.head()"
 },
 {
  "nbid": "NB_2876987",
  "file": "NB_2876987,df-1.ipynb",
  "row_id": 12,
  "target_code": "df['sex'     ] = df['sex'     ].map(lambda x: 1 if x == 'female'          else 0 )\ndf.head(5)",
  "generation": "df['sex'] = df['sex'].apply(lambda x: 1 if x == 'female' else 0)\ndf.head(20)"
 },
 {
  "nbid": "NB_2880416",
  "file": "NB_2880416,g_data-1.ipynb",
  "row_id": 19,
  "target_code": "g_data['Rolling'         ] = g_data.rolling(10).mean()['avg_temp'          ]\ng_data.head(25)",
  "generation": "global_data['Rolling'] = global_data['avg_temp'].rolling(7).mean()\ng_data = global_data[(global_data['year'] >= 1796) & (global_data['year'] <= 2013)]\ng_data.reset_index(inplace=True)\ng_data.drop(columns='index',inplace=True)\ng_data"
 },
 {
  "nbid": "NB_2881516",
  "file": "NB_2881516,df,X_numeric,y_df-1.ipynb",
  "row_id": 3,
  "target_code": "print('original dataframe has '                          + str(len(df)) + ' instances'             )\ndf = df.replace('?'   , np.NaN).dropna()\nprint('after removal, there are '                            + str(len(df)) + ' instances'             )\ndf.head(10)",
  "generation": "df = df.replace('?', np.nan)\ndf = df.dropna()\ndf.head()"
 },
 {
  "nbid": "NB_2881516",
  "file": "NB_2881516,df,X_numeric,y_df-1.ipynb",
  "row_id": 6,
  "target_code": "df = df.drop(['education'           , 'relationship'               , 'occupation'             , 'native.country'                 , 'race'       ], axis = 1)\ndf.head(5)",
  "generation": "df = df.drop(['education', 'relationship'], axis = 1)\ndf.head()"
 },
 {
  "nbid": "NB_2881516",
  "file": "NB_2881516,df,X_numeric,y_df-1.ipynb",
  "row_id": 7,
  "target_code": "le = LabelEncoder()\nle.fit(df['sex'     ].unique())\ndf['sex'     ] = le.transform(df['sex'     ])\ndf.head()",
  "generation": "le = LabelEncoder()\ndf['sex'] = le.fit_transform(df['sex'])\ndf.head()"
 },
 {
  "nbid": "NB_2881516",
  "file": "NB_2881516,df,X_numeric,y_df-1.ipynb",
  "row_id": 8,
  "target_code": "df['workclass'           ] = df['workclass'           ].replace(['State-gov'           , 'Federal-gov'              , 'Local-gov'            ], 'gov'      )\ndf['workclass'           ] = df['workclass'           ].replace(['Self-emp-not-inc'                  , 'Self-emp-inc'               ], 'employment'             )\ndf['workclass'           ] = df['workclass'           ].map({'gov'     : 0, 'employment'             : 1, 'Private'          : 2, 'Without-pay'              : 3}).astype(np.int64)\ndf['marital.status'                ] = df['marital.status'                ].replace(['Widowed'         , 'Divorced'           , 'Separated'            , 'Never-married'                ], 'single'         )\ndf['marital.status'                ] = df['marital.status'                ].replace(['Married-civ-spouse'                    , 'Married-spouse-absent'                        , 'Married-AF-spouse'                    ], \n                                                    'married'                                                             )\ndf['marital.status'                ] = df['marital.status'                ].map({'single'        : 0, 'married'          : 1}).astype(np.int64)\ndf.head()",
  "generation": "le.fit(df['workclass'].unique())\ndf['workclass'] = le.transform(df['workclass'])\nle.fit(df['marital.status'].unique())\ndf['marital.status'] = le.transform(df['marital.status'])\ndf.head()"
 },
 {
  "nbid": "NB_2887875",
  "file": "NB_2887875,df-1.ipynb",
  "row_id": 13,
  "target_code": "df['salary'        ] = df['salary'        ].fillna(0)\ndf.head()",
  "generation": "df['salary'].fillna(0, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_2887875",
  "file": "NB_2887875,df-1.ipynb",
  "row_id": 16,
  "target_code": "df = df.drop(['sl_no'       ],axis=1)\ndf.head()",
  "generation": "df.drop(labels=['sl_no'], axis=1,inplace = True)\ndf.head()"
 },
 {
  "nbid": "NB_2894935",
  "file": "NB_2894935,df-1.ipynb",
  "row_id": 9,
  "target_code": "df['PatientId'           ] = df['PatientId'           ].astype(str).str[:-2]\ndf.rename(columns = {\"Handcap\"          : \"Handicap\"           }, inplace = True)\ndf.rename(columns = {\"PatientId\"            : \"PatientID\"            }, inplace = True)\ndf.rename(columns = {\"Neighbourhood\"                : \"Neighborhood\"               }, inplace = True)\ndf.rename(columns = {\"Hipertension\"               : \"Hypertension\"               }, inplace = True)\ndf.head()",
  "generation": "df.rename(columns={'PatientId':'PatientID', 'Neighbourhood':'Neighborhood', 'Hipertension':'Hypertension', 'Handcap':'Handicap'}, inplace=True)\ndf['PatientID'] = df['PatientID'].astype(int)\ndf['AppointmentDay'] = pd.to_datetime(df['AppointmentDay']).dt.date\ndf['ScheduledDay'] = pd.to_datetime(df['ScheduledDay']).dt.date\ndf['No-show'] = df['No-show'].map({'No': 0, 'Yes': 1})\ndf.drop(['Unnamed: 0'], axis=1, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_368636",
  "file": "NB_368636,df-1.ipynb",
  "row_id": 2,
  "target_code": "df=df[[\"condition\"           ,\"robot_experience\"                  ,\"tech\"      ,\"time_help_robot\"                 ,\"help_attempts\"               ,\"experiment_runtime\"                    ]].dropna()\ndf",
  "generation": "df = df[['condition', 'robot_experience', 'tech', 'time_help_robot', 'help_attempts', 'experiment_runtime']]\ndf.head()"
 },
 {
  "nbid": "NB_381742",
  "file": "NB_381742,titanic-1.ipynb",
  "row_id": 10,
  "target_code": "titanic[\"Age\"     ] = titanic[\"Age\"     ].fillna(titanic[\"Age\"     ].mean())\ndef sex_to_numeric(x):\n    if x=='male'      :\n        return 0\n    if x=='female'        :\n        return 1\n    else:\n        return x\ntitanic[\"Sex\"     ] = titanic[\"Sex\"     ].apply(sex_to_numeric)\ntitanic.head()",
  "generation": "titanic['Age'].fillna(titanic['Age'].mean(), inplace=True)\ntitanic['Embarked'].fillna('Unknown', inplace=True)\ntitanic['Sex'] = titanic['Sex'].map({'male': 0, 'female': 1})\ntitanic['Embarked'] = titanic['Embarked'].map({'S': 0, 'C': 1, 'Q': 2, 'Unknown': 3})\ntitanic.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntitanic.head()"
 },
 {
  "nbid": "NB_381742",
  "file": "NB_381742,titanic-1.ipynb",
  "row_id": 13,
  "target_code": "features = [\"Survived\"          , \"Pclass\"         , \"Sex\"      , \"Age\"      , \"SibSp\"        , \"Parch\"        , \"Fare\"       , \"Embarked\"           ]\ntitanic = titanic[features]\ntitanic.head(10)",
  "generation": "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].mean())\n\ndef sex_to_numeric(x):\n    if x=='male':\n        return 0\n    if x=='female':\n        return 1\n    else:\n        return x\n\ntitanic[\"Sex\"] = titanic[\"Sex\"].apply(sex_to_numeric)\n\ntitanic = titanic[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n\ntitanic['Embarked'] = titanic['Embarked'].fillna('Unknown')\n\ndef embarked_to_numeric(x):\n    if x=='S':\n        return 0\n    if x=='C':\n        return 1\n    if x=='Q':\n        return 2\n    if x=='Unknown':\n        return 3\n    else:\n        return x\n\ntitanic[\"Embarked\"] = titanic[\"Embarked\"].apply(embarked_to_numeric)\n\ntitanic.head()"
 },
 {
  "nbid": "NB_389193",
  "file": "NB_389193,df_gr-1.ipynb",
  "row_id": 11,
  "target_code": "df_gr = df_gr.unstack()\ndf_gr.head()",
  "generation": "df_gr = df.groupby(['Fecha','Instrumento']).mean().unstack()['Precio']\ndf_gr.head(10)"
 },
 {
  "nbid": "NB_5804428",
  "file": "NB_5804428,data-1.ipynb",
  "row_id": 1,
  "target_code": "data = data.drop(columns=[\"PassengerId\"             , \"Name\"       ,\"Ticket\"        ,\"Cabin\"       ])\ndata.head()",
  "generation": "data = data.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis=1)\ndata['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\ndata = data.dropna()\ndata.head()"
 },
 {
  "nbid": "NB_5856723",
  "file": "NB_5856723,markets-1.ipynb",
  "row_id": 6,
  "target_code": "markets['SocialMedia'             ] = markets.apply(lambda row: social (row),axis=1)\nmarkets.head()",
  "generation": "markets[\"SocialMedia\"] = np.where((markets[\"Facebook\"] != \"nan\") | (markets[\"Twitter\"] != \"nan\") | (markets[\"Youtube\"] != \"nan\"), 1, 0)\nmarkets = markets[[\"Unnamed: 0\", \"FMID\", \"MarketName\", \"Website\", \"Facebook\", \"Twitter\", \"Youtube\", \"OtherMedia\", \"street\", \"city\", \"County\", \"Beans\", \"Fruits\", \"Grains\", \"Juices\", \"Mushrooms\", \"PetFood\", \"Tofu\", \"WildHarvested\", \"updateTime\", \"SocialMedia\"]]\nmarkets"
 },
 {
  "nbid": "NB_5860970",
  "file": "NB_5860970,df-1.ipynb",
  "row_id": 11,
  "target_code": "df = shuffle(df)\ndf.head()",
  "generation": "def clean_text(text):\n    text = [re.sub('[^A-z\\s]','',str(sent)) for sent in text]\n    return text\n\ndf = shuffle(df)\ndf['Phrase'] = df['Phrase'].apply(lambda x: x.lower())\ndf['Phrase'] = df['Phrase'].apply((lambda x: re.sub('[^A-z\\s]','',x)))\ndf['clean_text'] = clean_text(df['Phrase'].values)\ndf.head()"
 },
 {
  "nbid": "NB_5953967",
  "file": "NB_5953967,og_df-1.ipynb",
  "row_id": 7,
  "target_code": "og_df = og_df.drop(columns=['Channel'         , 'Region'         ])\nog_df.head()",
  "generation": "og_df=og_df.drop([\"Channel\"            ,\"Region\"            ],axis=1)\nog_df"
 },
 {
  "nbid": "NB_1828704",
  "file": "NB_1828704,chat_data-1.ipynb",
  "row_id": 14,
  "target_code": "chat_data = chat_data.drop(columns=['match_id'          ], axis=1)\nchat_data.head()",
  "generation": "data.drop('match_id', axis=1, inplace=True)\ndata.head()"
 },
 {
  "nbid": "NB_1832055",
  "file": "NB_1832055,df-1.ipynb",
  "row_id": 18,
  "target_code": "df['scheduled_day'               ] = df.loc[:,'scheduled_day'               ].apply(lambda s: s.split('T'   )[0])\ndf.head()",
  "generation": "df = df.drop(df[df['age'] == -1].index, axis=0)\ndf['scheduled_day'] = pd.to_datetime(df['scheduledday']).dt.date\ndf['appointment_day'] = pd.to_datetime(df['appointmentday']).dt.date\ndf = df.drop(['scheduledday', 'appointmentday'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_1832055",
  "file": "NB_1832055,df-1.ipynb",
  "row_id": 19,
  "target_code": "df['appointment_day'                 ] = df.loc[:, 'appointment_day'                  ].apply(lambda s: s.split('T'   )[0])\ndf.head()",
  "generation": "df['appointment_day'] = df.loc[:,'appointment_day'].apply(lambda s: s.split('T')[0])\ndf.head()"
 },
 {
  "nbid": "NB_1832603",
  "file": "NB_1832603,df-1.ipynb",
  "row_id": 15,
  "target_code": "timeColumns = ['time_1'        , 'time_2'         ,\n       'time_3'               , 'time_4'         , 'time_5'         , 'time_6'         , 'time_7'         , 'time_8'         ]\ndf = rowsToSecods(df, timeColumns)\ndf",
  "generation": "df.iloc[:, 13:] = df.iloc[:, 13:].apply(lambda x: x.dt.total_seconds())\ndf.iloc[:, 13:] = df.iloc[:, 13:].astype(int)\ndf.iloc[:, 13:] = df.iloc[:, 13:].apply(lambda x: x - x.shift(1))\ndf.iloc[:, 13:] = df.iloc[:, 13:].fillna(0)\ndf.iloc[:, 13:] = df.iloc[:, 13:].astype(int)\ndf.rename(columns={'time_1': 'time_0'}, inplace=True)"
 },
 {
  "nbid": "NB_1834677",
  "file": "NB_1834677,weather_df-1.ipynb",
  "row_id": 1,
  "target_code": "weather_df = weather_df.rename(columns = {\"pickup_dt\"           : \"Pickup Date\"              ,\n                                          \"borough\"                                                   : \"Borough\"          ,\n                                          \"pickups\"                                                   : \"Pickups\"          ,\n                                          \"spd\"                                               : \"Wind Speed (MPH)\"                   ,\n                                          \"vsb\"                                               : \"Visibility (Miles)\"                     ,\n                                          \"temp\"                                                : \"Temperature (F)\"                  ,\n                                          \"dewp\"                                                : \"Dewpoint\"           ,\n                                          \"slp\"                                               : \"Sea Level Pressure\"                     ,\n                                          \"pcp01\"                                                 : \"1 Hour Precipitation\"                       ,\n                                          \"pcp06\"                                                 : \"6 Hour Precipitation\"                       ,\n                                          \"pcp24\"                                                 : \"24 Hour Precipitation\"                        ,\n                                          \"sd\"                                              : \"Snow Depth (inches)\"                      ,\n                                          \"hday\"                                                : \"Holiday\"          })\nweather_df.head()",
  "generation": "weather_df.rename(columns={'pickup_dt':'Pickup Date', 'borough':'Borough', 'pickups':'Pickups', 'spd':'Wind Speed (MPH)', 'vsb':'Visibility (Miles)', 'temp':'Temperature (F)', 'dewp':'Dewpoint', 'slp':'Sea Level Pressure', 'pcp01':'1 Hour Precipitation', 'pcp06':'6 Hour Precipitation', 'pcp24':'24 Hour Precipitation', 'sd':'Snow Depth (inches)', 'hday':'Holiday'}, inplace=True)\nweather_df.head()"
 },
 {
  "nbid": "NB_1839391",
  "file": "NB_1839391,df-1.ipynb",
  "row_id": 8,
  "target_code": "df = df.drop('id'    , axis = 1)\ndf.head()",
  "generation": "df.drop('id', axis=1, inplace=True)\ndf"
 },
 {
  "nbid": "NB_1842400",
  "file": "NB_1842400,test_data-1.ipynb",
  "row_id": 53,
  "target_code": "test_data['City_Category'               ]=test_data['City_Category'               ].map({'A'   :0,'B'   :1,'C'   :2})\ntest_data.head()",
  "generation": "def fill_product2(x):\n    if pd.isnull(x):\n        return 0\n    else:\n        return x\n\ndef fill_product3(x):\n    if pd.isnull(x):\n        return 0\n    else:\n        return x\n\ndef func(x):\n    return int(x[1:]) - 1\n\ntest_data['Product_Category_2'] = test_data['Product_Category_2'].apply(fill_product2)\ntest_data['Product_Category_3'] = test_data['Product_Category_3'].apply(fill_product3)\ntest_data['Product_ID'] = test_data['Product_ID'].apply(func)\ntest_data['Male'] = test_data['Male'].map({'F':0, 'M':1})\ntest_data['Occupation'] = test_data['Occupation'] - 1\ntest_data['City_Category'] = test_data['City_Category'].map({'A':0, 'B':1, 'C':2})\ntest_data['Stay_In_Current_City_Years'] = test_data['Stay_In_Current_City_Years'].map({'0':0, '1':1, '2':2, '3':3, '4+':4})\ntest_data.head()"
 },
 {
  "nbid": "NB_1843800",
  "file": "NB_1843800,data-1.ipynb",
  "row_id": 6,
  "target_code": "data['Months'        ]=pd.to_datetime(data['Months'        ])\ndata.head(3)",
  "generation": "data['Month'] = pd.to_datetime(data['Month'])\ndata = data.rename(columns={'Perrin Freres monthly champagne sales millions ?64-?72': 'Sales'})\ndata['Months'] = data['Month'].dt.strftime('%Y-%m-%d')\ndata = data.drop(['Month'], axis=1)\ndata = data[['Months', 'Sales']]\ndata.head()"
 },
 {
  "nbid": "NB_1844886",
  "file": "NB_1844886,media,p2,lns-1.ipynb",
  "row_id": 5,
  "target_code": "media = media.drop(['Unnamed: 7'            ], axis=1)\nmedia['Date'      ] = pd.to_datetime(media['Date'      ]).dt.date\nmedia.head()",
  "generation": "media = media.drop(columns=['Unnamed: 7'])\nmedia['Date'] = pd.to_datetime(media['Date'])\nmedia.head()"
 },
 {
  "nbid": "NB_1844886",
  "file": "NB_1844886,media,p2,lns-1.ipynb",
  "row_id": 8,
  "target_code": "media['day'     ] = media['day'     ].astype(str)\nmedia['day'     ] = media['day'     ].map(lambda x:x[0:2])\nmedia['day'     ] = media['day'     ].astype(int)\nmedia.head()",
  "generation": "media['day'] = pd.DatetimeIndex(media['Date']).day\nmedia.head()"
 },
 {
  "nbid": "NB_1844886",
  "file": "NB_1844886,media,p2,lns-1.ipynb",
  "row_id": 12,
  "target_code": "media['weekday'         ] = (media['day'     ] + 3) % 7\nmedia.weekday.replace(0, 7, inplace=True)\nmedia['weekday'         ] = media['weekday'         ].astype(int)\nmedia.head()",
  "generation": "media['weekday'] = pd.to_datetime(media['Date']).dt.weekday + 1\nmedia.head()"
 },
 {
  "nbid": "NB_1846324",
  "file": "NB_1846324,csv_data,data_y-1.ipynb",
  "row_id": 29,
  "target_code": "add_finished = {0: 1, 1: 0}\ncsv_data['finished'          ] = csv_data['unfinished'            ].map(add_finished)\ncsv_data.head()",
  "generation": "csv_data['finished'        ]=0\ncsv_data.head()"
 },
 {
  "nbid": "NB_1852698",
  "file": "NB_1852698,new_data-1.ipynb",
  "row_id": 4,
  "target_code": "new_data.index = new_data.Date\nnew_data.drop('Date'      , axis=1, inplace=True)\nnew_data.head()",
  "generation": "new_data = new_data.set_index('Date')"
 },
 {
  "nbid": "NB_1858287",
  "file": "NB_1858287,df-1.ipynb",
  "row_id": 4,
  "target_code": "df = pd.merge(df, features, on=['Store'       , 'Date'       , 'IsHoliday'            ])\ndf = pd.merge(df, stores, on=['Store'       ])\ndf.head(3)",
  "generation": "df = pd.merge(df, stores, how='left', on='Store')\ndf = pd.merge(df, features, how='left', on=['Store', 'Date', 'IsHoliday'])\ndf.head()"
 },
 {
  "nbid": "NB_1858287",
  "file": "NB_1858287,df-1.ipynb",
  "row_id": 5,
  "target_code": "df['Date'      ] = pd.to_datetime(df['Date'      ])\ndf['Week'      ] = df.Date.dt.week\ndf['Month'       ] = df.Date.dt.month\ndf['Year'      ] = df.Date.dt.year\ndf['Year_Week'           ] = df.Date.dt.strftime('%Y_%W'       )\ndf.head()",
  "generation": "df['Week'] = pd.to_datetime(df['Date']).dt.week\ndf['Month'] = pd.to_datetime(df['Date']).dt.month\ndf['Year'] = pd.to_datetime(df['Date']).dt.year\ndf['Year_Week'] = pd.to_datetime(df['Date']).dt.strftime('%Y_%U')\ndf.head(3)"
 },
 {
  "nbid": "NB_1858934",
  "file": "NB_1858934,df-1.ipynb",
  "row_id": 63,
  "target_code": "df['grid'      ]=df.index\ndf.head()",
  "generation": "df['grid'] = df.index\ndf.head()"
 },
 {
  "nbid": "NB_1861232",
  "file": "NB_1861232,data-1.ipynb",
  "row_id": 3,
  "target_code": "data['Total'       ] = data.eval('East + West'             )\ndata.head()",
  "generation": "data.columns = ['East', 'West']\ndata['Total'] = data['East'] + data['West']\ndata.head()"
 },
 {
  "nbid": "NB_1866638",
  "file": "NB_1866638,user_df-1.ipynb",
  "row_id": 5,
  "target_code": "user_df = (pd.pivot_table(user_df, \n                      index = ['user_id'         ,'game_title'            ], \n                      columns='action'        , \n                      values='hours_played'              )\n        .reset_index()\n        .fillna(0)\n        .rename(index = str, columns = {'play'      : 'hours_played'               , 'purchase'           :'purchased'           }))\nuser_df.head(n=10)",
  "generation": "reader = Reader(rating_scale=(1, 5))\ndata = Dataset.load_from_df(user_df[['user_id', 'game_title', 'hours_played']], reader)\ntrainset, testset = train_test_split(data, test_size=.25)\nalgo = SVDpp()\nalgo.fit(trainset)\npredictions = algo.test(testset)\naccuracy.rmse(predictions)\nuser_df['purchased'] = user_df['action'].apply(lambda x: 1 if x == 'purchase' else 0)\nuser_df.drop('hours_played', axis=1, inplace=True)\nuser_df.drop('action', axis=1, inplace=True)\nuser_df.drop_duplicates(inplace=True)\nuser_df = user_df[user_df['purchased'] == 1]\nuser_df.drop('purchased', axis=1, inplace=True)\nuser_df.reset_index(drop=True, inplace=True)\nuser_df.head()"
 },
 {
  "nbid": "NB_1867389",
  "file": "NB_1867389,user_engagement_df-1.ipynb",
  "row_id": 12,
  "target_code": "user_engagement_df['time_stamp'            ] = pd.to_datetime(user_engagement_df['time_stamp'            ])\nuser_engagement_df.set_index('time_stamp'            , inplace=True)\nuser_engagement_df.head()",
  "generation": "user_engagement_df.set_index('time_stamp', inplace=True)\nuser_engagement_df.head()"
 },
 {
  "nbid": "NB_1869527",
  "file": "NB_1869527,df-1.ipynb",
  "row_id": 5,
  "target_code": "df.columns = ['timestamp'           , 'author'         , 'img'      , 'message'          , 'nlp'      ,  'face'        ]\ndf = df.drop(0, axis=0)\ndf.head()",
  "generation": "df.columns = ['timestamp', 'author', 'img', 'message', 'nlp', 'face']\ndf = df.drop(['message', 'nlp'], axis=1)\ndf = df[df['face'].notna()]\ndf = df.reset_index(drop=True)\ndf = df.iloc[1:]"
 },
 {
  "nbid": "NB_1869527",
  "file": "NB_1869527,df-1.ipynb",
  "row_id": 6,
  "target_code": "df['timestamp'           ] = pd.to_datetime(df.timestamp, unit=\"ms\"    ) \ndf.head()",
  "generation": "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\ndf.head()"
 },
 {
  "nbid": "NB_1869527",
  "file": "NB_1869527,df-1.ipynb",
  "row_id": 7,
  "target_code": "df.nlp = df.nlp.map(lambda item: json.loads(item)['documents'           ][0]['documentScores'                ]) \ndf.head()",
  "generation": "df['message'] = df['message'].apply(lambda x: json.loads(x))\ndf['nlp'] = df['message'].apply(lambda x: x['documentSentiment'])\ndf.head()"
 },
 {
  "nbid": "NB_1869527",
  "file": "NB_1869527,df-1.ipynb",
  "row_id": 8,
  "target_code": "json.loads(df.face[2])[0]['faceRectangle'               ]\ndf.face = df.face.map(lambda item: json.loads(item))\ndf.face = df.face.map(lambda item: item[0]['faceRectangle'               ] if len(item) else [])\ndf.head()",
  "generation": "df['face'] = df['face'].apply(lambda x: json.loads(x)[0]['faceRectangle'] if x != '[]' else {})"
 },
 {
  "nbid": "NB_1869663",
  "file": "NB_1869663,avg_beer_rating-1.ipynb",
  "row_id": 32,
  "target_code": "avg_beer_rating = avg_beer_rating.sort_values('review_overall'                )\navg_beer_rating.head()",
  "generation": "avg_beer_rating = pd.DataFrame(df1.groupby('beer_beerid')['review_overall'].mean())\navg_beer_rating = avg_beer_rating.reset_index()\navg_beer_rating['review_overall'] = round(avg_beer_rating['review_overall'],2)\navg_beer_rating = avg_beer_rating.sort_values(by=['review_overall'])\navg_beer_rating = avg_beer_rating.reset_index(drop=True)\navg_beer_rating = avg_beer_rating.rename(columns={'index': 'Unnamed: 0'})\navg_beer_rating.head()"
 },
 {
  "nbid": "NB_1869663",
  "file": "NB_1869663,avg_user_rating-1.ipynb",
  "row_id": 41,
  "target_code": "avg_user_rating = avg_user_rating.sort_values('review_overall'                )\navg_user_rating.head()",
  "generation": "least_rated_users = avg_user_rating.sort_values('review_overall').head()\nleast_rated_users"
 },
 {
  "nbid": "NB_1872299",
  "file": "NB_1872299,pivot-1.ipynb",
  "row_id": 47,
  "target_code": "pivot = pivot.reset_index()\npivot.head()",
  "generation": "pivot.reset_index(inplace=True)\npivot.head()"
 },
 {
  "nbid": "NB_1879582",
  "file": "NB_1879582,df-1.ipynb",
  "row_id": 13,
  "target_code": "df['no_show_bool'              ]=[True if x == 'Yes'       else False for x in df['no_show'         ]]\ndf.head(1)",
  "generation": "df['no_show_bool'] = df['no_show'].apply(lambda x: True if x == 'Yes' else False)\ndf.head()"
 },
 {
  "nbid": "NB_1879582",
  "file": "NB_1879582,df-1.ipynb",
  "row_id": 17,
  "target_code": "scheduled_date=pd.to_datetime(df['scheduled_day'               ]).dt.date\nappointment_date=pd.to_datetime(df['appointment_day'                 ]).dt.date\ndf['awaiting_time'               ]=appointment_date-scheduled_date\ndf['awaiting_time'               ]= df['awaiting_time'               ].apply(lambda x: x.days)\ndf.head()",
  "generation": "df['scheduled_day'] = pd.to_datetime(df['scheduled_day'])\ndf['appointment_day'] = pd.to_datetime(df['appointment_day'])\ndf['awaiting_time'] = (df['appointment_day'] - df['scheduled_day']).dt.days\ndf['awaiting_days'] = pd.cut(df['awaiting_time'], [-1,0,1,7,14,21,31,1000], labels=['Sameday','1Day','2Days~1Week','1Week~2Weeks','2Weeks~3Weeks','3Weeks~1Month','1Month~'])"
 },
 {
  "nbid": "NB_1881683",
  "file": "NB_1881683,df,y-1.ipynb",
  "row_id": 9,
  "target_code": "for s in range(1,6):\n    df['Close_{}_DayAgo'                 .format(s)] = df['Close'       ].shift(s)\nfor s in range(1,6):\n    df['Volume_{}_DayAgo'                  .format(s)] = df['Volume'        ].shift(s)\nfor s in range(1,6):\n    df['Low_{}_DayAgo'               .format(s)] = df['Volume'        ].shift(s)\nfor s in range(1,6):\n    df['High_{}_DayAgo'                .format(s)] = df['Volume'        ].shift(s)\nX = df.dropna().drop(['Close'       ,'Volume'        ,'Low'     ,'High'      ], axis=1)\ny = df.dropna()[['Close'       ]]\ndf.head(7)",
  "generation": "df['Close_1_DayAgo'] = df['Close'].shift(1)\ndf['Close_2_DayAgo'] = df['Close'].shift(2)\ndf['Close_3_DayAgo'] = df['Close'].shift(3)\ndf['Close_4_DayAgo'] = df['Close'].shift(4)\ndf['Close_5_DayAgo'] = df['Close'].shift(5)\ndf['Volume_1_DayAgo'] = df['Volume'].shift(1)\ndf['Low_1_DayAgo'] = df['Low'].shift(1)\ndf['Low_2_DayAgo'] = df['Low'].shift(2)\ndf['Low_3_DayAgo'] = df['Low'].shift(3)\ndf['Low_4_DayAgo'] = df['Low'].shift(4)\ndf['Low_5_DayAgo'] = df['Low'].shift(5)\ndf['High_1_DayAgo'] = df['High'].shift(1)\ndf['High_2_DayAgo'] = df['High'].shift(2)\ndf['High_3_DayAgo'] = df['High'].shift(3)\ndf['High_4_DayAgo'] = df['High'].shift(4)\ndf['High_5_DayAgo'] = df['High'].shift(5)\ndf"
 },
 {
  "nbid": "NB_1882044",
  "file": "NB_1882044,ratings-1.ipynb",
  "row_id": 5,
  "target_code": "ratings = ratings.join(movie_names, on='movieId'         , rsuffix='_'   )[['userId'        , 'movieId'          , 'rating'         , 'title'        , 'genres'         ]]\nratings.head()",
  "generation": "import pandas as pd\nimport matplotlib.pyplot as plt\nmovie_names = pd.read_csv('data/ml-latest-small/movies.csv')\nratings = pd.read_csv('data/ml-latest-small/ratings.csv')\nratings = pd.merge(ratings, movie_names, on='movieId')\nratings = ratings[['userId', 'movieId', 'rating', 'title', 'genres']]\nratings.head()"
 },
 {
  "nbid": "NB_1882044",
  "file": "NB_1882044,ratings-1.ipynb",
  "row_id": 7,
  "target_code": "userid2idx = {o:i for i,o in enumerate(users)}\nmovieid2idx = {o:i for i,o in enumerate(movies)}\nratings.movieId = ratings.movieId.apply(lambda x: movieid2idx[x])\nratings.userId = ratings.userId.apply(lambda x: userid2idx[x])\nratings.head()",
  "generation": "movie_dict = {}\nfor i, movie_id in enumerate(movies):\n    movie_dict[movie_id] = i\nratings['movieId'] = ratings['movieId'].apply(lambda x: movie_dict[x])\nratings = ratings.rename(columns={'Unnamed: 0': 'index'})\nratings['userId'] = ratings['userId'] - 1\nratings.head()"
 },
 {
  "nbid": "NB_19041",
  "file": "NB_19041,copy_df,h-1.ipynb",
  "row_id": 55,
  "target_code": "copy_df = copy_df.drop(['Price'       ], axis=1)\ncopy_df.head()",
  "generation": "final_df = final_df.drop('Price', axis=1) \nfinal_df.head()"
 },
 {
  "nbid": "NB_2900109",
  "file": "NB_2900109,df,demented_women,demented_women_count,demented_men,demented_men_count,non_demented,non_demented_women,non_demented_women_count,no-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.loc[df['Visit'       ]==1]\ndf.head()",
  "generation": "df = df[df['Visit'] == 1]\ndf.head()"
 },
 {
  "nbid": "NB_2900109",
  "file": "NB_2900109,df,demented_women,demented_women_count,demented_men,demented_men_count,non_demented,non_demented_women,non_demented_women_count,no-1.ipynb",
  "row_id": 6,
  "target_code": "df = df.reset_index(drop=True)\ndf.head(10)",
  "generation": "df = df.loc[df['Visit']==1].reset_index(drop=True)\ndf.head()"
 },
 {
  "nbid": "NB_2900109",
  "file": "NB_2900109,df,demented_women,demented_women_count,demented_men,demented_men_count,non_demented,non_demented_women,non_demented_women_count,no-1.ipynb",
  "row_id": 7,
  "target_code": "df['M/F'     ] = df['M/F'     ].replace(['F'   ,'M'   ], [0,1])\ndf.head(15)",
  "generation": "df['M/F'] = np.where(df['M/F'] == 'M', 1, 0)\ndf.head()"
 },
 {
  "nbid": "NB_2900109",
  "file": "NB_2900109,df,demented_women,demented_women_count,demented_men,demented_men_count,non_demented,non_demented_women,non_demented_women_count,no-1.ipynb",
  "row_id": 8,
  "target_code": "df['Group'       ] = df['Group'       ].replace(['Converted'           ], ['Demented'          ])\ndf['Classification'                ] = df['Group'       ]\ndf['Group'       ] = df['Group'       ].replace(['Demented'          , 'Nondemented'              ], [1,0])\ndf = df.drop(['MRI ID'        , 'Visit'        , 'Hand'       ], axis=1)\ndf.head(15)",
  "generation": "df = df.drop(['MRI ID', 'Visit', 'Hand', 'CDR'], axis=1)\ndf['M/F'] = df['M/F'].replace(['F','M'], [0,1])\ndf['Classification'] = df['Group'].replace(['Converted'], 'Demented')\ndf['Classification'] = df['Classification'].replace(['Demented', 'Nondemented'], [1, 0])\ndf = df.drop(['Group'], axis=1)\ndf = df.rename(columns={'Subject ID': 'Subject_ID', 'MR Delay': 'MR_Delay', 'M/F': 'Gender', 'EDUC': 'Education', 'MMSE': 'MMSE_Score', 'eTIV': 'Estimated_Total_IntraCranial_Vol', 'nWBV': 'Normalized_Whole_Brain_Vol', 'ASF': 'Atlas_Scaling_Factor'})\ndf.head()"
 },
 {
  "nbid": "NB_2902079",
  "file": "NB_2902079,dfsxpct-1.ipynb",
  "row_id": 41,
  "target_code": "dfsxpct = dfsxpct.reset_index()\ndfsxpct",
  "generation": "df = df.reset_index()\ndf = df[['Gender', 'NoShow']]\ndf['genderPct'] = df.groupby(['Gender', 'NoShow']).transform('count') / df.groupby(['Gender']).transform('count')\ndf = df.drop_duplicates().reset_index(drop=True)\ndf.columns = ['Gender', 'NoShow', 'genderPct']\ndf.head()"
 },
 {
  "nbid": "NB_2904994",
  "file": "NB_2904994,bird_data,x-1.ipynb",
  "row_id": 6,
  "target_code": "bird_data[\"timestamp\"           ] = pd.Series(timestamps, index = bird_data.index)\nbird_data.head()",
  "generation": "bird_data['timestamp'] = pd.to_datetime(bird_data['date_time'])\nbird_data.head()"
 },
 {
  "nbid": "NB_2907477",
  "file": "NB_2907477,melt_data-1.ipynb",
  "row_id": 8,
  "target_code": "melt_data = melt_data.rename(columns={'variable'          : 'Year'       })\nmelt_data",
  "generation": "melt_data = melt_data.rename(columns={'variable': 'Year'})\nmelt_data = melt_data.drop('MSN', axis=1)\nmelt_data.head()"
 },
 {
  "nbid": "NB_2912057",
  "file": "NB_2912057,df,females-1.ipynb",
  "row_id": 14,
  "target_code": "df['gendercolor'             ]=df['Gender'        ].map({'Male'      :'blue'      , 'Female'         :'green'       })\ndf.head()",
  "generation": "df['Month'] = pd.to_datetime(df['Month'])\ndf = df.set_index('Month')\ndf.head()"
 },
 {
  "nbid": "NB_2922031",
  "file": "NB_2922031,data-1.ipynb",
  "row_id": 29,
  "target_code": "data['msg_clean'           ] = data['msg'     ].apply(lambda x: remove_punctuation(x))\ndata.head()",
  "generation": "vectorizer = CountVectorizer()\ndata['msg_clean'] = data['sms'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndata['msg_clean'] = data['msg_clean'].str.replace('[^\\w\\s]','')\ndata['msg_clean'] = data['msg_clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ndata['msg_clean'] = data['msg_clean'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ndata.head()"
 },
 {
  "nbid": "NB_2922031",
  "file": "NB_2922031,data-1.ipynb",
  "row_id": 62,
  "target_code": "data['msg_nostop'            ] = data['msg'     ].apply(lambda x: clean_text(x.lower()))\ndata.head()",
  "generation": "vectorizer = CountVectorizer(stop_words='english')\ndata['msg_nostop'] = data['msg_clean'].apply(lambda x: vectorizer.build_analyzer()(x))\ndata.head()"
 },
 {
  "nbid": "NB_2922031",
  "file": "NB_2922031,data-1.ipynb",
  "row_id": 51,
  "target_code": "data['msg_stemmed'             ] = data['msg_nostop'            ].apply(lambda x: stemming(x))\ndata.head()",
  "generation": "from nltk.stem.porter import PorterStemmer\n\nstemmer = PorterStemmer()\n\ndata['msg_stemmed'] = data['msg_nostop'].apply(lambda x: [stemmer.stem(word) for word in x])\ndata.head()"
 },
 {
  "nbid": "NB_2922031",
  "file": "NB_2922031,data-1.ipynb",
  "row_id": 64,
  "target_code": "data['msg_lemmatized'                ] = data['msg_nostop'            ].apply(lambda x: lemmatization(x))\ndata.head()",
  "generation": "cv = CountVectorizer()\ndata_cv = cv.fit_transform(data['msg_nostop'])\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_dtm.index = data.index\ndata = pd.concat([data, data_dtm], axis=1)\ndata.head()"
 },
 {
  "nbid": "NB_301147",
  "file": "NB_301147,data,lower,mask-1.ipynb",
  "row_id": 54,
  "target_code": "create_ratio = lambda row: row.cement/row.water \ncol = data.apply(create_ratio, axis=1) \ndata = data.assign(ratio = col.values) \ndata.head()",
  "generation": "def ratio_cement_water(data):\n    data['ratio'] = data['cement'] / data['water']\n    return data\n\ndata = ratio_cement_water(data)\ndata.head()"
 },
 {
  "nbid": "NB_319196",
  "file": "NB_319196,COVID_GSPC-1.ipynb",
  "row_id": 8,
  "target_code": "COVID_GSPC.columns = [str(col).upper().replace(' '   , '_'    ) for col in COVID_GSPC.columns]\nCOVID_GSPC.head()",
  "generation": "COVID_GSPC = COVID_GSPC.rename(columns={\"Date\": \"DATE\", \"fips\": \"FIPS\", \"cases\": \"CASES\", \"deaths\": \"DEATHS\", \"High\": \"HIGH\", \"Low\": \"LOW\", \"Open\": \"OPEN\", \"Close\": \"CLOSE\", \"Volume\": \"VOLUME\", \"Adj Close\": \"ADJ_CLOSE\"})\nCOVID_GSPC.head()"
 },
 {
  "nbid": "NB_5412980",
  "file": "NB_5412980,data-1.ipynb",
  "row_id": 6,
  "target_code": "data['sample'        ] = data['height'        ] + 3\ndata.head()",
  "generation": "data['sample'] = data['height'] + 3\ndata.head()"
 },
 {
  "nbid": "NB_5417739",
  "file": "NB_5417739,df-1.ipynb",
  "row_id": 2,
  "target_code": "df = df.drop(columns=['Unnamed: 0'            ])\ndf = df.drop(columns=['DiabetesPedigreeFunction'                          ])\ndf",
  "generation": "df = df[['Pregnancies', 'Age', 'Outcome']]\ndf.head()"
 },
 {
  "nbid": "NB_5425212",
  "file": "NB_5425212,test-1.ipynb",
  "row_id": 11,
  "target_code": "test = test.drop('thal'      , 1)\ntest = test.drop('major_vessels_num'                   , 1)\ntest = pd.get_dummies(test, prefix=[\n    \"sex\"         , \"chest_pain_type\"                  , \"high_fasting_blood_sugar\"                           , \"resting_ecg\"              , \n    \"exercise_induced_angina\"                             , \"peak_exercise_st\"                   ])\ntest.head()",
  "generation": "categorical_attributes = [\"sex\", \"chest_pain_type\", \"high_fasting_blood_sugar\", \"resting_ecg\", \"exercise_induced_angina\", \"peak_exercise_st\", \"major_vessels_num\", \"thal\"]\nseries_attributes = [\"age\", \"rest_blood_pressure\", \"serum_cholestrol\", \"max_heart_rate\", \"st_depression\"]\n\ntest[categorical_attributes] = test[categorical_attributes].fillna(test.mode().iloc[0])\ntest[series_attributes] = test[series_attributes].fillna(test.mean())\ntest[categorical_attributes] = test[categorical_attributes].astype('category')\n\ntest = pd.get_dummies(test, columns=categorical_attributes, prefix='', prefix_sep='')\n\ntest.head()"
 },
 {
  "nbid": "NB_5888908",
  "file": "NB_5888908,model_data-1.ipynb",
  "row_id": 3,
  "target_code": "model_data = pd.get_dummies(model_data, drop_first=True)\nmodel_data.head(5)",
  "generation": "model_data = pd.get_dummies(model_data, columns=['work_rate_att'], prefix='work_rate_att')\nmodel_data = model_data[['age', 'height_cm', 'weight_kg', 'overall', 'work_rate_att_Low', 'work_rate_att_Medium']]\nmodel_data"
 },
 {
  "nbid": "NB_1887906",
  "file": "NB_1887906,df_happy-1.ipynb",
  "row_id": 10,
  "target_code": "df_happy['length'        ] = df_happy['cleaned_hm'            ].apply(lambda x: len(x.split()))\ndf_happy.head(2)",
  "generation": "df_happy['length'] = df_happy['cleaned_hm'].apply(len)\ndf_happy.head(2)"
 },
 {
  "nbid": "NB_1887906",
  "file": "NB_1887906,df_happy-1.ipynb",
  "row_id": 11,
  "target_code": "df_happy=pd.merge(df_happy, df_demographics, on='wid'     , validate = 'm:1'      )\ndf_happy.head(3)",
  "generation": "df_demographics.loc[:, 'age'] = 35\ndf_demographics.loc[:, 'country'] = 'USA'\ndf_demographics.loc[:, 'gender'] = 'm'\ndf_demographics.loc[:, 'marital'] = 'single'\ndf_demographics.loc[:, 'parenthood'] = 'n'\n\nmerged_df = pd.merge(df_happy, df_demographics, on='wid', how='inner')\nmerged_df.head(2)"
 },
 {
  "nbid": "NB_1890501",
  "file": "NB_1890501,df-1.ipynb",
  "row_id": 33,
  "target_code": "dic = [k_range, scores]\ndf = pd.DataFrame(np.array(dic))\ndf.head()\ndf=df.transpose()\ndf.head()",
  "generation": "df_corr = df.corr()\ndf_corr = df_corr[['chd']]\ndf_corr = df_corr[:-1]\ndf_corr.reset_index(inplace=True)\ndf_corr.columns = ['index', 'corr']\ndf_corr['corr'] = df_corr['corr'].abs()\ndf_corr = df_corr.sort_values(by='corr', ascending=False)\ndf_corr = df_corr.head(2)\ndf_corr = df_corr[['index']]\ndf_corr.reset_index(inplace=True)\ndf_corr.drop('index', axis=1, inplace=True)\ndf_corr.columns = [0, 1]\ndf_corr = df_corr.transpose()\ndf_corr.reset_index(inplace=True)\ndf_corr.drop('index', axis=1, inplace=True)\ndf_corr.columns = ['Unnamed: 0', 0, 1]\ndf_corr = df_corr[['Unnamed: 0', 0, 1]]\ndf_corr.reset_index(inplace=True)\ndf_corr.drop('index', axis=1, inplace=True)\ndf_corr.columns = ['Unnamed: 0', 0, 1]"
 },
 {
  "nbid": "NB_1900719",
  "file": "NB_1900719,car_dataset_dup-1.ipynb",
  "row_id": 27,
  "target_code": "car_dataset_dup['Fuel_Type_id'              ]=pd.Categorical(car_dataset_dup.Fuel_Type).codes\ncar_dataset_dup['Transmission_Type_id'                      ]=pd.Categorical(car_dataset_dup.Transmission_Type).codes\ncar_dataset_dup.head(10)",
  "generation": "car_dataset_dup['Fuel_Type_id'] = car_dataset_dup['Fuel_Type'].replace({'CNG':0,'Diesel':1,'Electric':2,'LPG':3,'Petrol':4})\ncar_dataset_dup['Transmission_Type_id'] = car_dataset_dup['Transmission_Type'].replace({'Automatic':0,'Manual':1})\ncar_dataset_dup.head()"
 },
 {
  "nbid": "NB_1902925",
  "file": "NB_1902925,services-1.ipynb",
  "row_id": 17,
  "target_code": "services = services.groupby('services'          ).sum()\nservices = services[services['counts'        ]>1]\nservices = services.reset_index()\nservices",
  "generation": "services = services.groupby('services').sum().reset_index()\nservices = services.sort_values(by='counts', ascending=False)\nservices.head()"
 },
 {
  "nbid": "NB_1904246",
  "file": "NB_1904246,iris-1.ipynb",
  "row_id": 47,
  "target_code": "iris['nueva_caracter\u00edstica'                      ] = iris.largo_petalo.apply(model)\niris.head()",
  "generation": "iris['nueva_caracter\u00edstica'] = iris['largo_sepalo'] / iris['ancho_sepalo']\niris.head()"
 },
 {
  "nbid": "NB_1912411",
  "file": "NB_1912411,data,women-1.ipynb",
  "row_id": 5,
  "target_code": "data['split_sec'           ] = data['split'       ] / np.timedelta64(1, 's'    )\ndata['final_sec'           ] = data['final'       ] / np.timedelta64(1, 's'    )\ndata.head()",
  "generation": "data['split_sec'] = pd.to_timedelta(data['split']).dt.total_seconds()\ndata['final_sec'] = pd.to_timedelta(data['final']).dt.total_seconds()\ndata.head()"
 },
 {
  "nbid": "NB_1917442",
  "file": "NB_1917442,suez_cement-1.ipynb",
  "row_id": 4,
  "target_code": "suez_cement['EWMA_12'         ]=suez_cement['CLOSE_PRICE'             ].ewm(span=12).mean()\nsuez_cement['HL_PCT'        ]=(suez_cement['HIGH_PRICE'            ]-suez_cement['LOW_PRICE'           ])/suez_cement['LOW_PRICE'           ]*100.0\nsuez_cement['PCT_change'            ]=(suez_cement['CLOSE_PRICE'             ]-suez_cement['OPEN_PRICE'            ])/suez_cement['OPEN_PRICE'            ]*100.0\nsuez_cement.head()",
  "generation": "suez_cement['EWMA_12'] = suez_cement['CLOSE_PRICE'].ewm(span=12, adjust=False).mean()\nsuez_cement['HL_PCT'] = (suez_cement['HIGH_PRICE'] - suez_cement['LOW_PRICE']) / suez_cement['CLOSE_PRICE'] * 100.0\nsuez_cement['PCT_change'] = (suez_cement['CLOSE_PRICE'] - suez_cement['OPEN_PRICE']) / suez_cement['OPEN_PRICE'] * 100.0\nsuez_cement.head()"
 },
 {
  "nbid": "NB_1921112",
  "file": "NB_1921112,df-1.ipynb",
  "row_id": 5,
  "target_code": "df=df.drop(['Unnamed: 0'            ],axis=1)\ndf.head(3)",
  "generation": "df = df.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\ndf.head(3)"
 },
 {
  "nbid": "NB_1925415",
  "file": "NB_1925415,vnc_crime_cat-1.ipynb",
  "row_id": 22,
  "target_code": "vnc_crime_cat.reset_index(inplace = True)\nvnc_crime_cat.columns = vnc_crime_cat.columns.map(''  .join)\nvnc_crime_cat.rename(columns={'YearAll'         :'Total'       }, inplace=True)\nvnc_crime_cat = vnc_crime_cat.head(4) \nvnc_crime_cat",
  "generation": "vnc_crime_cat = pd.pivot_table(vnc_boroughs_crime, values=['Year'], index=['Borough'], columns=['Type'], aggfunc=np.sum, fill_value=0, margins=False)\nvnc_crime_cat = vnc_crime_cat.reset_index()\nvnc_crime_cat.columns = vnc_crime_cat.columns.map(''.join)\nvnc_crime_cat = vnc_crime_cat.rename(columns={\"Borough\": \"Unnamed: 0\"})\nvnc_crime_cat = pd.melt(vnc_crime_cat, id_vars=[\"Unnamed: 0\"], var_name=\"Type\", value_name=\"Total\")\nvnc_crime_cat = vnc_crime_cat.sort_values(by=[\"Unnamed: 0\", \"Type\"])\nvnc_crime_cat = vnc_crime_cat.reset_index(drop=True)\nvnc_crime_cat.head()"
 },
 {
  "nbid": "NB_1925415",
  "file": "NB_1925415,vnc_crime_cat-1.ipynb",
  "row_id": 39,
  "target_code": "vnc_crime_cat.reset_index(inplace = True)\nvnc_crime_cat.columns = vnc_crime_cat.columns.map(''  .join)\nvnc_crime_cat.rename(columns={'YearAll'         :'Total'       ,\n                              'YearBreak and Enter Commercial'                                                               : 'Break and Enter Commercial'                             ,\n                              'YearBreak and Enter Residential/Other'                                                                      : 'Break and Enter Residential'                              ,\n                              'YearMischief'                                             : 'Mischief'           ,\n                              'YearOther Theft'                                                : 'Other'        ,\n                              'YearTheft from Vehicle'                                                       : 'Theft from Vehicle'                     ,\n                              'YearTheft of Bicycle'                                                     : 'Theft of Bicycle'                   ,\n                              'YearTheft of Vehicle'                                                     : 'Theft of Vehicle'                   ,\n                              'YearVehicle Collision or Pedestrian Struck (with Fatality)'                                                                                           : 'Vehicle Collision or Pedestrian Struck (with Fatality)'                                                         ,\n                              'YearVehicle Collision or Pedestrian Struck (with Injury)'                                                                                         : 'Vehicle Collision or Pedestrian Struck (with Injury)'                                                       }, inplace=True)\nvnc_crime_cat = vnc_crime_cat.head(4) \nvnc_crime_cat",
  "generation": "vnc_crime_cat = pd.pivot_table(vnc_boroughs_crime,values=['Year'],index=['Borough'],columns=['Type'],aggfunc=len,fill_value=0,margins=True)\nvnc_crime_cat.reset_index(inplace = True)\nvnc_crime_cat.columns = vnc_crime_cat.columns.map(''.join)\nvnc_crime_cat.rename(columns={'YearAll':'Total'}, inplace=True)\nvnc_crime_cat = vnc_crime_cat[['Borough', 'Break and Enter Commercial', 'Break and Enter Residential/Other', 'Mischief', 'Other Theft', 'Theft from Vehicle', 'Theft of Bicycle', 'Theft of Vehicle', 'Vehicle Collision or Pedestrian Struck (with Fatality)', 'Vehicle Collision or Pedestrian Struck (with Injury)', 'Total']]\nvnc_crime_cat.rename(columns={'Break and Enter Residential/Other':'Break and Enter Residential'}, inplace=True)\nvnc_crime_cat.head()"
 },
 {
  "nbid": "NB_1936801",
  "file": "NB_1936801,nba,y-1.ipynb",
  "row_id": 2,
  "target_code": "nba = nba.drop(['MIN'     ,'FGA'     ,'FG%'     ,'3PM'     ,'3PA'     ,'FTM'     ,'FTA'     ,'OREB'      ,'DREB'      ,'TOV'     ,'PF'    ,'EFF'     ,'AST/TOV'         ,'STL/TOV'         ,'Age'     ,'Birth_Place'             ,'Birthdate'           ,'Collage'         ,'Experience'            ,'Height'        ,'Pos'     ,'Team'      ,'Weight'        ,'BMI'     ], axis=1)\nnba",
  "generation": "nba = nba[['Name','Games Played', 'PTS','3P%', 'FT%', 'REB','AST','STL','BLK']]\nnba = nba.dropna()\nnba.head()"
 },
 {
  "nbid": "NB_1938977",
  "file": "NB_1938977,uber-1.ipynb",
  "row_id": 9,
  "target_code": "uber['Pickup_time'             ] = uber['Request timestamp'                   ].dt.time\nuber['Drop_time'           ] = uber['Drop timestamp'                ].dt.time\nuber['Request_day'             ] = uber['Request timestamp'                   ].dt.day_name()\nuber.head()",
  "generation": "uber['Pickup_time'] = pd.to_datetime(uber['Request timestamp']).dt.time\nuber['Drop_time'] = pd.to_datetime(uber['Drop timestamp']).dt.time\nuber['Request_day'] = pd.to_datetime(uber['Request timestamp']).dt.day_name()"
 },
 {
  "nbid": "NB_1938977",
  "file": "NB_1938977,uber-1.ipynb",
  "row_id": 10,
  "target_code": "uber['Request_hour'              ] = uber['Request timestamp'                   ].dt.hour\nuber.head()",
  "generation": "uber['Request_hour'] = uber['Request timestamp'].dt.hour\nuber.head()"
 },
 {
  "nbid": "NB_1938977",
  "file": "NB_1938977,uber-1.ipynb",
  "row_id": 11,
  "target_code": "uber['Day_part'          ] = pd.cut(uber['Request_hour'              ],[0,4,8,12,16,20,24],labels=['Late Night'            , 'Early Morning'                , 'Morning'          , 'Afternoon'            , 'Evening'          , 'Night'        ])\nuber.head()",
  "generation": "uber['Day_part'] = pd.cut(uber['Request_hour'], bins=[0, 6, 12, 16, 20, 24], labels=['Early Morning', 'Morning', 'Afternoon', 'Evening', 'Night'])\nuber.head()"
 },
 {
  "nbid": "NB_1944048",
  "file": "NB_1944048,df-1.ipynb",
  "row_id": 258,
  "target_code": "cols=list(df.columns)\ndf=df[cols[:2]+[cols[-1]]+cols[2:]]\ndf",
  "generation": "df = df[['Unnamed: 0', 'GEOID', 'State', 'exp', 'exp.1', 'exp.2', 'exp.3', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', 'exp.4', 'exp.5']]\ndf"
 },
 {
  "nbid": "NB_1950542",
  "file": "NB_1950542,df-1.ipynb",
  "row_id": 11,
  "target_code": "df['user_rating_label'                   ] = pd.cut(df['user_rating'             ], bins = 3, labels = ['Low'     , 'Medium'         , 'High'       ])\ndf.head()",
  "generation": "bins = [0, 2.99, 3.99, 5]\nlabels = ['Low', 'Medium', 'High']\ndf['user_rating_label'] = pd.cut(df['user_rating'], bins=bins, labels=labels)\ndf = df[['size_bytes', 'price', 'rating_count_tot', 'rating_count_ver', 'user_rating', 'user_rating_ver', 'ver', 'cont_rating', 'prime_genre', 'sup_devices.num', 'ipadSc_urls.num', 'lang.num', 'vpp_lic', 'user_rating_label']]\ndf.head()"
 },
 {
  "nbid": "NB_1952713",
  "file": "NB_1952713,user_amount,counts_total-1.ipynb",
  "row_id": 65,
  "target_code": "amount_total = user_amount.amount_cumsum.max()\nuser_amount['prop'      ] = user_amount.apply(lambda x: x.amount_cumsum/amount_total, axis = 1)\nuser_amount.tail()",
  "generation": "user_amount['prop'] = user_amount['amount_cumsum'] / user_amount['order_amount'].sum()\nuser_amount"
 },
 {
  "nbid": "NB_1955217",
  "file": "NB_1955217,data-1.ipynb",
  "row_id": 5,
  "target_code": "data = data.replace(np.nan,''  ,regex=True)\ndata.head(10)",
  "generation": "data = data.replace(np.nan, '', regex=True)\ndata.head()"
 },
 {
  "nbid": "NB_1962585",
  "file": "NB_1962585,df-1.ipynb",
  "row_id": 19,
  "target_code": "df['Request time'              ] = df['Request timestamp'                   ].dt.time\ndf['Request hour'              ] = df['Request timestamp'                   ].dt.hour\ndf['Request dayofweek'                   ] = df['Request timestamp'                   ].dt.strftime('%A'    )\ndf['Drop time'           ] = df['Request timestamp'                   ].dt.time\ndf.head()",
  "generation": "df['Request time'] = df['Request timestamp'].dt.time\ndf['Request hour'] = df['Request timestamp'].dt.hour\ndf['Request dayofweek'] = df['Request timestamp'].dt.day_name()\ndf['Drop time'] = df['Drop timestamp'].dt.time\ndf[['Unnamed: 0', 'Request id', 'Pickup point', 'Driver id', 'Status', 'Request timestamp', 'Drop timestamp', 'Request time', 'Request hour', 'Request dayofweek', 'Drop time']]"
 },
 {
  "nbid": "NB_1962585",
  "file": "NB_1962585,df-1.ipynb",
  "row_id": 21,
  "target_code": "df['Supply gap'            ] = df['Status'        ].apply(supply_gap)\ndf.head()",
  "generation": "df['Supply gap'] = df.apply(lambda x: 'supply' if pd.isnull(x['Driver id']) else 'gap', axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_1969917",
  "file": "NB_1969917,dat1-1.ipynb",
  "row_id": 4,
  "target_code": "dat1[\"FECHA\"       ] = dat1[\"DATE\"      ] + \" \"    + dat1[\"TIME\"      ]\ndat1.drop( [\"DATE\"      ,\"TIME\"      ], axis=1, inplace=True )\ndat1.head()",
  "generation": "dat1['FECHA'] = pd.to_datetime(dat1['DATE'] + ' ' + dat1['TIME'])\ndat1 = dat1.drop(['DATE', 'TIME'], axis=1)\ndat1.head()"
 },
 {
  "nbid": "NB_1969917",
  "file": "NB_1969917,dat1-1.ipynb",
  "row_id": 5,
  "target_code": "dat1 = dat1.drop(dat1.index[0])\ndat1.reset_index(drop=True)\ndat1.head()",
  "generation": "dat1 = dat1.drop(dat1.index[0])\ndat1.head()"
 },
 {
  "nbid": "NB_1970893",
  "file": "NB_1970893,c_T-1.ipynb",
  "row_id": 24,
  "target_code": "c_T = c_T.rename(columns = labels_change)\nc_T.head()",
  "generation": "comb = comb.rename(columns=labels_change)\ncomb = comb.set_index('Margins').T.reset_index()\ncomb = comb.rename(columns={'index': 'Margins'})\ncomb"
 },
 {
  "nbid": "NB_1971517",
  "file": "NB_1971517,df-1.ipynb",
  "row_id": 17,
  "target_code": "df['Reason'        ] = df['title'       ].apply(lambda x: x.split(':'   )[0])\ndf['title'       ] = df['title'       ].apply(lambda x: x.split(':'   )[1])\ndf.columns = ['Injury'        , 'timeStamp'            , 'Reason'         ]\ndf.head()",
  "generation": "df[['Reason', 'Injury']] = df['title'].str.split(': ', expand=True)\ndf = df.drop(['title'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_327155",
  "file": "NB_327155,df_expanded-1.ipynb",
  "row_id": 27,
  "target_code": "df_expanded['index'       ] = df_expanded['index'       ].fillna(method='ffill'       )\ndf_expanded = df_expanded.set_index('index'       )\ndf_expanded.tail()",
  "generation": "last_idx = df2.index[-1] + 1\ndf_expanded = df2.reindex(range(last_idx))\ndf_expanded = df_expanded.apply(pd.to_numeric, errors='coerce')\ndf_expanded = df_expanded.fillna(method='ffill')\ndf_expanded = df_expanded.reset_index()\ndf_expanded = df_expanded.rename(columns={'index': 'index', 'Province_State': 'Province_State'})\ndf_expanded.head()"
 },
 {
  "nbid": "NB_327155",
  "file": "NB_327155,df_expanded-1.ipynb",
  "row_id": 32,
  "target_code": "df_expanded = df_expanded.interpolate()\ndf_expanded.tail()",
  "generation": "last_idx = df2.index[-1] + 1\ndf_expanded = df2.reindex(range(last_idx))\ndf_expanded = df_expanded.apply(pd.to_numeric, errors='coerce')\ndf_expanded = df_expanded.interpolate()\ndf_expanded.head()"
 },
 {
  "nbid": "NB_327155",
  "file": "NB_327155,df_rank_expanded,tb-1.ipynb",
  "row_id": 33,
  "target_code": "df_rank_expanded = df_rank_expanded.interpolate()\ndf_rank_expanded.tail()",
  "generation": "df_expanded = df_expanded.interpolate()\ndf_rank_expanded = df_expanded.rank(axis=1, method='first')\ndf_rank_expanded = df_rank_expanded.sub(0.5)\ndf_rank_expanded = df_rank_expanded.div(len(df_rank_expanded.columns))\ndf_rank_expanded = df_rank_expanded.mul(0.5)\ndf_rank_expanded = df_rank_expanded.add(df_expanded)\ndf_rank_expanded['colors'] = random_color(len(df_rank_expanded.index), 'rgba')\ndf_rank_expanded"
 },
 {
  "nbid": "NB_334943",
  "file": "NB_334943,car-1.ipynb",
  "row_id": 8,
  "target_code": "car['carcompany'            ] = car['CarName'         ].apply(lambda x: x.split(' '   )[0])\ncar.drop(['CarName'         ],axis=1,inplace=True)\ncar.rename(columns={'carcompany'            : 'CarName'          }, inplace=True)\ncar.drop(['car_ID'        ],axis=1,inplace=True)\ncar.reset_index()\ncar.head()",
  "generation": "car['CarName'] = car['CarName'].apply(lambda x: x.split(\" \")[0])\ncar = car.drop(['CarName', 'car_ID'], axis=1)\ncar = car.rename(columns={'carcompany': 'CompanyName'})"
 },
 {
  "nbid": "NB_5436384",
  "file": "NB_5436384,sonar,sonar_new_scale-1.ipynb",
  "row_id": 4,
  "target_code": "sonar = sonar.sample(frac = 1)\nsonar.head()",
  "generation": "scaler = StandardScaler()\nsonar.iloc[:, :-1] = scaler.fit_transform(sonar.iloc[:, :-1])\nsonar.head()"
 },
 {
  "nbid": "NB_5547432",
  "file": "NB_5547432,df,y,X_scaled-1.ipynb",
  "row_id": 8,
  "target_code": "dummies = pd.get_dummies(df['state'       ])\ndel df['state'       ]\ndf = pd.concat([df,dummies],axis=1)\ndf.head(5)",
  "generation": "df = pd.get_dummies(df, columns=['state'])\ndf.head()"
 },
 {
  "nbid": "NB_2000570",
  "file": "NB_2000570,days_freq-1.ipynb",
  "row_id": 62,
  "target_code": "days_freq = days_freq.sort_index()\ndays_freq",
  "generation": "days_freq = days_freq.rename(columns={0: 'Freq', 'Day of Week': 'Day'})\ndays_freq['Day'] = pd.Categorical(days_freq['Day'], categories=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], ordered=True)\ndays_freq = days_freq.sort_values('Day')\ndays_freq = days_freq.reset_index(drop=True)\ndays_freq"
 },
 {
  "nbid": "NB_2000592",
  "file": "NB_2000592,df,versicolor,virginica-1.ipynb",
  "row_id": 13,
  "target_code": "flowers=list(df['flower'        ].unique())\nnum=list(range(len('flowers'         )))\nprint(num)\nprint(flowers)\nmapping=dict(zip(flowers,num))\nprint(mapping)\ndf['flower_index'              ]=df[\"flower\"        ].apply(lambda x:mapping[x])\ndf.head()",
  "generation": "df['flower_index'] = df['flower'].apply(lambda x: {'Iris-setosa':0,'Iris-versicolor':1,'Iris-virginica':2}[x])\ndf.head()"
 },
 {
  "nbid": "NB_2004225",
  "file": "NB_2004225,stats-1.ipynb",
  "row_id": 28,
  "target_code": "stats['NewCol'        ] = [(1,2)]*len(stats)     \nstats\nstats['NewCol'        ] = [1]*len(stats)         \nstats.head()",
  "generation": "stats = stats.drop(columns=['myCalc']) \nstats = stats.drop(columns=['NewCol']) \nstats"
 },
 {
  "nbid": "NB_2004225",
  "file": "NB_2004225,stats-1.ipynb",
  "row_id": 29,
  "target_code": "stats.drop(['myCalc'        ],1)      \nprint(stats.head())\nstats = stats.drop(['myCalc'        ],1)\nprint(stats.head())\nstats.drop(['NewCol'        ],1,inplace=True)\nstats.head()",
  "generation": "stats.drop('NewCol', axis=1, inplace=True)\nstats.head()"
 },
 {
  "nbid": "NB_2004609",
  "file": "NB_2004609,df,test-1.ipynb",
  "row_id": 2,
  "target_code": "df['date'      ] = pd.to_datetime(df[['year'      , 'month'        , 'day'      ]])\ndf.head()",
  "generation": "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\ndf['year'] = df['year'].astype(int)\ndf['month'] = df['month'].astype(int)\ndf['day'] = df['day'].astype(int)\ndf = df[['year', 'month', 'day', 'decimal', 'ppm', '#days', '1 yr ago', '10 yr ago', 'since 1800', 'date']]\ndf.head()"
 },
 {
  "nbid": "NB_2007476",
  "file": "NB_2007476,ratings-1.ipynb",
  "row_id": 25,
  "target_code": "ratings['FreeGames'           ] = ratings['FreeGames'           ] / (len(free_games)) * 100\nratings['FreeandMicro'              ] = ratings['FreeandMicro'              ] / (len(free_and_microtransactions)) * 100\nratings['PaidGames'           ] = ratings['PaidGames'           ] / (len(paid_games)) * 100\nratings['PaidandMicro'              ] = ratings['PaidandMicro'              ] / (len(paid_and_microtransactions)) * 100\nratings = ratings[['AverageUserRating'                   ,'FreeGames'           ,'FreeandMicro'              ,'PaidGames'           ,'PaidandMicro'              ]]\nratings",
  "generation": "ratings['FreeGames'       ] = (ratings['FreeGames'       ] / len(free_games)                    ) * 100\nratings['FreeandMicro'   ] = (ratings['FreeandMicro'   ] / len(free_and_microtransactions)    ) * 100\nratings['PaidGames'      ] = (ratings['PaidGames'      ] / len(paid_games)                    ) * 100\nratings['PaidandMicro'   ] = (ratings['PaidandMicro'   ] / len(paid_and_microtransactions)    ) * 100\nratings = ratings.drop(columns=['TotalRatings'])"
 },
 {
  "nbid": "NB_2009972",
  "file": "NB_2009972,frequence-1.ipynb",
  "row_id": 64,
  "target_code": "frequence['frequence'           ]=frequence['session_id'            ]/12\nfrequence.head()",
  "generation": "frequence = sell.groupby(['client_id', 'age']).agg({'id_prod': 'nunique', 'session_id': 'nunique', 'price': 'sum'}).reset_index()\nfrequence['frequence'] = frequence['id_prod'] / frequence['session_id']\nfrequence = frequence.drop(['id_prod', 'session_id'], axis=1)\nfrequence.head()"
 },
 {
  "nbid": "NB_2019440",
  "file": "NB_2019440,df-1.ipynb",
  "row_id": 4,
  "target_code": "df.Price = pd.DataFrame({'Price'       : [x.strip('$'   ) for x in df.Price]})\ndf.Price\ndf.head()",
  "generation": "df['Price'] = df['Price'].str.replace('$','').str.replace(',','').astype(int)\ndf.head()"
 },
 {
  "nbid": "NB_2023086",
  "file": "NB_2023086,dfa-1.ipynb",
  "row_id": 90,
  "target_code": "dfa = dfs.copy()\ndfa.gender = dfa.gender.map({'Female'        : 0, 'Male'       : 1})\ndfa",
  "generation": "dfa['MonthlyCharges'] = (dfa['MonthlyCharges']-dfa['MonthlyCharges'].min())/(dfa['MonthlyCharges'].max()-dfa['MonthlyCharges'].min())*100\ndfa = dfa.astype({'MonthlyCharges': 'int32'})\ndfa.rename(columns={'MonthlyCharges': 'MonthlyCharges_scaled'}, inplace=True)\ndfa.head()"
 },
 {
  "nbid": "NB_2034029",
  "file": "NB_2034029,df-1.ipynb",
  "row_id": 10,
  "target_code": "df['age'     ] = (df['age'     ] / 365).round().astype('int'     )\ndf.head(5)",
  "generation": "df['age'] = (df['age'] / 365).round().astype('int')\ndf.head(5)"
 },
 {
  "nbid": "NB_2039459",
  "file": "NB_2039459,data_SVM-1.ipynb",
  "row_id": 67,
  "target_code": "import time as tm\nimport numpy as np \nimport datetime \ndata_SVM[\"time\"      ] = list(map(lambda x : int(str(x).split(':'   )[0]), data_SVM[\"time\"      ]))\ndata_SVM['hr_sin'        ] = list(map(lambda x: np.sin(x*(2.*np.pi/24)), data_SVM[\"time\"      ])) \ndata_SVM['hr_cos'        ] = list(map(lambda x: np.cos(x*(2.*np.pi/24)), data_SVM[\"time\"      ]))\ndel(data_SVM[\"time\"      ])\ndata_SVM.head(3)",
  "generation": "le = LabelEncoder()\ndata_SVM[\"population_density_categorical\"] = le.fit_transform(data_SVM[\"population_density_categorical\"])\ndata_SVM[\"hr_sin\"] = np.sin(data_SVM[\"time\"].apply(lambda x: x.hour*(2*np.pi/24)))\ndata_SVM[\"hr_cos\"] = np.cos(data_SVM[\"time\"].apply(lambda x: x.hour*(2*np.pi/24)))\ndata_SVM.drop(\"time\", axis=1, inplace=True)\ndata_SVM = pd.get_dummies(data_SVM, prefix='holiday_', columns=['holiday'])\ndata_SVM.head(3)"
 },
 {
  "nbid": "NB_2048197",
  "file": "NB_2048197,movies_df-1.ipynb",
  "row_id": 3,
  "target_code": "movies_df['plot'      ] = movies_df['wiki_plot'           ].astype(str) + \"\\n\"      +                movies_df['imdb_plot'           ].astype(str)\nmovies_df.head()",
  "generation": "movies_df['plot'                     ] = movies_df['wiki_plot'                  ] + movies_df['imdb_plot'                  ]\nmovies_df.head()"
 },
 {
  "nbid": "NB_2050997",
  "file": "NB_2050997,mdf,dfy-1.ipynb",
  "row_id": 22,
  "target_code": "mdf['\uc5ec\uc131'    ] = mdf['\uc8fc\uad6c\ub9e4\ucf54\ub108\ubcc4\uc5ec\uc131\ube44\uc728'            ].apply(lambda x: 5 if x < mdf['\uc8fc\uad6c\ub9e4\ucf54\ub108\ubcc4\uc5ec\uc131\ube44\uc728'            ].quantile(0.20)\n                                              else 4 if mdf['\uc8fc\uad6c\ub9e4\ucf54\ub108\ubcc4\uc5ec\uc131\ube44\uc728'            ].quantile(0.20) < x < mdf['\uc8fc\uad6c\ub9e4\ucf54\ub108\ubcc4\uc5ec\uc131\ube44\uc728'            ].quantile(0.40)\n                                              else 3 if mdf['\uc8fc\uad6c\ub9e4\ucf54\ub108\ubcc4\uc5ec\uc131\ube44\uc728'            ].quantile(0.40) < x < mdf['\uc8fc\uad6c\ub9e4\ucf54\ub108\ubcc4\uc5ec\uc131\ube44\uc728'            ].quantile(0.60)\n                                              else 2 if mdf['\uc8fc\uad6c\ub9e4\ucf54\ub108\ubcc4\uc5ec\uc131\ube44\uc728'            ].quantile(0.60) < x < mdf['\uc8fc\uad6c\ub9e4\ucf54\ub108\ubcc4\uc5ec\uc131\ube44\uc728'            ].quantile(0.80)\n                                              else 1)\nmdf.head()",
  "generation": "mdf['\uc5ec\uc131'] = mdf['gender']\nmdf['\uc5ec\uc131'] = mdf['\uc5ec\uc131'].apply(lambda x: 1 if x == 'F' else 0)\nmdf.head(5)"
 },
 {
  "nbid": "NB_2054868",
  "file": "NB_2054868,tmdb_data-1.ipynb",
  "row_id": 6,
  "target_code": "del_col = ['id'    , 'imdb_id'          , 'popularity'             , 'budget_adj'             , 'revenue_adj'              , 'homepage'           ,'keywords'          , \n           'overview'                     , 'production_companies'                       ,'vote_count'            , 'vote_average'               ]\ntmdb_data = tmdb_data.drop(del_col, 1)\ntmdb_data.head()",
  "generation": "tmdb_data.drop(['id', 'imdb_id', 'popularity', 'budget_adj', 'revenue_adj', 'homepage', 'keywords', 'overview', 'production_companies', 'vote_count', 'vote_average'], axis=1, inplace=True)\ntmdb_data.drop_duplicates(inplace=True)\ntmdb_data = tmdb_data[tmdb_data['budget'] != 0]\ntmdb_data = tmdb_data[tmdb_data['revenue'] != 0]\ntmdb_data['release_date'] = pd.to_datetime(tmdb_data['release_date'])\ntmdb_data['budget'] = tmdb_data['budget'].replace(0, np.nan)\ntmdb_data['revenue'] = tmdb_data['revenue'].replace(0, np.nan)\ntmdb_data['budget'] = tmdb_data['budget']/1000000\ntmdb_data['revenue'] = tmdb_data['revenue']/1000000\ntmdb_data['release_year'] = pd.DatetimeIndex(tmdb_data['release_date']).year\ntmdb_data = tmdb_data[['budget', 'revenue', 'original_title', 'cast', 'director', 'tagline', 'runtime', 'genres', 'release_date', 'release_year']]\ntmdb_data.head()"
 },
 {
  "nbid": "NB_2057120",
  "file": "NB_2057120,df,Y_r-1.ipynb",
  "row_id": 30,
  "target_code": "df = pd.get_dummies(df,columns=['Mjob'      ,'Fjob'      ,'reason'        ,'guardian'          ],drop_first=True)\ndf.head()",
  "generation": "df['traveltime'] = df['traveltime'].astype('category').cat.codes\ndf['studytime'] = df['studytime'].astype('category').cat.codes\ndf['Mjob'] = df['Mjob'].astype('category').cat.codes\ndf['Fjob'] = df['Fjob'].astype('category').cat.codes\ndf['reason'] = df['reason'].astype('category').cat.codes\ndf['guardian'] = df['guardian'].astype('category').cat.codes\ndf['school'] = df['school'].astype('category').cat.codes\ndf['sex'] = df['sex'].astype('category').cat.codes\ndf['address'] = df['address'].astype('category').cat.codes\ndf['famsize'] = df['famsize'].astype('category').cat.codes\ndf['Pstatus'] = df['Pstatus'].astype('category').cat.codes\ndf['Mjob_teacher'] = np.where(df['Mjob'] == 4, 1, 0)\ndf['Fjob_health'] = np.where(df['Fjob'] == 0, 1, 0)\ndf['Fjob_other'] = np.where(df['Fjob'] == 1, 1, 0)\ndf['Fjob_services'] = np.where(df['Fjob'] == 2, 1, 0)\ndf['Fjob_teacher'] = np.where(df['Fjob'] == 3, 1, 0)\ndf['reason_home'] = np.where(df['reason'] == 0, 1, 0)\ndf['reason_other'] = np.where(df['reason'] == 1, 1, 0)\ndf['reason_reputation'] = np.where(df['reason'] == 2, 1, 0)\ndf['guardian_mother'] = np.where(df['guardian'] == 0, 1, 0)\ndf['guardian_other'] = np.where(df['guardian'] == 1, 1, 0)\ndf = df.drop(['Mjob', 'Fjob', 'reason', 'guardian'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_2057382",
  "file": "NB_2057382,df_new-1.ipynb",
  "row_id": 105,
  "target_code": "df_new['Content Rating'                ]=df_new['Content Rating'                ].apply(count2)\ndf_new.head()",
  "generation": "df.drop(['App'], axis=1, inplace=True)\ndf.drop(['Genres'], axis=1, inplace=True)\ndf.drop(['Price'], axis=1, inplace=True)\ndf.drop(['Android Ver'], axis=1, inplace=True)\ndf.drop(['Size'], axis=1, inplace=True)\ndf.drop(['Content Rating'], axis=1, inplace=True)\ndf.drop(['Rating'], axis=1, inplace=True)\ndf.drop(['Reviews'], axis=1, inplace=True)\ndf.drop(['Installs'], axis=1, inplace=True)\ndf.drop(['Last Updated'], axis=1, inplace=True)\ndf_new.columns = ['Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Price', 'Content Rating', 'Last Updated', 'Android Ver', 'AppId']\ndf_new['Category'] = df_new['Category'].apply(count)\ndf_new.head()"
 },
 {
  "nbid": "NB_2060118",
  "file": "NB_2060118,movie_data-1.ipynb",
  "row_id": 10,
  "target_code": "del_col = [ 'id'     , 'imdb_id'          , 'budget_adj'             , 'revenue_adj'              , 'homepage'           , 'keywords'           , 'overview'           , 'production_companies'                       , 'vote_average'               ]\nmovie_data = movie_data.drop(del_col, 1)\nmovie_data.head(3)",
  "generation": "movie_data.drop(['imdb_id', 'homepage', 'tagline', 'overview', 'budget_adj', 'revenue_adj', 'production_companies', 'vote_average', 'vote_count'], axis=1, inplace=True)\nmovie_data.head()"
 },
 {
  "nbid": "NB_263660",
  "file": "NB_263660,genr_g,top10g,gen10y-1.ipynb",
  "row_id": 49,
  "target_code": "corr_dict = {\n    'Juvenile Fiction'                       : 'Fiction'          ,\n    'Young Adult Fiction'                         : 'Fiction'          \n}\ndata.genre = data.genre.apply(lambda g: corr_dict[g] if g in corr_dict.keys() else g)\ngenr = data.loc[:,['genre'       , 'holds'        , 'hold_year'            , 'branch_name'              ]]\ngenr_g = genr.groupby('genre'       ).agg(total_holds=('holds'       ,'sum'     )).sort_values('total_holds'             , ascending=False)\ntotal = genr_g.loc[:,'total_holds'             ].sum()\ngenr_g['%total'        ] = (genr_g['total_holds'             ]/total)*100\nprint('Top 10 genres % in total holdings:{}'                                      .format(genr_g['%total'        ].head(10).sum()))\ngenr_g.head(10)",
  "generation": "genr.loc[genr['genre'].str.contains('Fiction'), 'genre'] = 'Fiction'\ngenr_g = genr.groupby('genre').agg(total_holds=('holds','sum')).sort_values('total_holds', ascending=False)\n\n#getting total percentages\ntotal = genr_g.loc[:,'total_holds'].sum()\ngenr_g['%total'] = (genr_g['total_holds']/total)*100\n\n#taking a look at the top 10\ngenr_g.head(10)"
 },
 {
  "nbid": "NB_268252",
  "file": "NB_268252,df,ys,zs-1.ipynb",
  "row_id": 41,
  "target_code": "df[\"Elite\"       ] = pd.cut(df.Top10perc, bins=[0, 50, np.inf], labels=[False, True])\ndf.head(2)",
  "generation": "df['Private'] = df['Private'].map({'Yes': 1, 'No': 0})\ndf.head()"
 },
 {
  "nbid": "NB_269202",
  "file": "NB_269202,Q_13-1.ipynb",
  "row_id": 42,
  "target_code": "Q_13=Q_13.groupby(by=[\"Category\"          ])[[\"total_policy_claims\"                     ]].count().add_prefix(\"Total_\"        )\nQ_13",
  "generation": "Q_13 = Final_data.loc[Final_data.fraudulent == \"Yes\"]\nQ_13 = Q_13.groupby(\"Category\").agg({\"total_policy_claims\": \"sum\"})\nQ_13 = Q_13.reset_index()\nQ_13.columns = [\"Category\", \"Total_total_policy_claims\"]\nQ_13"
 },
 {
  "nbid": "NB_271893",
  "file": "NB_271893,df-1.ipynb",
  "row_id": 9,
  "target_code": "d={'male'      : 'MALE'       , 'female'         : 'FEMALE'         }\ndf['Sex'     ]=df['Sex'     ].map(d)\ndf.head()",
  "generation": "df['Sex'] = None\ndf.head()"
 },
 {
  "nbid": "NB_276931",
  "file": "NB_276931,plt_data-1.ipynb",
  "row_id": 11,
  "target_code": "plt_data = pd.melt(plt_data, id_vars='diagnosis'           , var_name='features'          )\nplt_data",
  "generation": "plt_data = pd.melt(X, id_vars=['diagnosis'], var_name='features', value_name='value')"
 },
 {
  "nbid": "NB_278585",
  "file": "NB_278585,df-1.ipynb",
  "row_id": 8,
  "target_code": "df.Good_Bad = (df.Good_Bad=='Good'      ).astype(int)\nprint('Dataset Balance = '                    ,df.Good_Bad.sum()/df.shape[0])\ndf.head(3)",
  "generation": "df = df.rename(columns={'NumberOfTime30-59DaysPastDueNotWorse': 'NumberOfTime30_59DaysPastDueNotWorse', 'MonthlyIncome.1': 'MonthlyIncome_1', 'NumberOfTime60-89DaysPastDueNotWorse': 'NumberOfTime60_89DaysPastDueNotWorse'})\ndf['Good_Bad'] = df['Good_Bad'].apply(lambda x: 0 if x == 'Bad' else 1)"
 },
 {
  "nbid": "NB_278937",
  "file": "NB_278937,train_df-1.ipynb",
  "row_id": 44,
  "target_code": "train_df=train_df.apply(lambda x: x.fillna('NA'    ) if x.dtypes == 'O'     else x.fillna(0))\ntrain_df.head()",
  "generation": "train_df_num_incl_price = train_df.select_dtypes(include=[np.number])\ntrain_df_num_incl_price.fillna(0, inplace=True)\ntrain_df_obj_incl_price = train_df.select_dtypes(include=['object'])\ntrain_df_obj_incl_price.fillna('NA', inplace=True)\ntrain_df = pd.concat([train_df_num_incl_price, train_df_obj_incl_price], axis=1)\ntrain_df.rename(columns={'Id': 'id', 'PID': 'pid', 'MS SubClass': 'ms_subclass', 'MS Zoning': 'ms_zoning', 'Lot Frontage': 'lot_frontage', 'Lot Area': 'lot_area', 'Street': 'street', 'Alley': 'alley', 'Lot Shape': 'lot_shape', 'Land Contour': 'land_contour', 'Screen Porch': 'screen_porch', 'Pool Area': 'pool_area', 'Pool QC': 'pool_qc', 'Fence': 'fence', 'Misc Feature': 'misc_feature', 'Misc Val': 'misc_val', 'Mo Sold': 'mo_sold', 'Yr Sold': 'yr_sold', 'Sale Type': 'sale_type', 'SalePrice': 'saleprice'}, inplace=True)\ntrain_df.head()"
 },
 {
  "nbid": "NB_342664",
  "file": "NB_342664,dfKS-1.ipynb",
  "row_id": 4,
  "target_code": "dfKS = pd.DataFrame(dfKS.groupby(['iSubj'       ,'myVars'        ,'Age'     ]).normVal2.agg('mean'      )).reset_index()\ndfKS",
  "generation": "dfKS = longTable[(longTable['myVars']=='medK') | (longTable['myVars']=='medSpeed')]\ndfKS = dfKS.groupby(['iSubj', 'myVars', 'Age']).agg({'value': 'mean', 'normVal2': 'mean'}).reset_index()\ndfKS = dfKS[['iSubj', 'myVars', 'Age', 'normVal2']]\ndfKS.head()"
 },
 {
  "nbid": "NB_5593197",
  "file": "NB_5593197,df,y,X_train,y_train,X_test,y_test-1.ipynb",
  "row_id": 3,
  "target_code": "cut_class_dict = {'Fair'      : 1, 'Good'       : 2, 'Very Good'            : 3, 'Premium'          : 4, 'Ideal'        : 5}\nclarity_dict = {'I3'    : 1, 'I2'     : 2, 'I1'     : 3, 'SI2'      : 4, 'SI1'      : 5, 'VS2'      : 6, 'VS1'      : 7, 'VVS2'       : 8, 'VVS1'       : 9, 'IF'     : 10, 'FL'     : 11}\ncolor_dict = {'J'   : 1, 'I'    : 2, 'H'    : 3, 'G'    : 4, 'F'    : 5, 'E'    : 6, 'D'    : 7}\ndf['cut'     ] = df['cut'     ].map(cut_class_dict)\ndf['clarity'         ] = df['clarity'         ].map(clarity_dict)\ndf['color'       ] = df['color'       ].map(color_dict)\ndf.head()",
  "generation": "df['cut'] = pd.factorize(df['cut'])[0]\ndf['color'] = pd.factorize(df['color'])[0]\ndf['clarity'] = pd.factorize(df['clarity'])[0]\ndf.head()"
 },
 {
  "nbid": "NB_5603557",
  "file": "NB_5603557,df10,SmallCable,SmallWireless-1.ipynb",
  "row_id": 6,
  "target_code": "df10 = df10.sort_values(\"Value\"       ,ascending= False)\ndf10",
  "generation": "df10.sort_values(by='Value', ascending=False)"
 },
 {
  "nbid": "NB_5623283",
  "file": "NB_5623283,df,y-1.ipynb",
  "row_id": 4,
  "target_code": "df=clean(df)\ndf.head()",
  "generation": "df['Age'] = df['age']\ndf['referred'] = df['recruitment_channel'].apply(lambda x: 1 if x == 'referred' else 0)\ndf['sourcing'] = df['recruitment_channel'].apply(lambda x: 1 if x == 'sourcing' else 0)\ndf = pd.concat([df.drop('department', axis=1), pd.get_dummies(df['department'])], axis=1)\ndf = pd.concat([df.drop('region', axis=1), pd.get_dummies(df['region'])], axis=1)\ndf = df.drop(['employee_id', 'education', 'recruitment_channel', 'age'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_5655388",
  "file": "NB_5655388,ratings,n_movie_ratings-1.ipynb",
  "row_id": 5,
  "target_code": "ratings = ratings.pivot_table(index='userId'        ,columns='movieId'         ,values='rating'        )\nprint(ratings.shape)\nratings.head()",
  "generation": "ratings = ratings.pivot_table(index='userId', columns='movieId', values='rating')\nratings.head()"
 },
 {
  "nbid": "NB_5659093",
  "file": "NB_5659093,df-1.ipynb",
  "row_id": 2,
  "target_code": "incr = df['high'      ].rolling(10).max().shift(-10) / df['close'       ] - 1\ndf['incr'      ] = incr\ndf['label'       ] = incr.apply(lambda x: 1 if x > 0.05 else 0)\ndf = df[['incr'      , 'label'        ]]\ndf",
  "generation": "df['incr'] = (df['close'] - df['open']) / df['open']\ndf['label'] = (df['incr'] > 0).astype(int)\ndf[['incr', 'label']]"
 },
 {
  "nbid": "NB_5698725",
  "file": "NB_5698725,tel_data-1.ipynb",
  "row_id": 16,
  "target_code": "tel_data['CC'    ] = tel_data.apply(cc, axis=1)\ntel_data['TV'    ] = tel_data.apply(tv, axis=1)\ntel_data['CC_TV'       ] = tel_data.apply(cc_tv, axis=1)\ntel_data['Ones'      ] = tel_data.apply(a1, axis=1)\ntel_data.head()",
  "generation": "tel_data['CC'] = 1\ntel_data['TV'] = tel_data['TBPP']\ntel_data['CC_TV'] = tel_data['CC']*tel_data['TV']\ntel_data['Ones'] = 1\ntel_data.head()"
 },
 {
  "nbid": "NB_5715976",
  "file": "NB_5715976,data,y-1.ipynb",
  "row_id": 6,
  "target_code": "from datetime import date, datetime, timedelta\ndata['DaysBack'          ] = (pd.to_datetime(data['ReadDate'          ]) - datetime.today()).dt.days\ndata.head()",
  "generation": "today = datetime.now().date()\ndata['ReadDate'] = pd.to_datetime(data['ReadDate'])\ndata['DaysBack'] = (today - data['ReadDate']).dt.days * -1\nresult = data[['Unnamed: 0', 'ReadDate', 'GBUsed', 'DaysBack']]"
 },
 {
  "nbid": "NB_1612531",
  "file": "NB_1612531,df-1.ipynb",
  "row_id": 2,
  "target_code": "CONTINUOUS_COLUMNS = ['raisedhands'             , 'VisITedResources'                   , 'AnnouncementsView'                    , 'Discussion'             ]\nDISCRETE_COLUMNS = ['StudentAbsenceDays'                    ]\nFEATURE_COLUMNS = CONTINUOUS_COLUMNS + DISCRETE_COLUMNS\nLABEL_COLUMN = 'Class'        \ndf = df[FEATURE_COLUMNS + [LABEL_COLUMN]]\nlabel_str2int = {'L'   : 0, 'M'    : 1, 'H'    : 2}\nlabel_int2str = ['L'   , 'M'    , 'H'    ]\ndf[LABEL_COLUMN] = df[LABEL_COLUMN].apply(lambda x: label_str2int[x])\ndf.head(1)",
  "generation": "df = df[['raisedhands', 'VisITedResources', 'AnnouncementsView', 'Discussion', 'StudentAbsenceDays', 'Class']]\ndf['StudentAbsenceDays'] = df['StudentAbsenceDays'].map({'Under-7': 1, 'Above-7': 0})\ndf['Class'] = df['Class'].map({'M': 0, 'L': 1, 'H': 2})\ndf.head(1)"
 },
 {
  "nbid": "NB_1615183",
  "file": "NB_1615183,df,principalDF-1.ipynb",
  "row_id": 17,
  "target_code": "df=df.dropna()\ndf=df.sample(frac=1, random_state=7896)\ndf.head()",
  "generation": "pca = PCA(n_components=3)\ndf_pca = pca.fit_transform(df)\ndf_pca = pd.DataFrame(df_pca, columns=['PCA1', 'PCA2', 'PCA3'])\ndf = pd.concat([df, df_pca], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_1618520",
  "file": "NB_1618520,attr-1.ipynb",
  "row_id": 8,
  "target_code": "attr=attr.replace({'male'      : 0, 'female'         : 1})\nattr.head(5)",
  "generation": "attr['Sex'] = attr['Sex'].replace(['male','female'],[0,1])\nattr = attr.fillna(attr.mean())\nattr.head()"
 },
 {
  "nbid": "NB_1620753",
  "file": "NB_1620753,decks-1.ipynb",
  "row_id": 49,
  "target_code": "decks = decks.dropna()\ndecks.head()",
  "generation": "decks = decks.dropna(subset=['Cabin'])\ndecks = decks[decks['Cabin'].str[0] != 'T']\ndecks.head()"
 },
 {
  "nbid": "NB_1622020",
  "file": "NB_1622020,media,p2,lns-1.ipynb",
  "row_id": 4,
  "target_code": "media = media.drop('Unnamed: 7'            ,axis = 1)\nmedia.head()",
  "generation": "media = media.drop('Unnamed: 7',axis = 1)\nmedia.head()"
 },
 {
  "nbid": "NB_1625042",
  "file": "NB_1625042,df-1.ipynb",
  "row_id": 5,
  "target_code": "df['age'     ] = df['age'     ] / 365\ndf.head()",
  "generation": "df['age'] = df['age']/365\ndf.head()"
 },
 {
  "nbid": "NB_1629701",
  "file": "NB_1629701,housing_new-1.ipynb",
  "row_id": 11,
  "target_code": "housing_new['bbratio'         ] = housing_new['bathrooms'           ] / housing_new['bedrooms'          ]\nhousing_new['areaperbedroom'                ] = housing_new['area'      ] / housing_new['bedrooms'          ]\nhousing_new.head()",
  "generation": "housing_new['bbratio'] = housing_new['bathrooms'] / housing_new['bedrooms']\nhousing_new['areaperbedroom'] = housing_new['area'] / housing_new['bedrooms']\nhousing_new.head()"
 },
 {
  "nbid": "NB_1648152",
  "file": "NB_1648152,titanic_data-1.ipynb",
  "row_id": 20,
  "target_code": "titanic_data = titanic_data.rename(columns={'Pclass'        :'Passenger_class'                 })\ntitanic_data = titanic_data.rename(columns={'Ticket'        :'Ticket_number'               })\ntitanic_data = titanic_data.rename(columns={'Embarked'          :'Ports'       })\ntitanic_data = titanic_data.rename(columns={'Survived'          :'Survival'          })\ntitanic_data.head()",
  "generation": "titanic_data = titanic_data.rename(columns={'Survived': 'Survival', 'Pclass': 'Passenger_class', 'Ticket': 'Ticket_number', 'Embarked': 'Ports'})\ntitanic_data.head()"
 },
 {
  "nbid": "NB_2064598",
  "file": "NB_2064598,samsung_ms_line_per,y-1.ipynb",
  "row_id": 23,
  "target_code": "samsung_ms_line_per = np.transpose(samsung_ms_line_per)\nsamsung_ms_line_per.head()",
  "generation": "samsung_ms_line_per.T"
 },
 {
  "nbid": "NB_2069563",
  "file": "NB_2069563,dados,y-1.ipynb",
  "row_id": 3,
  "target_code": "a_renomear = {\n    'mileage_per_year'                       : 'milhas_por_ano'                 ,\n    'model_year'                 : 'ano_do_modelo'                ,\n    'price'            :'preco'       ,\n    'sold'           : 'vendido'          \n}\ndados = dados.rename(columns=a_renomear)\ndados.head()",
  "generation": "dados = dados.rename(columns={'mileage_per_year': 'milhas_por_ano', 'model_year': 'ano_do_modelo', 'price': 'preco', 'sold': 'vendido'})\ndados.head()"
 },
 {
  "nbid": "NB_2069563",
  "file": "NB_2069563,dados,y-1.ipynb",
  "row_id": 5,
  "target_code": "a_trocar = {\n    'no'         : 0,\n    'yes'         : 1\n}\ndados.vendido = dados.vendido.map(a_trocar)\ndados.head()",
  "generation": "dados['vendido'] = np.where(dados['vendido'] == 'yes', 1, 0)\ndados.head()"
 },
 {
  "nbid": "NB_2069563",
  "file": "NB_2069563,dados,y-1.ipynb",
  "row_id": 9,
  "target_code": "dados['idade_do_modelo'                 ] = ano_atual - dados.ano_do_modelo\ndados.head()",
  "generation": "dados['idade_do_modelo'] = ano_atual - dados.ano_do_modelo\ndados.head()"
 },
 {
  "nbid": "NB_2073740",
  "file": "NB_2073740,data-1.ipynb",
  "row_id": 13,
  "target_code": "data['split_sec'           ] = data['split'       ].astype(np.int64) / 1E9\ndata['final_sec'           ] = data['final'       ].astype(np.int64) / 1E9\ndata.head()",
  "generation": "data['split_sec'] = pd.to_timedelta(data['split']).dt.total_seconds()\ndata['final_sec'] = pd.to_timedelta(data['final']).dt.total_seconds()\ndata.head(5)"
 },
 {
  "nbid": "NB_2079080",
  "file": "NB_2079080,data,y_axis,y1_axis-1.ipynb",
  "row_id": 12,
  "target_code": "data['CPI'     ]=data['Production'            ]/data['Area'      ]\ndata.head()",
  "generation": "data['CPI'] = data['Production'] / data['Area']\ndata.head()"
 },
 {
  "nbid": "NB_2079080",
  "file": "NB_2079080,data,y_axis,y1_axis-1.ipynb",
  "row_id": 15,
  "target_code": "data = data[np.isfinite(data['CPI'     ])]\ndata=data[data.CPI >43]\ndata=data[data.CPI <51]\ndata.set_index('Crop_Year'           )\ndata",
  "generation": "data = data[data['Crop_Year'].between(2000, 2004)]\ndata.head()"
 },
 {
  "nbid": "NB_2083754",
  "file": "NB_2083754,df-1.ipynb",
  "row_id": 16,
  "target_code": "df['Date'      ]=pd.to_datetime(df['Date'      ],format=\"%d/%m/%Y\"          )\ndf.head()",
  "generation": "There seems to be some mistake in the data context of output dataframe. Please provide the correct data context of output dataframe."
 },
 {
  "nbid": "NB_2084168",
  "file": "NB_2084168,grouped_pivot,row_labels,col_labels-1.ipynb",
  "row_id": 37,
  "target_code": "grouped_pivot= grouped_pivot.fillna(0)\ngrouped_pivot",
  "generation": "grouped_pivot.fillna(0, inplace=True)\ngrouped_pivot"
 },
 {
  "nbid": "NB_2084642",
  "file": "NB_2084642,final_dataset,top_corr_features-1.ipynb",
  "row_id": 14,
  "target_code": "final_dataset = pd.get_dummies(final_dataset, drop_first=True)\nfinal_dataset.head()",
  "generation": "final_dataset = pd.get_dummies(final_dataset, drop_first=True)\nfinal_dataset.head()"
 },
 {
  "nbid": "NB_2098835",
  "file": "NB_2098835,df-1.ipynb",
  "row_id": 8,
  "target_code": "df = df.drop(columns=['area'      , 'perimeter'            ], axis=1)\ndf.head(3)",
  "generation": "df = df.drop(['area', 'perimeter'], axis=1)\ndf.head(3)"
 },
 {
  "nbid": "NB_2102057",
  "file": "NB_2102057,df-1.ipynb",
  "row_id": 9,
  "target_code": "df = pd.concat([df,sex,embarked],axis=1)\ndf.head()",
  "generation": "df = pd.read_csv('titanic_train.csv')\nsex = pd.get_dummies(df['Sex'], drop_first=True)\nembarked = pd.get_dummies(df['Embarked'], drop_first=True)\ndf = pd.concat([df, sex, embarked], axis=1)\ndf.drop(['Sex', 'Embarked'], axis=1, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_2102674",
  "file": "NB_2102674,dfmushroom-1.ipynb",
  "row_id": 2,
  "target_code": "dfmushroom['class'       ] = dfmushroom['class'       ].apply(lambda x: 'poisonous'             if x == 'p'     else ('non-poisonous'                if x == 'e'     else np.NaN))\ndfmushroom.head()",
  "generation": "le = LabelEncoder()\nfor col in dfmushroom.columns:\n    dfmushroom[col] = le.fit_transform(dfmushroom[col])\ndfmushroom['class'] = dfmushroom['class'].replace(0, 'non-poisonous')\ndfmushroom['class'] = dfmushroom['class'].replace(1, 'poisonous')\ndfmushroom.head()"
 },
 {
  "nbid": "NB_219885",
  "file": "NB_219885,df,y-1.ipynb",
  "row_id": 3,
  "target_code": "df = df.drop(columns=['Unnamed: 0'            ,'ballot'        ])\ndf.head()",
  "generation": "pipeline = make_pipeline(XGBClassifier())\ndf.iloc[:,2:-5] = pipeline.fit_transform(df.iloc[:,2:-5])\ndf.head()"
 },
 {
  "nbid": "NB_223870",
  "file": "NB_223870,user_df-1.ipynb",
  "row_id": 10,
  "target_code": "user_df['adopted'         ] = user_df['object_id'           ].isin(adopted_users)\nuser_df.head()",
  "generation": "adopted_users = pd.read_csv('takehome_user_engagement.csv')\nadopted_users['time_stamp'] = pd.to_datetime(adopted_users['time_stamp'])\nadopted_users.set_index('time_stamp', inplace=True)\nadopted_users = adopted_users.groupby('user_id').rolling('7D').visited.count().reset_index()\nadopted_users = adopted_users[['user_id', 'visited']].drop_duplicates().set_index('user_id')\nadopted_users = adopted_users[adopted_users.visited >= 3]\nuser_df['adopted'] = user_df['object_id'].isin(adopted_users.index)\nuser_df['adopted'] = user_df['adopted'].astype(int)\nuser_df.head()"
 },
 {
  "nbid": "NB_225051",
  "file": "NB_225051,df-1.ipynb",
  "row_id": 17,
  "target_code": "df = df.astype(int)\ndf.head()",
  "generation": "df = df.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_286506",
  "file": "NB_286506,df-1.ipynb",
  "row_id": 3,
  "target_code": "df = df[pd.notnull(df['UG College'            ])]\ndf",
  "generation": "df = df.dropna(subset=['UG College'])"
 },
 {
  "nbid": "NB_286506",
  "file": "NB_286506,df-1.ipynb",
  "row_id": 4,
  "target_code": "df = df.fillna(0)\ndf",
  "generation": "le = preprocessing.LabelEncoder()\ndf['UG College'] = le.fit_transform(df['UG College'])\ndf.head()"
 },
 {
  "nbid": "NB_4672293",
  "file": "NB_4672293,df,y-1.ipynb",
  "row_id": 7,
  "target_code": "df['hps'     ]=standard(df['horsepower'            ])\ndf.head()",
  "generation": "df['hps'] = df['horsepower'].apply(lambda x: (x - df['horsepower'].mean()) / df['horsepower'].std())\ndf.head()"
 },
 {
  "nbid": "NB_4691793",
  "file": "NB_4691793,df-1.ipynb",
  "row_id": 17,
  "target_code": "df=df.drop(['tweet_tokenized'                 ,'tweet_nonstop'               ,'tweet_clean'             ,'tweet'       ], axis=1)\ndf.head()",
  "generation": "tfidf_vect = TfidfVectorizer(analyzer=lambda x: x)\nX_tfidf = tfidf_vect.fit_transform(df['tweet'])\ndf_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vect.get_feature_names())\ndf_tfidf = pd.concat([df['class'], df_tfidf], axis=1)\ndf_tfidf.head()"
 },
 {
  "nbid": "NB_4722928",
  "file": "NB_4722928,data-1.ipynb",
  "row_id": 6,
  "target_code": "data[\"is_red\"        ] = (data[\"color\"       ] == \"red\"      ).astype(int) \ndata.head()",
  "generation": "data['is_red'] = data['color'].apply(lambda x: 1 if x == 'red' else 0)\nnumeric_data = data.drop(['color', 'quality', 'high_quality'], axis = 1)\nnumeric_data.head()"
 },
 {
  "nbid": "NB_1652611",
  "file": "NB_1652611,df,p2,lns-1.ipynb",
  "row_id": 5,
  "target_code": "df[\"Date\"      ] = pd.to_datetime(df[\"Date\"      ]).dt.date\ndf.head()",
  "generation": "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')\ndf.head()"
 },
 {
  "nbid": "NB_1652611",
  "file": "NB_1652611,df,p2,lns-1.ipynb",
  "row_id": 7,
  "target_code": "df[\"day\"     ] = df[\"day\"     ].astype(str)\ndf[\"day\"     ] = df[\"day\"     ].map(lambda x: x[0:2])\ndf[\"day\"     ] = df[\"day\"     ].astype(int)\ndf.head()",
  "generation": "df['day'] = pd.to_datetime(df['Date']).dt.day\ndf.head()"
 },
 {
  "nbid": "NB_1652611",
  "file": "NB_1652611,df,p2,lns-1.ipynb",
  "row_id": 11,
  "target_code": "df[\"weekday\"         ] = (df[\"day\"     ] +3)%7\ndf.weekday.replace(0, 7, inplace= True)\ndf[\"weekday\"         ] = df[\"weekday\"         ].astype(int)\ndf.head()",
  "generation": "df[\"weekday\"] = pd.to_datetime(df[\"Date\"]).dt.weekday + 1\ndf.head()"
 },
 {
  "nbid": "NB_1652611",
  "file": "NB_1652611,df,p2,lns-1.ipynb",
  "row_id": 20,
  "target_code": "df[\"Lag_Views\"           ] = np.roll(df[\"Views_show\"            ], 1)\ndf.Lag_Views.replace(108961,0, inplace= True)\ndf.head()",
  "generation": "df['weekend'] = np.where(df['weekday'].isin([6,7]), 1, 0)\ndf['Lag_Views'] = df['Views_show'].shift(1).fillna(0)\ndf.head()"
 },
 {
  "nbid": "NB_1653036",
  "file": "NB_1653036,df_ger-1.ipynb",
  "row_id": 7,
  "target_code": "df_ger['conceded'          ] = df_ger.loc[df_ger['home_team'           ] == 'Germany'          ,'away_score'            ]\ndf_ger['conceded'          ] = df_ger['conceded'          ].fillna(df_ger.loc[df_ger['away_team'           ] == 'Germany'          ,'home_score'            ])\ndf_ger.tail()",
  "generation": "df_ger['conceded'] = df_ger.loc[df_ger['home_team'] == 'Germany','away_score']\ndf_ger['conceded'] = df_ger['conceded'].fillna(df_ger.loc[df_ger['away_team'] == 'Germany','home_score'])\ndf_ger.head()"
 },
 {
  "nbid": "NB_1653036",
  "file": "NB_1653036,df_ger-1.ipynb",
  "row_id": 8,
  "target_code": "df_ger['goaldiff'          ] = df_ger['goals'       ] - df_ger['conceded'          ]\ndf_ger.tail()",
  "generation": "df_ger['goaldiff'] = df_ger['goals'] - df_ger['conceded']\ndf_ger.head()"
 },
 {
  "nbid": "NB_1653036",
  "file": "NB_1653036,df_ger-1.ipynb",
  "row_id": 9,
  "target_code": "df_ger['friendly'          ] = df_ger['tournament'            ] == 'Friendly'           \ndf_ger.tail()",
  "generation": "df_ger['friendly'] = df_ger['tournament'].apply(lambda x: True if x == 'Friendly' else False)\ndf_ger.tail()"
 },
 {
  "nbid": "NB_1653036",
  "file": "NB_1653036,df_ger-1.ipynb",
  "row_id": 11,
  "target_code": "df_ger['opponent'          ] = df_ger['home_team'           ].loc[(df_ger['home_team'           ] != 'Germany'          )] \ndf_ger['opponent'          ] = df_ger['opponent'          ].fillna(df_ger['away_team'           ].loc[(df_ger['away_team'           ] != 'Germany'          )])\ndf_ger.tail()",
  "generation": "df_ger['home'] = df_ger.apply(lambda x: 'home' if x['home_team'] == 'Germany' else 'away', axis=1)\ndf_ger['opponent'] = df_ger.apply(lambda x: x['away_team'] if x['home_team'] == 'Germany' else x['home_team'], axis=1)\ndf_ger.head()"
 },
 {
  "nbid": "NB_1653036",
  "file": "NB_1653036,df_ger-1.ipynb",
  "row_id": 12,
  "target_code": "df_ger['year'      ] = df_ger['date'      ].str[:4]\ndf_ger.tail()",
  "generation": "df_ger['year'] = pd.DatetimeIndex(df_ger['date']).year\ndf_ger.tail()"
 },
 {
  "nbid": "NB_1653036",
  "file": "NB_1653036,df_ger-1.ipynb",
  "row_id": 13,
  "target_code": "df_ger = df_ger.drop(['home_score'            ,'away_score'            ,'tournament'            ,'neutral'         ,'home_team'           ,'away_team'           ], axis=1)\ndf_ger.tail()",
  "generation": "df_ger = df_ger.drop(['home_team', 'away_team', 'home_score', 'away_score', 'tournament', 'neutral'], axis=1)\ndf_ger.head()"
 },
 {
  "nbid": "NB_1658894",
  "file": "NB_1658894,user_amount,counts_total-1.ipynb",
  "row_id": 33,
  "target_code": "amount_total = user_amount.amount_cumsum.max()\nuser_amount['prop'      ] = user_amount.apply(lambda x: x.amount_cumsum / amount_total, axis = 1)\nuser_amount.tail()",
  "generation": "user_amount['prop'] = user_amount['amount_cumsum'] / user_amount['order_amount'].sum()\nuser_amount.tail()"
 },
 {
  "nbid": "NB_1664151",
  "file": "NB_1664151,df,y-1.ipynb",
  "row_id": 28,
  "target_code": "df['Dummy'       ] = np.where(df['Equpt'       ]=='DRY'     , df['Dist'      ].astype(float), 0)\ndf.head()",
  "generation": "df = pd.get_dummies(df, columns=['Equpt'], prefix=['Dummy'])\ndf.head()"
 },
 {
  "nbid": "NB_1664666",
  "file": "NB_1664666,df-1.ipynb",
  "row_id": 12,
  "target_code": "df[\"areaperbedroom\"                ] = df[\"area\"      ]/df[\"bedrooms\"          ]\ndf[\"bbratio\"         ] = df[\"bathrooms\"           ]/df[\"bedrooms\"          ]\ndf.head()",
  "generation": "df['areaperbedroom'] = df['area']/df['bedrooms']\ndf['bbratio'] = df['bathrooms']/df['bedrooms']\ndf = df.drop('furnishingstatus', axis = 1)\ndf.head()"
 },
 {
  "nbid": "NB_1670669",
  "file": "NB_1670669,df-1.ipynb",
  "row_id": 13,
  "target_code": "df = df.dropna()\ndf.head()",
  "generation": "df = df.dropna(subset=['birth_city'])\ndf = df[df['height'] >= 198]\ndf = df.sort_values(by=['height'], ascending=False)\ndf"
 },
 {
  "nbid": "NB_1672167",
  "file": "NB_1672167,dataset-1.ipynb",
  "row_id": 9,
  "target_code": "feature_variables_to_consideration_train = set(dataset.columns.tolist()) - set(null_variables)\nfeature_variables_to_consideration_test = set(test.columns.tolist()) - set(null_variables)\ndataset = dataset[list(feature_variables_to_consideration_train)]\ntest = test[list(feature_variables_to_consideration_test)]\ndataset.head(10)",
  "generation": "dataset.drop(null_variables, axis=1, inplace=True)"
 },
 {
  "nbid": "NB_1672809",
  "file": "NB_1672809,data-1.ipynb",
  "row_id": 2,
  "target_code": "n_sample = 200\nassert(n_sample<n)\nfrom random import shuffle\nkeep = [True] * n_sample + [False] * (len(data) - n_sample)\nshuffle(keep)\ndata = data[keep]\ndata.head()",
  "generation": "n_sample = 100\nsample_indices = list(range(n))\nshuffle(sample_indices)\nsample_indices = sample_indices[:n_sample]\nsample_data = data.iloc[sample_indices]\nsample_data.head()"
 },
 {
  "nbid": "NB_1680244",
  "file": "NB_1680244,df-1.ipynb",
  "row_id": 37,
  "target_code": "df = sns.load_dataset('iris'      )\ndf = df[df.species.isin(['setosa'        , 'versicolor'             ])]\ndf.head()",
  "generation": "There seems to be an error in the provided data context of the output dataframe. It is not related to the input dataframe and the code provided. Please provide the correct data context of the output dataframe."
 },
 {
  "nbid": "NB_1681550",
  "file": "NB_1681550,df_s,df_type-1.ipynb",
  "row_id": 36,
  "target_code": "df_s['Item_Fat_Content'                  ] = df_s.apply(func1, axis = 1)\ndf_s.head(30)",
  "generation": "def func(row):\n    if row[\"Item_Fat_Content\"] == \"Low Fat\":\n        return row[\"Item_Type\"]\n    elif row[\"Item_Fat_Content\"] == \"Regular\":\n        if row[\"Item_Type\"] == \"Snack Foods\":\n            return \"Invalid\"\n        else:\n            return row[\"Item_Type\"]\n        \ndf_s['Item_Combined_Type'] = df_s.apply(func, axis = 1)\ndf_s.head()"
 },
 {
  "nbid": "NB_1694661",
  "file": "NB_1694661,r-1.ipynb",
  "row_id": 47,
  "target_code": "periods=train['PERIOD'        ].unique()\nfor period in periods:\n    mask = ( train.PERIOD == period )\n    trx = train.loc[mask]\n    trx_pr_sum = trx.pivot_table(\n                        index='cl_id'       , \n                        columns='trx_category'              , \n                        values='amount'        , \n                        aggfunc=sum).fillna(0).applymap(float)\n    trx_pr_sum.reset_index(inplace=True)\n    coln=trx_pr_sum.columns\n    col2=[]\n    col2.append('cl_id'       )\n    for c in coln:\n        if c!='cl_id'       :\n            c2=c+ '_'     +period\n            col2.append(c2)\n    trx_pr_sum.columns=col2\n    r=r.merge( trx_pr_sum, how = 'left'       , left_on = 'cl_id'        , right_on = 'cl_id'         )\n    for co in col2:\n        r[co].fillna( 0, inplace = True )\nr.head()",
  "generation": ""
 },
 {
  "nbid": "NB_1694661",
  "file": "NB_1694661,r-1.ipynb",
  "row_id": 71,
  "target_code": "r=r.merge( cht2, how = 'inner'        , left_on = 'cl_id'        , right_on = 'cl_id'         )\nr.head()\nr_test=r_test.merge( cht2_test, how = 'inner'        , left_on = 'cl_id'        , right_on = 'cl_id'         )\nr_test.head()",
  "generation": ""
 },
 {
  "nbid": "NB_1694661",
  "file": "NB_1694661,r-1.ipynb",
  "row_id": 75,
  "target_code": "r=r.merge( cur2, how = 'inner'        , left_on = 'cl_id'        , right_on = 'cl_id'         )\nr.head()\nr_test=r_test.merge( cur2_test, how = 'inner'        , left_on = 'cl_id'        , right_on = 'cl_id'         )\nr_test.head()",
  "generation": ""
 },
 {
  "nbid": "NB_1698575",
  "file": "NB_1698575,df,avg_per-1.ipynb",
  "row_id": 20,
  "target_code": "df = df[['Player'        ,'Salary'        ,'Pos'     , 'PER'      , 'TS%'      , '3PAr'       , 'FTr'      , 'ORB%'       , 'DRB%'       , 'AST%'       , 'STL%'       , 'BLK%'       , 'TOV%'       , 'OWS'      , 'DWS'      , 'WS'     , 'VORP'       ]]\ndf",
  "generation": "df['Salary'] = df['Salary'].replace('[\\\\$,]', '', regex=True).astype(float)\ndf = df[['Player', 'Salary', 'Pos', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'OWS', 'DWS', 'WS', 'VORP']]\ndf.head()"
 },
 {
  "nbid": "NB_1699808",
  "file": "NB_1699808,df,mean-1.ipynb",
  "row_id": 37,
  "target_code": "df = df.replace(-99999, df[\"temperature\"             ].max())\ndf",
  "generation": "df.reset_index(inplace=True)\ndf['day'] = pd.to_datetime(df['day'])\ndf.head()"
 },
 {
  "nbid": "NB_1699808",
  "file": "NB_1699808,df,mean-1.ipynb",
  "row_id": 38,
  "target_code": "df = df.replace({\n    \"temperature\"                 : -99999,\n    \"windspeed\"                : -99999,\n    \"event\"            : \"0\"    \n}, np.nan)\ndf",
  "generation": "df.replace(-99999, np.nan, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_1700751",
  "file": "NB_1700751,stock,y_test,date_test-1.ipynb",
  "row_id": 23,
  "target_code": "stock = stock.drop(['TICKER'        ],1)\nstock.head()",
  "generation": "stock.drop(['TICKER'],1,inplace=True)\nstock.head()"
 },
 {
  "nbid": "NB_1703070",
  "file": "NB_1703070,df-1.ipynb",
  "row_id": 41,
  "target_code": "df = df.drop(['provinceEnglishName'                     ,'cityName'          ,'province_zipCode'                  ,'cityEnglishName'                 ,'city_zipCode'              ,'city_confirmedCount'                     ,'city_suspectedCount'                     ,'city_curedCount'                 ,'city_deadCount'                ], axis=1)\ndf",
  "generation": "data = { 'Month': [1, 3, 5, 7],\n        'Year': [2009, 2012, 2015, 2019],\n        'Sales': [132, 188, 159, 137]\n}\ndf = pd.DataFrame(data)\ndf[\"provinceName\"           ] = \"\u5409\u6797\u7701\"\ndf[\"province_confirmedCount\"] = 93\ndf[\"province_suspectedCount\"] = 0\ndf[\"province_curedCount\"    ] = 67\ndf[\"province_deadCount\"     ] = 1\ndf[\"updateTime\"             ] = \"2020-02-27 20:42:21.431\"\ndf.head()"
 },
 {
  "nbid": "NB_1704438",
  "file": "NB_1704438,inp0-1.ipynb",
  "row_id": 24,
  "target_code": "inp0['job'     ]=  inp0.jobedu.str.split(','   , expand=True)[0]\ninp0.head()",
  "generation": "inp0['job'] = inp0['jobedu'].apply(lambda x: x.split(',')[0])\ninp0.head()"
 },
 {
  "nbid": "NB_1704438",
  "file": "NB_1704438,inp0-1.ipynb",
  "row_id": 25,
  "target_code": "inp0['education'           ]= inp0.jobedu.str.split(','   , expand=True)[1]\ninp0.head()",
  "generation": "inp0['education']=  inp0.jobedu.str.split(',', expand=True)[1]\ninp0.head()"
 },
 {
  "nbid": "NB_1711324",
  "file": "NB_1711324,df-1.ipynb",
  "row_id": 7,
  "target_code": "Month_map = {'Jan'     :1, 'Feb'      :2, 'Mar'      :3, 'Apr'      :4, 'May'      :5, 'June'       :6, 'Jul'      :7, 'Aug'      :8, 'Sep'      :9, 'Oct'      :10, 'Nov'      :11, 'Dec'      :12}\nVisitor_map = {'Returning_Visitor'                   :0,'New_Visitor'             :1,'Other'       :2}\nWeekend_map = {False:0,True:1}\nRevenue_map = {False:0,True:1}\ndf['Month'       ] = df['Month'       ].map(Month_map)\ndf['VisitorType'             ] = df['VisitorType'             ].map(Visitor_map)\ndf['Weekend'         ] = df['Weekend'         ].map(Weekend_map)\ndf['Revenue'         ] = df['Revenue'         ].map(Revenue_map)\ndf.head()",
  "generation": "df = df.replace({'Month': {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'June': 6, 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}})\ndf['VisitorType'] = df['VisitorType'].replace({'Returning_Visitor': 1, 'New_Visitor': 0, 'Other': 0})\ndf['Weekend'] = df['Weekend'].replace({True: 1, False: 0})\ndf['Revenue'] = df['Revenue'].replace({True: 1, False: 0})\ndf.head()"
 },
 {
  "nbid": "NB_1718532",
  "file": "NB_1718532,df_actual-1.ipynb",
  "row_id": 9,
  "target_code": "df_actual = df_actual.drop([\"index\"       ], axis=1)\ndf_actual",
  "generation": "df_actual = df_actual.drop([\"index\"], axis=1)\ndf_actual.head()"
 },
 {
  "nbid": "NB_1718532",
  "file": "NB_1718532,df_per_dep-1.ipynb",
  "row_id": 15,
  "target_code": "df_per_dep = df_per_dep.reset_index()\ndf_per_dep = df_per_dep.drop([\"index\"       ], axis=1)\ndf_per_dep",
  "generation": "df_2014 = df[df[\"YEAR\"] == 2014]\ndf_2014 = df_2014.drop([\"YEAR\", \"Unnamed: 0\"], axis=1) \ndf_per_dep = df_2014\ndf_per_dep.head()"
 },
 {
  "nbid": "NB_1722812",
  "file": "NB_1722812,df-1.ipynb",
  "row_id": 29,
  "target_code": "df = df.drop(['URL'     , 'ID'     , 'Icon URL'           ], axis=1)\ndf.head()",
  "generation": "from sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\nexpandedLabels = mlb.fit_transform(filtered_data['genres'].str.split(', '))\nfiltered_data = filtered_data.drop(columns=['id', 'url', 'icon_url', 'name', 'description', 'developer', 'languages', 'primary_genre', 'genres', 'original_release_date', 'current_version_release_date'])\ndf = pd.concat([filtered_data.reset_index(drop=True),pd.DataFrame(expandedLabels, columns=mlb.classes_).reset_index(drop=True)], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_1725277",
  "file": "NB_1725277,df-1.ipynb",
  "row_id": 2,
  "target_code": "df['Ndate'       ] = pd.to_datetime(df['DateTime'          ], format='%m %d %Y %H:%M:%S'                   )\ndf['month'       ] = df['Ndate'       ].dt.month\ndf.head()",
  "generation": "df['Ndate'] = pd.to_datetime(df['DateTime'], format='%d %m %Y %H:%M:%S')\ndf['month'] = df['Ndate'].dt.month\ndf.head()"
 },
 {
  "nbid": "NB_1725277",
  "file": "NB_1725277,dg-1.ipynb",
  "row_id": 5,
  "target_code": "dg['Ndate'       ] = pd.to_datetime(dg['DateTime'          ], format='%m %d %Y %H:%M:%S'                   )\ndg['month'       ] = dg['Ndate'       ].dt.month\ndg.head()",
  "generation": "dg['DateTime'] = pd.to_datetime(dg['DateTime'], format='%d %m %Y %H:%M:%S')\ndg['Ndate'] = dg['DateTime'].dt.date\ndg['month'] = dg['DateTime'].dt.month\ndg.head()"
 },
 {
  "nbid": "NB_1726390",
  "file": "NB_1726390,ds1-1.ipynb",
  "row_id": 11,
  "target_code": "ds1 = ds1.drop(\"Date\"      ,axis = 1)\nds1",
  "generation": "scaler = MinMaxScaler()\nds1[cols] = scaler.fit_transform(ds1[cols])\nds1.head()"
 },
 {
  "nbid": "NB_1728167",
  "file": "NB_1728167,df-1.ipynb",
  "row_id": 11,
  "target_code": "df = pd.get_dummies(df, prefix='label'       , drop_first=True)\ndf.head()",
  "generation": "df['label_M'] = (df['label'] == 'M').astype(float)\ndf.head()"
 },
 {
  "nbid": "NB_1731155",
  "file": "NB_1731155,d-1.ipynb",
  "row_id": 3,
  "target_code": "labels=['date'      ,'year'      ,'geoId'       ,'countryterritoryCode'                      ,'popData2018'             ]\nd=d.drop(labels,axis=1)\nd.head()",
  "generation": "d = d[['day', 'month', 'cases', 'deaths', 'countries']]\nd.head()"
 },
 {
  "nbid": "NB_1735560",
  "file": "NB_1735560,df-1.ipynb",
  "row_id": 41,
  "target_code": "df = df.drop(labels=['sectfund,unit,geo\\\\time'                         ], axis=1)\ndf.head()",
  "generation": "cols = 'sectfund,unit,geo\\\\time'.split(',')\ndf[cols] = df['sectfund,unit,geo\\\\time'].str.split(',', expand=True)\ndf = df.drop('sectfund,unit,geo\\\\time', axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_1735560",
  "file": "NB_1735560,df-1.ipynb",
  "row_id": 43,
  "target_code": "import numpy as np\ndf = df.replace(': '    ,np.nan)\ndf.head()",
  "generation": "cols = 'sectfund,unit,geo\\\\time'.split(',')\ndf[cols] = df['sectfund,unit,geo\\\\time'].str.split(',', expand=True)\ndf[cols] = df[cols].replace(':', np.nan)\ndf[cols[:-1]] = df[cols[:-1]].apply(lambda x: x.str.extract('(\\d+\\.?\\d?)', expand=False))\ndf.head()"
 },
 {
  "nbid": "NB_1735560",
  "file": "NB_1735560,df-1.ipynb",
  "row_id": 48,
  "target_code": "df = df.drop(labels = ['unit'      ],axis=1)\ndf.head()",
  "generation": "cols = 'sectfund,unit,geo\\\\time'.split(',')\ndf[cols] = df['sectfund,unit,geo\\\\time'].str.split(',', expand=True)\ndf = df.drop(labels=['sectfund,unit,geo\\\\time'], axis=1)\ndf = df.replace(': ',np.nan)\ndf[data_columns[:-3]]=df[data_columns[:-3]].replace(r'[^0-9\\.]','', regex=True)\ndf[data_columns[:-3]]=df[data_columns[:-3]].replace(r'',np.nan, regex=True)\ndf[data_columns[:-3]]=df[data_columns[:-3]].astype(np.float)\ndf = df.drop(labels=['unit'], axis=1)"
 },
 {
  "nbid": "NB_1739009",
  "file": "NB_1739009,data_maths-1.ipynb",
  "row_id": 6,
  "target_code": "toMapfeatres= ['schoolsup'            , 'famsup'         , 'paid'        , 'activities'             , 'nursery'          , 'higher'          , 'internet'           , 'romantic'           ]\nfor i in range(len(toMapfeatres)):\n    data_maths[toMapfeatres[i]]= data_maths[toMapfeatres[i]].map({'yes'     : 1, 'no'     : 0})\ndata_maths['Pstatus'         ]= data_maths['Pstatus'         ].map({'T'   : 1, 'A'    : 0})\ndata_maths['guardian'          ]= data_maths['guardian'          ].map({\"father\"        :0, \"mother\"         :1, \"other\"        :2})\ndata_maths.head(10)",
  "generation": "data_maths = data_maths.drop(['school', 'sex', 'age', 'address', 'famsize', 'Mjob', 'Fjob', 'reason', 'guardian'], axis=1)\ndata_maths"
 },
 {
  "nbid": "NB_228005",
  "file": "NB_228005,confirmed-1.ipynb",
  "row_id": 37,
  "target_code": "confirmed['ds'    ] = pd.to_datetime(confirmed['ds'    ])\nconfirmed.tail()",
  "generation": "# Combine the dataframes into a single dataframe\ndf = pd.concat([confirmed['Date'], confirmed['Confirmed']], axis=1)\ndf.columns = ['ds', 'y']\n\n# Create a Prophet model and fit the data\nm = Prophet()\nm.fit(df)\n\n# Generate future dates to forecast\nfuture = m.make_future_dataframe(periods=5)\n\n# Use the model to make predictions for the future dates\nforecast = m.predict(future)\n\n# Extract the relevant columns from the forecast dataframe\noutput = forecast[['ds', 'yhat']].tail(5)\n\n# Rename the columns to match the output dataframe\noutput.columns = ['ds', 'y']"
 },
 {
  "nbid": "NB_231213",
  "file": "NB_231213,df_nima,df_nima_stds-1.ipynb",
  "row_id": 17,
  "target_code": "stds = [[slope1,intercept1],[slope2,intercept2],[slope3,intercept3],[slope4,intercept4],[slope5,intercept5],[slope6,intercept6],[slope6_5,intercept6_5], [slope7,intercept7],[slope7_5,intercept7_5]]\ndays = [1,2,3,4,5,6,6.5,7,7.5]\nframes = []\ni = 0\nfor x in days:\n    df = df_nima[df_nima.Day == x]\n    slope = stds[i][0]\n    intercept = stds[i][1]\n    df['mCherry_corrected'                   ] = (df[\"mCherry\"         ]-(df[\"OD\"    ]*slope + intercept))\n    frames.append(df)\n    i+=1\ndf_nima = pd.concat(frames)\ndf_nima.loc[df_nima[\"mCherry_corrected\"                   ] <0,\"mCherry_corrected\"                   ] = 0\ndf_nima['Norm_mCherry'              ] = (df_nima[\"mCherry_corrected\"                   ])/df_nima[\"OD\"    ]\ndf_nima.head()",
  "generation": "df_nima[\"mCherry_corrected\"] = df_nima[\"mCherry\"] - (slope1 * df_nima[\"OD\"] + intercept1)\ndf_nima[\"mCherry_corrected\"] = df_nima[\"mCherry_corrected\"].apply(lambda x: 0 if x < 0 else x)\ndf_nima[\"Norm_mCherry\"] = df_nima[\"mCherry_corrected\"] / df_nima[\"OD\"]"
 },
 {
  "nbid": "NB_233566",
  "file": "NB_233566,data,df_brooklyn,df_queens,df_staten_island,df_bronx-1.ipynb",
  "row_id": 11,
  "target_code": "data = data.drop(['id'    , 'host_id'          , 'host_name'            ], axis=1)\ndata = data.dropna(subset=['name'      ])\ndata['last_review'             ] = pd.to_datetime(data['last_review'             ])\ntoday = pd.to_datetime('today'       )\ndata['days_since_last_review'                        ] = today - data['last_review'             ]\ndata['days_since_last_review'                        ] = data['days_since_last_review'                        ].dt.days\ndata.drop(['last_review'             ], axis=1, inplace=True)\ndata.head()",
  "generation": "data = data.drop(['id', 'host_id', 'last_review'], axis=1)\ndata = data.dropna(subset=['name'])\ndata['days_since_last_review'] = (pd.to_datetime('2019-07-08') - pd.to_datetime(data['reviews_per_month'].index)) / np.timedelta64(1, 'D')\ndata = data.drop(['reviews_per_month'], axis=1)\ndata = pd.get_dummies(data, columns=['neighbourhood_group', 'room_type'], prefix=['ng', 'rt'])\ndata"
 },
 {
  "nbid": "NB_235226",
  "file": "NB_235226,data-1.ipynb",
  "row_id": 2,
  "target_code": "data = data.drop(\"CUSTOMER\"          ,1) \ndata.head()",
  "generation": "data = data.drop(columns=['CUSTOMER'])\ndata['FASH-MWAL'] = data[['FASH-WWAL', 'FASH-MIS']].sum(axis=1)\ndata = data.drop(columns=['FASH-WWAL', 'FASH-MIS'])\ndata.head()"
 },
 {
  "nbid": "NB_238837",
  "file": "NB_238837,project_error2-1.ipynb",
  "row_id": 70,
  "target_code": "project_error2=project_error2[project_error2['AverageForecast'                 ].notnull()]\nproject_error2",
  "generation": "project_error2 = pd.merge(project_error, diff_year, left_index=True, right_index=True)\nyear_produced = flatdata_adjusted['YearForecastProduced'].groupby(flatdata_adjusted['InternalProjectID']).mean().to_frame(name='YearForecastProduced')\nyear_open = flatdata_adjusted['ForecastYear'].groupby(flatdata_adjusted['InternalProjectID']).mean().to_frame(name='ForecastYear')\narea_type = flatdata_adjusted['Area'].groupby(flatdata_adjusted['InternalProjectID']).mean().to_frame(name='Area_Type')\nproject_error2 = project_error2.assign(UnifiedImprovementType=project_error2.apply(UnifiedImprovement, axis=1))\nproject_error2 = pd.merge(project_error2, year_produced, left_index=True, right_index=True)\nproject_error2 = pd.merge(project_error2, year_open, left_index=True, right_index=True)\nproject_error2 = pd.merge(project_error2, area_type, left_index=True, right_index=True)\nproject_error2 = project_error2.drop(labels='ImprovementType', axis=1)\nproject_error2 = project_error2.rename(columns={'Unnamed: 0_level_0': 'Unnamed: 0', 'UnifiedImprovementType': 'UnifiedImprovementType', 'YearForecastProduced': 'YearForecastProduced', 'ForecastYear': 'ForecastYear', 'Area_Type': 'Area_Type'}) \nproject_error2"
 },
 {
  "nbid": "NB_4751976",
  "file": "NB_4751976,testingdata-1.ipynb",
  "row_id": 16,
  "target_code": "prospects = testingdata[\"name\"      ]\ntestingdata = testingdata.drop([\"name\"      ,\"college\"         , \"position\"           ],1)\ntestingdata.head()",
  "generation": "testingdata = testingdata.drop(columns=['name', 'college', 'position']) \ntestingdata.head()"
 },
 {
  "nbid": "NB_4769182",
  "file": "NB_4769182,df,y-1.ipynb",
  "row_id": 6,
  "target_code": "df = df.drop(['Region'        ], axis=1)\ndf.head()",
  "generation": "df = df.drop(['Region'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_4771397",
  "file": "NB_4771397,results,y_pred-1.ipynb",
  "row_id": 5,
  "target_code": "results['predict'         ] = list(map(lambda x: targets_dict[x], results['preds'       ]))\nresults.head(10)",
  "generation": "results[\"predict\"] = 1\nresults.head()"
 },
 {
  "nbid": "NB_4786010",
  "file": "NB_4786010,df,df2,df3-1.ipynb",
  "row_id": 7,
  "target_code": "scaler = MinMaxScaler()\nscaler.fit(df[['Income($)'           ]])\ndf['Income($)'           ] = scaler.transform(df[['Income($)'           ]])\nscaler.fit(df[['Age'     ]])\ndf.Age = scaler.transform(df[['Age'     ]])\ndf.head()",
  "generation": "scaler = MinMaxScaler()\ndf[['Age','Income($)']] = scaler.fit_transform(df[['Age','Income($)']])\nkm.fit(df[['Age','Income($)']])\ndf['Clusters'] = km.predict(df[['Age','Income($)']])\ndf.head()"
 },
 {
  "nbid": "NB_4791886",
  "file": "NB_4791886,df_val,x_test,validation-1.ipynb",
  "row_id": 18,
  "target_code": "df_val.drop(['Unnamed: 0'            ], axis = 1, inplace=True)\ndf_val['y'   ] = df_val['y'   ].map({'yes'     :1, 'no'     :0})\ndf_val.head()",
  "generation": "df_val['y'] = df_val['y'].apply(lambda x: 0 if x == 'no' else 1)\ndf_val.head()"
 },
 {
  "nbid": "NB_4814992",
  "file": "NB_4814992,data-1.ipynb",
  "row_id": 3,
  "target_code": "data['GDP_pc'        ] = data[['GDP'     ]].div(data.Pop, axis=0)\ndata['Prod_pc'         ] = data[['Steel'       ]].div(data.Pop, axis=0)\ndata.head()",
  "generation": "data['GDP_pc'] = data['GDP'] / data['Pop']\ndata['Prod_pc'] = data['Steel'] / data['Pop'] * 1000\ndata.head()"
 },
 {
  "nbid": "NB_4821813",
  "file": "NB_4821813,ks_data,ks_rdd,ks_test_df,ks_test_rdd,ks_lrm_pred_result-1.ipynb",
  "row_id": 2,
  "target_code": "ks_data = ks_data.mask(ks_data.eq('NaN'     )).dropna()\nks_data['launched'          ] = pd.to_datetime(ks_data['launched'          ])\nks_data['deadline'          ] = pd.to_datetime(ks_data['deadline'          ])\nks_data['project_age'             ]=((ks_data['deadline'          ] - ks_data['launched'          ]) / pd.offsets.Day(1))\ndel ks_data['deadline'          ]\ndel ks_data['launched'          ]\nks_data.head(15)",
  "generation": "ks_data['deadline'] = pd.to_datetime(ks_data['deadline'])\nks_data['launched'] = pd.to_datetime(ks_data['launched'])\nks_data['project_age'] = (ks_data['deadline'] - ks_data['launched']).dt.days\nks_data = ks_data.drop(['deadline', 'launched'], axis=1)\nks_data = ks_data[ks_data['pledged'] <= ks_data['goal'] * 1.5]"
 },
 {
  "nbid": "NB_4855145",
  "file": "NB_4855145,data_,val,p-1.ipynb",
  "row_id": 9,
  "target_code": "data_['Last_Week_Sales'                 ] = data_.groupby(['Product_Code'              ])['Sales'       ].shift()\ndata_['Last_Week_Diff'                ] = data_.groupby(['Product_Code'              ])['Last_Week_Sales'                 ].diff()\ndata_ = data_.dropna()\ndata_.head()",
  "generation": "data_['Last_Week_Sales'] = data_.groupby('Product_Code')['Sales'].shift()\ndata_['Last_Week_Diff'] = data_.groupby('Product_Code')['Last_Week_Sales'].diff()\ndata_ = data_.dropna()\ndata_.reset_index(drop=True, inplace=True)\ndata_.head()"
 },
 {
  "nbid": "NB_4861704",
  "file": "NB_4861704,df-1.ipynb",
  "row_id": 3,
  "target_code": "df[['level1'        , 'level2'         , 'level3'         , 'level4'         ]] = pd.get_dummies(df['prestige'          ])\ndf.drop(['prestige'          , 'level4'         ], axis=1, inplace=True)\ndf.head()",
  "generation": "df = pd.get_dummies(df, columns=['prestige'], prefix='level')\ndf.head()"
 },
 {
  "nbid": "NB_1741093",
  "file": "NB_1741093,waterUsage-1.ipynb",
  "row_id": 24,
  "target_code": "waterUsage['DateTime'          ] = pd.to_datetime(waterUsage['Year'      ],format='%Y'    )\nwaterUsage.head()",
  "generation": "waterUsage['DateTime'] = pd.to_datetime(waterUsage['Year'], format='%Y') + pd.to_timedelta(6, unit='M') - pd.DateOffset(months=6) \nwaterUsage"
 },
 {
  "nbid": "NB_1750308",
  "file": "NB_1750308,library_new-1.ipynb",
  "row_id": 60,
  "target_code": "library_new = library_new.drop(columns='Notice Preference Code'                        )\nlibrary_new.head()",
  "generation": "library_new = library.drop(columns='Patron Type Code')\nlibrary_new = library_new[new_col_order]\nlibrary_new = library_new[~library_new['Patron Type Definition'].isin(patron_outliers)]\nlibrary_new = library_new.rename(columns={'Notice Preference Code': 'Notice Preference Definition'})\nlibrary_new.head()"
 },
 {
  "nbid": "NB_1750308",
  "file": "NB_1750308,library_new-1.ipynb",
  "row_id": 77,
  "target_code": "library_new = library_new.drop(columns=['Circulation Active Year'                         , 'Circulation Active Month'                           ])\nlibrary_new.head()",
  "generation": "library_new['Circulation Active Date'] = pd.to_datetime(library_new['Circulation Active Month'] + ' ' + library_new['Circulation Active Year'].astype(str))\nlibrary_new = library_new[new_col_order]\nlibrary_new = library_new[~library_new['Patron Type Definition'].isin(patron_outliers)]\nlibrary_new.head()"
 },
 {
  "nbid": "NB_1750308",
  "file": "NB_1750308,library_new-1.ipynb",
  "row_id": 85,
  "target_code": "library_new = library_new.drop(columns='Home Library Code'                   )\nlibrary_new.head()",
  "generation": "library_new = library.drop(columns=['Home Library Code'])\nlibrary_new = library_new[new_col_order]\nlibrary_new = library_new[~library_new['Patron Type Definition'].isin(patron_outliers)]\nlibrary_new.tail()"
 },
 {
  "nbid": "NB_1750308",
  "file": "NB_1750308,library_new-1.ipynb",
  "row_id": 101,
  "target_code": "library_new = library_new[new_col_order]\nlibrary_new.head()",
  "generation": "library_new = library[['Patron Type Definition', 'Age Range', 'Home Library Definition', 'Circulation Active Date', 'Total Checkouts', 'Total Renewals', 'Notice Preference Definition', 'Provided Email Address', 'Year Patron Registered', 'Outside of County', 'Supervisor District']]\nlibrary_new.head()"
 },
 {
  "nbid": "NB_1750909",
  "file": "NB_1750909,df,charges,preds-1.ipynb",
  "row_id": 11,
  "target_code": "df['sex'     ] = df['sex'     ].apply(lambda x: 1 if x == 'female'          else 0)\ndf['smoker'        ] = df['smoker'        ].apply(lambda x: 1 if x == 'yes'       else 0)\ndf.head()",
  "generation": "df[\"sex\"] = np.where(df[\"sex\"]==\"female\", 1, 0)\ndf[\"smoker\"] = np.where(df[\"smoker\"]==\"yes\", 1, 0)\ndf.head()"
 },
 {
  "nbid": "NB_1750958",
  "file": "NB_1750958,df,df1996,df1997-1.ipynb",
  "row_id": 11,
  "target_code": "df.index = df['date'      ] \ndf['month'       ] = df['date'      ].dt.strftime('%b'    )\ndf['year'      ] = df['date'      ].dt.strftime('%Y'    )\ndf['month_year'            ] = df.date.dt.to_period('M'   )\ndf.head(5)",
  "generation": "df.set_index('date', inplace=True)\ndf['month'     ] = df.index.strftime('%b')\ndf['year'      ] = df.index.strftime('%Y')\ndf['month_year'] = df.index.strftime('%Y-%m')\ndf.head()"
 },
 {
  "nbid": "NB_1756234",
  "file": "NB_1756234,df1-1.ipynb",
  "row_id": 7,
  "target_code": "df1 = df1.reindex(index=df1.index[::-1])\ndf1 = df1.set_index('Date'      )\ndf1.head()",
  "generation": "df1 = df1.sort_values(by='Date', ascending=True)\ndf1.head()"
 },
 {
  "nbid": "NB_1756703",
  "file": "NB_1756703,test-1.ipynb",
  "row_id": 94,
  "target_code": "test = test.merge(stores, how='left'      ).merge(features, how='left'      )\ntest.head(5)",
  "generation": "merged_data = pd.merge(test, stores, on='Store', how='left')\nmerged_data = pd.merge(merged_data, features, on=['Store', 'Date'], how='left')\nmerged_data = pd.concat([merged_data, type_test], axis=1)\nmerged_data.head()"
 },
 {
  "nbid": "NB_1756703",
  "file": "NB_1756703,test-1.ipynb",
  "row_id": 101,
  "target_code": "test['year'      ] = pd.DatetimeIndex(test['Date'      ]).year\ntest['month'       ] = pd.DatetimeIndex(test['Date'      ]).month\ntest.head()",
  "generation": "test['year'] = pd.DatetimeIndex(test['Date']).year\ntest['month'] = pd.DatetimeIndex(test['Date']).month\ntest = test.drop(['Fuel_Price', 'CPI', 'Unemployment'], axis=1)\ntest.head()"
 },
 {
  "nbid": "NB_1760904",
  "file": "NB_1760904,star_wars,grouped-1.ipynb",
  "row_id": 6,
  "target_code": "star_wars[star_wars.columns[9:15]] = star_wars[star_wars.columns[9:15]].astype(float)\nstar_wars = star_wars.rename(columns={\n        \"Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.\"                                                                                                                                                       : \"ranking_1\"            ,\n        \"Unnamed: 10\"                     : \"ranking_2\"            ,\n        \"Unnamed: 11\"                     : \"ranking_3\"            ,\n        \"Unnamed: 12\"                     : \"ranking_4\"            ,\n        \"Unnamed: 13\"                     : \"ranking_5\"            ,\n        \"Unnamed: 14\"                     : \"ranking_6\"            \n})\npd.set_option(\"display.max_columns\"                     ,38)\nstar_wars.head()",
  "generation": "star_wars = star_wars.rename(columns={\n        \"Which of the following Star Wars films have you seen? Please select all that apply.\": \"seen_1\",\n        \"Unnamed: 4\": \"seen_2\",\n        \"Unnamed: 5\": \"seen_3\",\n        \"Unnamed: 6\": \"seen_4\",\n        \"Unnamed: 7\": \"seen_5\",\n        \"Unnamed: 8\": \"seen_6\"\n})\n\nmovie_names = {\n    \"Star Wars: Episode I  The Phantom Menace\": True,\n    \"Star Wars: Episode II  Attack of the Clones\": True,\n    \"Star Wars: Episode III  Revenge of the Sith\": True,\n    \"Star Wars: Episode IV  A New Hope\": True,\n    \"Star Wars: Episode V The Empire Strikes Back\": True,\n    \"Star Wars: Episode VI Return of the Jedi\": True,\n    np.nan: False\n}\n\nfor c in star_wars.columns[3:9]:\n    star_wars[c] = star_wars[c].map(movie_names)\n    \nstar_wars.head()"
 },
 {
  "nbid": "NB_1760904",
  "file": "NB_1760904,star_wars,grouped-1.ipynb",
  "row_id": 18,
  "target_code": "print(star_wars[\"Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.\"                                                                                                                ].unique())\nstar_wars = star_wars.rename(columns={\n        \"Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.\"                                                                                                                        : \"Han Solo\"           ,\n        \"Unnamed: 16\"                     : \"Luke Skywalker\"                 ,\n        \"Unnamed: 17\"                     : \"Princess Leia Organa\"                       ,\n        \"Unnamed: 18\"                     : \"Anakin Skywalker\"                   ,\n        \"Unnamed: 19\"                     : \"Obi Wan Kenobi\"                 ,\n        \"Unnamed: 20\"                     : \"Emperor Palpatine\"                    ,\n        \"Unnamed: 21\"                     : \"Darth Vader\"              ,\n        \"Unnamed: 22\"                     : \"Lando Calrissian\"                   ,\n        \"Unnamed: 23\"                     : \"Boba Fett\"            ,\n        \"Unnamed: 24\"                     : \"C-3P0\"        ,\n        \"Unnamed: 25\"                     : \"R2 D2\"        ,\n        \"Unnamed: 26\"                     : \"Jar Jar Binks\"                ,\n        \"Unnamed: 27\"                     : \"Padme Amidala\"                ,\n        \"Unnamed: 28\"                     : \"Yoda\"       ,\n})\ncharacter_scores = {\n    \"Very unfavorably\"                      : -2,\n    \"Somewhat unfavorably\"                          : -1,\n    \"Neither favorably nor unfavorably (neutral)\"                                                 : 0,\n    \"Somewhat favorably\"                        : 1,\n    \"Very favorably\"                    : 2,\n    \"Unfamiliar (N/A)\"                      : 0,\n    np.nan: 0\n}\nfor c in star_wars.columns[15:29]:\n    star_wars[c] = star_wars[c].map(character_scores)\nstar_wars.head()",
  "generation": "char_cols = star_wars.columns[15:29]\n\nchar_dict = {\n    \"Very favorably\": 2,\n    \"Somewhat favorably\": 1,\n    \"Neither favorably nor unfavorably (neutral)\": 0,\n    \"Somewhat unfavorably\": -1,\n    \"Very unfavorably\": -2,\n    np.nan: 0\n}\n\nfor c in char_cols:\n    star_wars[c] = star_wars[c].map(char_dict)\n\nstar_wars[char_cols] = star_wars[char_cols].astype(int)\n\nstar_wars.head()"
 },
 {
  "nbid": "NB_1763443",
  "file": "NB_1763443,df-1.ipynb",
  "row_id": 4,
  "target_code": "df.month = pd.to_datetime(df.month)\ndf.set_index('month'       ,inplace=True)\ndf.head()",
  "generation": "df['Month'] = pd.to_datetime(df['Month'], format='%Y-%m')\ndf = df.set_index(df['Month'])\ndf = df.drop(columns=['Month'])\ndf.head()"
 },
 {
  "nbid": "NB_1763453",
  "file": "NB_1763453,df,x-1.ipynb",
  "row_id": 39,
  "target_code": "df['wide petal'            ] = df['petal width'             ].apply(lambda v: 1 if v >= 1.3 else 0)\ndf.head()",
  "generation": "df['wide petal'] = df['petal width'].apply(lambda x: 1 if x >= 1.3 else 0)\ndf.head()"
 },
 {
  "nbid": "NB_1763453",
  "file": "NB_1763453,df,x-1.ipynb",
  "row_id": 40,
  "target_code": "df['petal area'            ] = df.apply(lambda r: r['petal length'              ] * r['petal width'             ], axis=1)\ndf.head()",
  "generation": "df['petal area'] = df['petal length'] * df['petal width']\ndf"
 },
 {
  "nbid": "NB_1776123",
  "file": "NB_1776123,df,showed-1.ipynb",
  "row_id": 29,
  "target_code": "df.head(20)",
  "generation": "df.rename(columns={'PatientId':'patient_id', 'AppointmentID':'appointment_id', 'Gender':'gender', 'ScheduledDay':'scheduled_day', 'AppointmentDay':'appointment_day', 'Age':'age', 'Neighbourhood':'neighborhood', 'Scholarship':'scholarship', 'Hipertension':'hypertension', 'Diabetes':'diabetes', 'Alcoholism':'alcoholism', 'Handcap':'handicap', 'SMS_received':'sms_received', 'No-show':'no_show'}, inplace=True)\ndf['no_show'] = df['no_show'].apply(lambda x: 0 if x == 'No' else 1)\ndf.head()"
 },
 {
  "nbid": "NB_1778791",
  "file": "NB_1778791,mtcars-1.ipynb",
  "row_id": 20,
  "target_code": "mtcars['wt'    ] = (mtcars['wt'    ] * 1000)/2\nmtcars.head()",
  "generation": "mtcars['wt'] = mtcars['wt'] * 453.592 / 1000\nmtcars.rename(columns={'wt':'wt(kg)'}, inplace=True)\nmtcars.head()"
 },
 {
  "nbid": "NB_1779019",
  "file": "NB_1779019,poverty_by_type-1.ipynb",
  "row_id": 9,
  "target_code": "poverty_by_type = pd.merge(poverty_by_type, types)\npoverty_by_type",
  "generation": "poverty_by_type = pd.merge(poverty_by_type, types, on='SchoolID', how='left')\npoverty_by_type.head()"
 },
 {
  "nbid": "NB_1779019",
  "file": "NB_1779019,poverty_by_type-1.ipynb",
  "row_id": 10,
  "target_code": "poverty_by_type = poverty_by_type.groupby(['District'          , 'SchoolType'             ]).sum()\npoverty_by_type",
  "generation": "poverty_by_type = poverty_by_type.groupby(['District', 'SchoolType']).sum()\npoverty_by_type = poverty_by_type.reset_index()\npoverty_by_type = poverty_by_type[['District', 'SchoolType', 'Enrollment', 'Eligible_Students']]\npoverty_by_type = poverty_by_type.sort_values(['District', 'SchoolType'])\npoverty_by_type = poverty_by_type.reset_index(drop=True)\npoverty_by_type"
 },
 {
  "nbid": "NB_1779019",
  "file": "NB_1779019,poverty_by_type-1.ipynb",
  "row_id": 15,
  "target_code": "poverty_by_type = poverty_by_type.dropna()\npoverty_by_type",
  "generation": "poverty_by_type = poverty_by_type.dropna()\npoverty_by_type = poverty_by_type.reset_index()\npoverty_by_type = poverty_by_type.drop(['Unnamed: 1_level_1', 'Unnamed: 2_level_1', 'Unnamed: 3_level_1'], axis=1)\npoverty_by_type = poverty_by_type.set_index(['District'])\npoverty_by_type = poverty_by_type.dropna()\npoverty_by_type"
 },
 {
  "nbid": "NB_1791389",
  "file": "NB_1791389,carData-1.ipynb",
  "row_id": 10,
  "target_code": "carData = pd.get_dummies(carData, columns=['origin'        ])\ncarData.head()",
  "generation": "carData['horsepower'] = carData['horsepower'].replace('?', np.nan)\ncarData = carData.dropna()\ncarData['horsepower'] = carData['horsepower'].astype('float64')\ncarData = pd.get_dummies(carData, columns=['origin'])\ncarData.head()"
 },
 {
  "nbid": "NB_1792049",
  "file": "NB_1792049,team_playoff-1.ipynb",
  "row_id": 6,
  "target_code": "team_playoff['\u5931\u5206'    ] = team_playoff['\u6bd4\u5206'    ].map(get_loss_score)\nteam_playoff['\u8d5b\u5b63'    ] = team_playoff['\u65f6\u95f4'    ].map(convert_time_to_season)\nteam_playoff['\u56de\u5408'    ] = (team_playoff['\u51fa\u624b'    ] + 0.44 * team_playoff['\u7f5a\u7403\u51fa\u624b'      ] - 0.96 * team_playoff['\u524d\u573a'    ] + team_playoff['\u5931\u8bef'    ]) / 2\nteam_playoff.head()",
  "generation": "team_playoff['\u524d\u573a'] = team_playoff['\u7bee\u677f'] - team_playoff['\u540e\u573a']\nteam_playoff['\u5931\u5206'] = team_playoff['\u6bd4\u5206'].str.split('-').str[0].astype(int)\nteam_playoff['\u8d5b\u5b63'] = team_playoff['\u65f6\u95f4'].str.split('-').str[0].astype(int)\nteam_playoff['\u56de\u5408'] = team_playoff['\u5f97\u5206'] / (team_playoff['\u51fa\u624b'] - team_playoff['\u4e09\u5206\u547d\u4e2d'])\nteam_playoff.head()"
 },
 {
  "nbid": "NB_1796022",
  "file": "NB_1796022,big_battles-1.ipynb",
  "row_id": 40,
  "target_code": "big_battles = big_battles[['name'      ,'attacker_size'               ,'defender_size'               ]]\nbig_battles",
  "generation": "big_battles = battles[battles.attacker_size + battles.defender_size >= 10000][['name', 'attacker_size', 'defender_size']]\nbig_battles"
 },
 {
  "nbid": "NB_1796115",
  "file": "NB_1796115,dataset-1.ipynb",
  "row_id": 17,
  "target_code": "dataset = dataset.drop(['X2'    ,'X4'    ,'X7'    ], axis=1)\ndataset.head()",
  "generation": "dataset = dataset.drop(columns=[\"X2\", \"X4\", \"X7\"])\ndataset.rename(columns={\"X1\": \"X1\", \"X3\": \"X2\", \"X5\": \"X3\", \"X6\": \"X4\", \"X8\": \"X5\", \"Y1\": \"Y1\", \"Y2\": \"Y2\"}, inplace=True)\ndataset.head()"
 },
 {
  "nbid": "NB_1800746",
  "file": "NB_1800746,data-1.ipynb",
  "row_id": 8,
  "target_code": "data['portfolio_type'                ] = data['portfolio_type'                ].replace('investing'           , 0)\ndata['portfolio_type'                ] = data['portfolio_type'                ].replace('saving'        , 1)\ndata['portfolio_type'                ] = data['portfolio_type'                ].replace('multi'       , 2)\ndata.head()",
  "generation": "data['gender'                            ]=data['gender'                            ].apply(lambda x: 0 if x=='female' else 1)\ndata = data.drop(columns=['aim_id','os', 'initial_date'])"
 },
 {
  "nbid": "NB_1800746",
  "file": "NB_1800746,data-1.ipynb",
  "row_id": 17,
  "target_code": "data = data.drop(columns=['portfolio_type'                ,'add_amount'            , 'prior_investment'                   ,'occupation'            ,'total_asset_amount'                    ,'target_period'               ,'investment_priority'                     ,'patience_to_loss_percentage'                             ])\ndata.head()",
  "generation": "data = data[['user_id', 'gender', 'age', 'os', 'annual_income', 'initial_date', 'initial_amount', 'etf_knowledge', 'investment_amount', 'add_count', 'total_amount', 'target_count']]\ndata['portfolio_type'] = portfolio_type_dummy['portfolio_type_1']*0 + portfolio_type_dummy['portfolio_type_2']*1\ndata = data.drop(columns=['os', 'add_amount', 'prior_investment', 'occupation', 'total_asset_amount', 'target_period', 'investment_priority', 'patience_to_loss_percentage'])\ndata.head()"
 },
 {
  "nbid": "NB_1802335",
  "file": "NB_1802335,data-1.ipynb",
  "row_id": 11,
  "target_code": "data[\"total_PerCapita\"                 ] = data[\"Beer_PerCapita\"                ] + data[\"Spirit_PerCapita\"                  ] + data[\"Wine_PerCapita\"                ]\ndata.head(1)",
  "generation": "data['total_PerCapita'] = data['Beer_PerCapita'] + data['Spirit_PerCapita'] + data['Wine_PerCapita']\ndata.head()"
 },
 {
  "nbid": "NB_1802335",
  "file": "NB_1802335,df-1.ipynb",
  "row_id": 34,
  "target_code": "df = data1[['country'         ,'price'       ]].groupby([\"country\"         ]).mean()\ndf['code'      ] = ''   \nlst = []\nfor i in range(len(df)):\n    if(df.index[i] == 'Argentina'            ):\n        lst.append('ARG'     )\n    if(df.index[i] == 'Armenia'          ):\n        lst.append('ARM'     )\n    if(df.index[i] == 'Australia'            ):\n        lst.append('AUS'     )\n    if(df.index[i] == 'Austria'          ):\n        lst.append('AUT'     )\n    if(df.index[i] == 'Bosnia and Herzegovina'                         ):\n        lst.append('BIH'     )       \n    if(df.index[i] == 'Brazil'         ):\n        lst.append('BRA'     )          \n    if(df.index[i] == 'Bulgaria'           ):\n        lst.append('BGR'     )    \n    if(df.index[i] == 'Canada'         ):\n        lst.append('CAN'     )  \n    if(df.index[i] == 'Chile'        ):\n        lst.append('CHL'     )     \n    if(df.index[i] == 'China'        ):\n        lst.append('CHN'     )  \n    if(df.index[i] == 'Croatia'          ):\n        lst.append('HRV'     )          \n    if(df.index[i] == 'Cyprus'         ):\n        lst.append('CYP'     )         \n    if(df.index[i] == 'Czech Republic'                 ):\n        lst.append('CZE'     ) \n    if(df.index[i] == 'Egypt'        ):\n        lst.append('EGY'     ) \n    if(df.index[i] == 'England'          ):\n        lst.append('GBR'     )  \n    if(df.index[i] == 'France'         ):\n         lst.append('FRA'     )\n    if(df.index[i] == 'Georgia'          ):\n        lst.append('GEO'     )             \n    if(df.index[i] == 'Germany'          ):\n        lst.append('DEU'     )\n    if(df.index[i] == 'Greece'         ):\n        lst.append('GRC'     )        \n    if(df.index[i] == 'Hungary'          ):\n        lst.append('HUN'     )        \n    if(df.index[i] == 'India'        ):\n        lst.append('IND'     )    \n    if(df.index[i] == 'Israel'         ):\n        lst.append('ISR'     )              \n    if(df.index[i] == 'Italy'        ):\n        lst.append('ITA'     )\n    if(df.index[i] == 'Lebanon'          ):\n        lst.append('LBN'     )        \n    if(df.index[i] == 'Luxembourg'             ):\n        lst.append('LUX'     )          \n    if(df.index[i] == 'Macedonia'            ):\n        lst.append('MKD'     )          \n    if(df.index[i] == 'Mexico'         ):\n        lst.append('MEX'     )  \n    if(df.index[i] == 'Moldova'          ):\n        lst.append('MDA'     )  \n    if(df.index[i] == 'Morocco'          ):\n        lst.append('MAR'     ) \n    if(df.index[i] == 'New Zealand'              ):\n        lst.append('NZL'     ) \n    if(df.index[i] == 'Peru'       ):\n        lst.append('PER'     ) \n    if(df.index[i] == 'Portugal'           ):\n        lst.append('PRT'     )\n    if(df.index[i] == 'Romania'          ):\n        lst.append('ROU'     )\n    if(df.index[i] == 'Serbia'         ):\n        lst.append('SRB'     )\n    if(df.index[i] == 'Slovakia'           ):\n        lst.append('SVK'     )\n    if(df.index[i] == 'Slovenia'           ):\n        lst.append('SVN'     )\n    if(df.index[i] == 'South Africa'               ):\n        lst.append('ZAF'     )\n    if(df.index[i] == 'Spain'        ):\n        lst.append('ESP'     )\n    if(df.index[i] == 'Switzerland'              ):\n        lst.append('CHE'     )      \n    if(df.index[i] == 'Turkey'         ):\n        lst.append('TUR'     )  \n    if(df.index[i] == 'US'     ):\n        lst.append('USA'     )\n    if(df.index[i] == 'Ukraine'          ):\n        lst.append('UKR'     )          \n    if(df.index[i] == 'Uruguay'          ):\n        lst.append('URY'     )\ndf['code'      ] = lst\ndf = df.dropna(how = 'any'      )\ndf",
  "generation": "import pycountry\ndf['code'] = ''\nfor i in range(len(df)):\n    try:\n        country = pycountry.countries.search_fuzzy(df['country'][i])[0].alpha_3\n    except:\n        country = 'None'\n    df['code'][i] = country\ndf = df[df['code'] != 'None']\nfig = go.Figure(data=go.Choropleth(\n    locations=df['code'],\n    z=df['price'],\n    text=df['country'],\n    colorscale='Reds',\n    autocolorscale=False,\n    reversescale=True,\n    marker_line_color='darkgray',\n    marker_line_width=0.5,\n    colorbar_title='Average Wine Price'\n))\nfig.update_layout(\n    title_text='Average Wine Price by Country',\n    geo=dict(\n        showframe=False,\n        showcoastlines=False,\n        projection_type='equirectangular'\n    ),\n    annotations=[dict(\n        x=0.55,\n        y=0.1,\n        xref='paper',\n        yref='paper',\n        text='Source: <a href=\"https://www.kaggle.com/zynicide/wine-reviews\">\\\n            Wine Reviews</a>',\n        showarrow=False\n    )]\n)\nfig.show()"
 },
 {
  "nbid": "NB_1810341",
  "file": "NB_1810341,military_expense,arg_data,china_data,india_data,usa_data-1.ipynb",
  "row_id": 8,
  "target_code": "for year in years_filter:\n    military_expense[year]=military_expense[year].astype(int)\nmilitary_expense=military_expense.apply(lambda x: x/1000)\nmilitary_expense.head(5)",
  "generation": "military_expense = military_expense.apply(lambda x: x.astype('float64'))\nmilitary_expense = military_expense.apply(lambda x: x/1000)\nmilitary_expense"
 },
 {
  "nbid": "NB_1812855",
  "file": "NB_1812855,new_df-1.ipynb",
  "row_id": 9,
  "target_code": "new_df['Review'        ] = new_df['Title'       ] + ' '     + new_df['Review Text'             ]\nnew_df = new_df.drop(labels=['Title'       ,'Review Text'             ] , axis=1)\nnew_df.head()",
  "generation": "new_df['Review'] = new_df['Title'] + ' ' + new_df['Review Text']\nnew_df = new_df.drop(['Title', 'Review Text'], axis=1)\nnew_df.head()"
 },
 {
  "nbid": "NB_1812855",
  "file": "NB_1812855,new_df-1.ipynb",
  "row_id": 10,
  "target_code": "new_df = new_df.rename(columns={\"Recommended IND\"                 : \"Recommended\"              })\nprint(\"My data's shape is:\"                     , new_df.shape)\nnew_df.head()",
  "generation": "new_df = new_df.rename(columns={'Recommended IND': 'Recommended'})\nnew_df.head()"
 },
 {
  "nbid": "NB_1812855",
  "file": "NB_1812855,new_df-1.ipynb",
  "row_id": 11,
  "target_code": "new_df['Review_length'               ] = new_df['Review'        ].apply(len)\nprint(new_df.shape)\nnew_df.head()",
  "generation": "new_df['Review_length'] = new_df['Review'].apply(len)\nnew_df.head()"
 },
 {
  "nbid": "NB_1815040",
  "file": "NB_1815040,data_g,prices-1.ipynb",
  "row_id": 4,
  "target_code": "data_g['orderdate'           ] = pd.to_datetime(data_g.orderdate,format='%Y-%m-%d'          )\ndata_g = data_g.set_index('orderdate'           )\ndata_g.head()",
  "generation": "data_g['orderdate'] = pd.to_datetime(data_g['orderdate'])\ndata_g = data_g.set_index(['orderdate', 'id', 'cd', 'baseprice', 'basenum', 'contprice', 'dealnum', 'trans-amount', 'jsd'])\ndata_g.head()"
 },
 {
  "nbid": "NB_1818171",
  "file": "NB_1818171,virus_zika-1.ipynb",
  "row_id": 5,
  "target_code": "virus_zika['month'       ] = pd.DatetimeIndex(virus_zika['Last Update'             ]).month\nvirus_zika.head()",
  "generation": "virus_zika['month'] = pd.DatetimeIndex(virus_zika['Last Update']).month\nvirus_zika.head()"
 },
 {
  "nbid": "NB_1818171",
  "file": "NB_1818171,virus_zika-1.ipynb",
  "row_id": 6,
  "target_code": "virus_zika['day'     ] = pd.DatetimeIndex(virus_zika['Last Update'             ]).day\nvirus_zika.head()",
  "generation": "virus_zika['day'] = pd.DatetimeIndex(virus_zika['Last Update']).day\nvirus_zika.head()"
 },
 {
  "nbid": "NB_248393",
  "file": "NB_248393,us_states_df-1.ipynb",
  "row_id": 8,
  "target_code": "state_codes = {\n    'WA'        : '53'     , 'DE'     : '10'     , 'DC'     : '11'     , 'WI'     : '55'     , 'WV'     : '54'     , 'HI'     : '15'     ,\n    'FL'        : '12'     , 'WY'     : '56'     , 'PR'     : '72'     , 'NJ'     : '34'     , 'NM'     : '35'     , 'TX'     : '48'     ,\n    'LA'        : '22'     , 'NC'     : '37'     , 'ND'     : '38'     , 'NE'     : '31'     , 'TN'     : '47'     , 'NY'     : '36'     ,\n    'PA'        : '42'     , 'AK'     : '2'    , 'NV'     : '32'     , 'NH'     : '33'     , 'VA'     : '51'     , 'CO'     : '8'    ,\n    'CA'        : '6'    , 'AL'     : '1'    , 'AR'     : '5'    , 'VT'     : '50'     , 'IL'     : '17'     , 'GA'     : '13'     ,\n    'IN'        : '18'     , 'IA'     : '19'     , 'MA'     : '25'     , 'AZ'     : '4'    , 'ID'     : '16'     , 'CT'     : '9'    ,\n    'ME'        : '23'     , 'MD'     : '24'     , 'OK'     : '40'     , 'OH'     : '39'     , 'UT'     : '49'     , 'MO'     : '29'     ,\n    'MN'        : '27'     , 'MI'     : '26'     , 'RI'     : '44'     , 'KS'     : '20'     , 'MT'     : '30'     , 'MS'     : '28'     ,\n    'SC'        : '45'     , 'KY'     : '21'     , 'OR'     : '41'     , 'SD'     : '46'     \n}\nfips2code = {value: key for key, value in state_codes.items()}\nus_states_df.fips = us_states_df.fips.astype('str'     )\nus_states_df.fips = us_states_df.fips.map(fips2code)\nus_states_df = us_states_df.dropna()\nus_states_df.rename(columns = {'fips'      :'code'      }, inplace = True) \nus_states_df.head(5)",
  "generation": "us_states_df['code'] = us_states_df['fips'].apply(lambda x: str(x).zfill(2))\nus_states_df.head(5)"
 },
 {
  "nbid": "NB_254221",
  "file": "NB_254221,investments_fs-1.ipynb",
  "row_id": 29,
  "target_code": "investments_fs['funding_total_usd'                   ] = investments_fs.funding_total_usd.str.strip()\ninvestments_fs['status'        ] = investments_fs.status.str.upper()\ninvestments_fs.head()",
  "generation": "investments_fs['status'] = investments_fs['status'].str.upper()\ninvestments_fs.head()"
 },
 {
  "nbid": "NB_257384",
  "file": "NB_257384,df,correlation_model-1.ipynb",
  "row_id": 94,
  "target_code": "month_dummies = pd.get_dummies(df['date'      ], prefix=\"month\"       )\ndf.drop([\"date\"      ], axis = 1, inplace = True)\ndf = pd.concat([df, month_dummies], axis = 1)\ndf.head()",
  "generation": "df['month_3'] = np.where(df['date'] == 3, 1, 0)\ndf['month_4'] = np.where(df['date'] == 4, 1, 0)\ndf['month_5'] = np.where(df['date'] == 5, 1, 0)\ndf['month_6'] = np.where(df['date'] == 6, 1, 0)\ndf['month_7'] = np.where(df['date'] == 7, 1, 0)\ndf['month_8'] = np.where(df['date'] == 8, 1, 0)\ndf['month_9'] = np.where(df['date'] == 9, 1, 0)\ndf['month_10'] = np.where(df['date'] == 10, 1, 0)\ndf['month_11'] = np.where(df['date'] == 11, 1, 0)\ndf['month_12'] = np.where(df['date'] == 12, 1, 0)\ndf.drop('date', axis = 1, inplace = True)\ndf.head()"
 },
 {
  "nbid": "NB_4898509",
  "file": "NB_4898509,data,lbl-1.ipynb",
  "row_id": 8,
  "target_code": "data = data.drop('Cabin'       , 1)\ndata.head()",
  "generation": "data = data.drop(['Cabin'], axis=1)"
 },
 {
  "nbid": "NB_4898509",
  "file": "NB_4898509,data,lbl-1.ipynb",
  "row_id": 10,
  "target_code": "data = data.drop('PassengerId'             , 1)\ndata.head()",
  "generation": "data = data.drop('PassengerId', 1)\ndata.head()"
 },
 {
  "nbid": "NB_4912336",
  "file": "NB_4912336,data,y-1.ipynb",
  "row_id": 2,
  "target_code": "a_trocar = {\n    'no'        : 0,\n    'yes'         : 1\n}\ndata.sold = data.sold.map(a_trocar)\ndata.head()",
  "generation": "data['sold'] = data['sold'].apply(lambda x: 1 if x == 'yes' else 0)\ndata.head()"
 },
 {
  "nbid": "NB_4912336",
  "file": "NB_4912336,data,y-1.ipynb",
  "row_id": 3,
  "target_code": "from datetime import datetime\ncurrent_year = datetime.today().year\ndata['model_age'           ] = current_year - data.model_year\ndata.head()",
  "generation": "data['model_age'] = datetime.now().year - data['model_year']\ndata.head()"
 },
 {
  "nbid": "NB_4912336",
  "file": "NB_4912336,data,y-1.ipynb",
  "row_id": 5,
  "target_code": "data = data.drop(columns = ['Unnamed: 0'            , 'mileage_per_year'                   , 'model_year'             ], axis = 1)\ndata.head()",
  "generation": "data = data[['price', 'sold', 'model_age', 'km_per_year']]\ndata.head()"
 },
 {
  "nbid": "NB_4937824",
  "file": "NB_4937824,df-1.ipynb",
  "row_id": 7,
  "target_code": "df['City_Category'               ]=df['City_Category'               ].map({'A'   :'Metro Cities'              , 'B'    :'Small Towns'             , 'C'    :'Villages'          , 'X'     :'X'    })\ndf.head()",
  "generation": "df['City_Category'] = df['City_Category'].map({'A': 'Metro Cities', 'B': 'Small Towns', 'C': 'Villages', 'X': 'Unknown'})\ndf.head()"
 },
 {
  "nbid": "NB_4937824",
  "file": "NB_4937824,df-1.ipynb",
  "row_id": 9,
  "target_code": "df['Marital_Status'                ]=df['Marital_Status'                ].map({1:'Married'         ,0:'Un-Married'            })\ndf.head()",
  "generation": "df['Marital_Status']=df['Marital_Status'].map({0:'Un-Married', 1:'Married'})\ndf.head()"
 },
 {
  "nbid": "NB_4949422",
  "file": "NB_4949422,sms-1.ipynb",
  "row_id": 6,
  "target_code": "sms['length'        ] = sms['message'         ].apply(len)\nsms.head()",
  "generation": "sms['length'] = sms['message'].apply(len)\nsms.head()"
 },
 {
  "nbid": "NB_4961049",
  "file": "NB_4961049,charity_df-1.ipynb",
  "row_id": 4,
  "target_code": "charity_df = charity_df.drop(columns=['EIN'     , 'NAME'       ])\ncharity_df.head()",
  "generation": "charity_df = charity_df.drop(columns=['EIN', 'NAME'], axis=1)\ncharity_cat = charity_df.dtypes[(charity_df.dtypes == 'object')].index.tolist()\nenc = OneHotEncoder(sparse=False)\nencode_df = pd.DataFrame(enc.fit_transform(charity_df[charity_cat]))\nencode_df.columns = enc.get_feature_names(charity_cat)\ncharity_df = charity_df.merge(encode_df,left_index=True,right_index=True)\ncharity_df = charity_df.drop(charity_cat,axis=1)\ncharity_df.head()"
 },
 {
  "nbid": "NB_4975305",
  "file": "NB_4975305,df2-1.ipynb",
  "row_id": 21,
  "target_code": "df2['duplicate_value'                 ] = df2.duplicated('user_id'         )\ndf2.head()",
  "generation": "df2['duplicate_value'] = df2.duplicated(subset='user_id', keep=False)\ndf2.head()"
 },
 {
  "nbid": "NB_4980289",
  "file": "NB_4980289,features-1.ipynb",
  "row_id": 12,
  "target_code": "features = features.fillna(0.0)\nfeatures.head()",
  "generation": "data['Cabin'] = data['Cabin'].fillna(0)\nfeatures = data.drop(['PassengerId', 'Name', 'Survived'], axis = 1)\nfeatures.head()"
 },
 {
  "nbid": "NB_4986662",
  "file": "NB_4986662,df-1.ipynb",
  "row_id": 3,
  "target_code": "dt=pd.to_datetime(df.date,format=\"%Y-%m-%d\"          )\ndf.index=pd.DatetimeIndex(dt)\ndf=df.drop(['date'      ],axis=1)\ndf.head()",
  "generation": "df = df.set_index(['date', 'store', 'item'])\ndf.head()"
 },
 {
  "nbid": "NB_5002524",
  "file": "NB_5002524,gradebook-1.ipynb",
  "row_id": 8,
  "target_code": "gradebook.index = gradebook.student\ngradebook = gradebook.drop('student'         , axis = 1)\ngradebook",
  "generation": "gradebook = pd.read_csv('gradebook_v2.csv')\ngradebook.set_index('student', inplace=True)\ngradebook.index.name = None\ngradebook.columns = pd.MultiIndex.from_product([[''], gradebook.columns])\ngradebook.head()"
 },
 {
  "nbid": "NB_5008487",
  "file": "NB_5008487,df_new-1.ipynb",
  "row_id": 71,
  "target_code": "df_new[['CA'    , 'UK'     , 'US'     ]] = pd.get_dummies(df_new.country)\ndf_new.head(2)",
  "generation": "df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])\ndf_new.head()"
 },
 {
  "nbid": "NB_1129062",
  "file": "NB_1129062,df,cols,cm-1.ipynb",
  "row_id": 8,
  "target_code": "intFeatures = ['Age'     ]\nstringFeatures = ['Gender'        , 'Country'          , 'self_employed'                , 'family_history'                 , 'treatment'            , 'work_interfere'                 ,\n                 'no_employees'                               , 'remote_work'              , 'tech_company'               , 'anonymity'            , 'leave'        , 'mental_health_consequence'                            ,\n                 'phys_health_consequence'                                          , 'coworkers'            , 'supervisor'             , 'mental_health_interview'                          , 'phys_health_interview'                        ,\n                 'mental_vs_physical'                                     , 'obs_consequence'                  , 'benefits'           , 'care_options'               , 'wellness_program'                   ,\n                 'seek_help'                            ]\nfloatFeatures = []\nfor feature in df:\n    if feature in intFeatures:\n        df[feature] = df[feature].fillna(defaultInt)\n    elif feature in stringFeatures:\n        df[feature] = df[feature].fillna(defaultString)\n    elif feature in floatFeatures:\n        df[feature] = df[feature].fillna(defaultFloat)\n    else:\n        print('Error: Feature %s not recognized.'                                    % feature)\ndf",
  "generation": "df = df.fillna(defaultString)\nfor feature in df:\n    if df[feature].dtype == 'object':\n        le = preprocessing.LabelEncoder()\n        df[feature] = le.fit_transform(df[feature])\n    else:\n        df[feature].fillna(defaultInt, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_1129702",
  "file": "NB_1129702,DF,y-1.ipynb",
  "row_id": 5,
  "target_code": "sex_encoding = {'male'      : 0, 'female'         : 1}\nembarked_encoding = {'C'   : 0, 'Q'    : 1, 'S'    : 2}\nDF['Sex'     ] = DF['Sex'     ].apply(lambda r: sex_encoding.get(r))\nDF['Embarked'          ] = DF['Embarked'          ].apply(lambda r: embarked_encoding.get(r))\nDF.head()",
  "generation": "# Encoding\nDF['Sex'] = DF['Sex'].map({'female': 1, 'male': 0})\nDF['Embarked'] = DF['Embarked'].map({'S': 2, 'C': 0, 'Q': 1})\nDF.head()"
 },
 {
  "nbid": "NB_1133049",
  "file": "NB_1133049,df-1.ipynb",
  "row_id": 10,
  "target_code": "df['salary_encoded'                ] = df['salary'        ].apply(lambda x: salarary_dict[x])\ndf['sales_encoded'               ] = df['sales'       ].apply(lambda x: sales_dict[x])\ndf.head(10)",
  "generation": "df['salary_encoded'] = df['salary'].map(salarary_dict)\ndf['sales_encoded'] = df['sales'].map(sales_dict)\ndf.head()"
 },
 {
  "nbid": "NB_1136710",
  "file": "NB_1136710,model2-1.ipynb",
  "row_id": 54,
  "target_code": "model2 = model2.drop(\"yearsrank\"           , axis=1)\nmodel2.head()",
  "generation": "multicollinearity_salary_df.drop(columns=['yearsrank'], inplace=True)"
 },
 {
  "nbid": "NB_1143377",
  "file": "NB_1143377,data,y-1.ipynb",
  "row_id": 86,
  "target_code": "data['Size_large'            ] = data.Size.map({'small'       :0, 'large'        :1})\ndata.head()",
  "generation": "data['Size_large'] = data.Size.map({'small':0, 'large':1})\ndata.head()"
 },
 {
  "nbid": "NB_1143377",
  "file": "NB_1143377,data,y-1.ipynb",
  "row_id": 95,
  "target_code": "data = pd.concat([data, area_dummies], axis=1)\ndata.head()",
  "generation": "data = pd.concat([data, area_dummies], axis = 1)\ndata = data.drop(['Area'], axis = 1)\ndata.head()"
 },
 {
  "nbid": "NB_1148579",
  "file": "NB_1148579,mails,testData-1.ipynb",
  "row_id": 4,
  "target_code": "mails['class'       ] = mails['class'       ].map({'ham'     : 0, 'spam'       : 1})\nmails.head()",
  "generation": "mails['class'] = mails['class'].map({'ham': 0, 'spam': 1})\nmails.rename(columns={'class': 'Unnamed: 0', 'SMS': 'class'}, inplace=True)\nmails = mails[['Unnamed: 0', 'class']] + mails[['class', 'Unnamed: 0']]\nmails.rename(columns={'Unnamed: 0': 'index'}, inplace=True)\nmails.set_index('index', inplace=True)\nmails"
 },
 {
  "nbid": "NB_1180886",
  "file": "NB_1180886,df-1.ipynb",
  "row_id": 63,
  "target_code": "df = df.dropna(axis=1)\ndf.head()",
  "generation": "df['datetime'] = df['datetime'].apply(lambda x: pd.to_datetime(x))\ndf = df.drop('Unnamed: 1_level_1', axis=1)\ndf = df.drop('Unnamed: 2_level_1', axis=1)\ndf = df.drop('Unnamed: 3_level_1', axis=1)\ndf = df.drop('Unnamed: 4_level_1', axis=1)\ndf = df.drop('Unnamed: 5_level_1', axis=1)\ndf = df.drop('Unnamed: 6_level_1', axis=1)\ndf = df.drop('Unnamed: 7_level_1', axis=1)\ndf = df.drop('Unnamed: 8_level_1', axis=1)\ndf = df.drop('Unnamed: 9_level_1', axis=1)\ndf = df.drop('Unnamed: 10_level_1', axis=1)\ndf = df.drop('Unnamed: 12_level_1', axis=1)\ndf = df.drop('Unnamed: 13_level_1', axis=1)\ndf = df.drop('Unnamed: 14_level_1', axis=1)\ndf = df.drop('Unnamed: 15_level_1', axis=1)\ndf = df.drop('Unnamed: 16_level_1', axis=1)\ndf = df.drop('Unnamed: 17_level_1', axis=1)\ndf = df.drop('Unnamed: 18_level_1', axis=1)\ndf = df.drop('Unnamed: 19_level_1', axis=1)\ndf = df.drop('Unnamed: 20_level_1', axis=1)\ndf = df.drop('Unnamed: 21_level_1', axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_1188020",
  "file": "NB_1188020,df_test-1.ipynb",
  "row_id": 48,
  "target_code": "df_test = preprocess_data(df_test)\ndf_test.head()",
  "generation": "df_test = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Housing price predictor/test.csv\")\ndf_test['MSZoning'  ] = df_test['MSZoning'  ].fillna('RL')\ndf_test['LotFrontage'] = df_test['LotFrontage'].fillna(df_test['LotFrontage'].mean())\ndf_test['MasVnrArea'] = df_test['MasVnrArea'].fillna(df_test['MasVnrArea'].mean())\ndf_test['BsmtFinSF1'] = df_test['BsmtFinSF1'].fillna(df_test['BsmtFinSF1'].mean())\ndf_test['BsmtFinSF2'] = df_test['BsmtFinSF2'].fillna(df_test['BsmtFinSF2'].mean())\ndf_test['BsmtUnfSF' ] = df_test['BsmtUnfSF' ].fillna(df_test['BsmtUnfSF' ].mean())\ndf_test['TotalBsmtSF'] = df_test['TotalBsmtSF'].fillna(df_test['TotalBsmtSF'].mean())\ndf_test['BsmtFullBath'] = df_test['BsmtFullBath'].fillna(df_test['BsmtFullBath'].mean())\ndf_test['BsmtHalfBath'] = df_test['BsmtHalfBath'].fillna(df_test['BsmtHalfBath'].mean())\ndf_test['GarageYrBlt'] = df_test['GarageYrBlt'].fillna(df_test['GarageYrBlt'].mean())\ndf_test['GarageCars' ] = df_test['GarageCars' ].fillna(df_test['GarageCars' ].mean())\ndf_test['GarageArea' ] = df_test['GarageArea' ].fillna(df_test['GarageArea' ].mean())\ndf_test['SaleType'  ] = df_test['SaleType'  ].fillna('WD')\ndf_test['GarageFinish'] = df_test['GarageFinish'].fillna('Unf')\ndf_test['Utilities'  ] = df_test['Utilities'  ].fillna('AllPub')\ndf_test['Functional' ] = df_test['Functional' ].fillna('Typ')\ndf_test['KitchenQual'] = df_test['KitchenQual'].fillna('TA')\ndf_test['Exterior1st'] = df_test['Exterior1st'].fillna('VinylSd')\ndf_test['Exterior2nd'] = df_test['Exterior2nd'].fillna('VinylSd')\ndf_test['Electrical' ] = df_test['Electrical' ].fillna('SBrkr')\ndf_test.head()"
 },
 {
  "nbid": "NB_1215132",
  "file": "NB_1215132,tweets_por_locacion-1.ipynb",
  "row_id": 39,
  "target_code": "specified_location = (tweets_por_locacion.loc[:,'location'          ] != 'Not Especified'                 )\ntweets_por_locacion = tweets_por_locacion.loc[specified_location]\ntweets_por_locacion.head(15)",
  "generation": "tweets_por_locacion = tweets.groupby('location').agg({'id': 'count', 'target': 'sum', 'cuantity': 'sum', 'text lenght': 'sum'}).reset_index()\ntweets_por_locacion = tweets_por_locacion.sort_values('cuantity', ascending = True)\ntweets_por_locacion"
 },
 {
  "nbid": "NB_1215820",
  "file": "NB_1215820,df_scale-1.ipynb",
  "row_id": 15,
  "target_code": "from scipy.stats import zscore\ndf_scale=df_scale.apply(zscore)\ndf_scale.head()",
  "generation": "df_scale = df_scale.apply(zscore)\ndf_scale.head()"
 },
 {
  "nbid": "NB_1216050",
  "file": "NB_1216050,data-1.ipynb",
  "row_id": 7,
  "target_code": "print (\"Shape of dataset before cleaning: \"                                    , data.size)\ndata[[ 'sales'        , 'resale'         , 'type'       , 'price'        , 'engine_s'           ,\n       'horsepow'                 , 'wheelbas'           , 'width'        , 'length'         , 'curb_wgt'           , 'fuel_cap'           ,\n       'mpg'            , 'lnsales'          ]] = data[['sales'       , 'resale'         , 'type'       , 'price'        , 'engine_s'           ,\n       'horsepow'                 , 'wheelbas'           , 'width'        , 'length'         , 'curb_wgt'           , 'fuel_cap'           ,\n       'mpg'            , 'lnsales'          ]].apply(pd.to_numeric, errors='coerce'        )\ndata.dropna(inplace=True)\ndata = data.reset_index(drop=True)\nprint (\"Shape of dataset after cleaning: \"                                   , data.size)\ndata.head(5)",
  "generation": "data.dropna(inplace=True)\ndata.drop(['manufact', 'model', 'type', 'price', 'engine_s', 'fuel_cap', 'mpg'], axis=1, inplace=True)\ndata['resale'] = data['resale'].astype('float')\ndata['sales'] = data['sales'].astype('float')\ndata['lnsales'] = np.log(data['sales'])\ndata.head()"
 },
 {
  "nbid": "NB_1235700",
  "file": "NB_1235700,movie_data-1.ipynb",
  "row_id": 7,
  "target_code": "del_col = [ 'id'     , 'imdb_id'          , 'budget_adj'             , 'revenue_adj'              , 'homepage'           , 'keywords'           , 'overview'           , 'production_companies'                       , 'vote_count'             , 'vote_average'               ]\nmovie_data = movie_data.drop(del_col, 1)\nmovie_data.head(3)",
  "generation": "movie_data = movie_data.drop(['Unnamed: 0', 'cast', 'homepage', 'tagline', 'production_companies', 'vote_count', 'vote_average', 'budget_adj', 'revenue_adj'], axis=1)\nmovie_data['release_date'] = pd.to_datetime(movie_data['release_date'])\nmovie_data['release_year'] = movie_data['release_date'].dt.year\nmovie_data = movie_data.drop(['release_date'], axis=1)\nmovie_data = movie_data[['popularity', 'budget', 'revenue', 'original_title', 'director', 'runtime', 'genres', 'release_year', 'imdb_score']]\nmovie_data.head()"
 },
 {
  "nbid": "NB_1823834",
  "file": "NB_1823834,daily_week-1.ipynb",
  "row_id": 10,
  "target_code": "daily_week=daily_week.reset_index()\ndaily_week",
  "generation": "daily_week['apparentTemperatureMax'] = daily_week['apparentTemperatureMax'] / 7\ndaily_week['apparentTemperatureMin'] = daily_week['apparentTemperatureMin'] / 7\ndaily_week = daily_week.reset_index()\ndaily_week = daily_week[['year', 'week', 'energy_sum', 'apparentTemperatureMax', 'apparentTemperatureMin']]\ndaily_week.head()"
 },
 {
  "nbid": "NB_187905",
  "file": "NB_187905,emp-1.ipynb",
  "row_id": 3,
  "target_code": "emp['experience_level'                  ] = pd.cut(emp['experience'            ], \n                                 bins=[0, 5, 15, 100], \n                                 labels=['Novice'        , 'Experienced'              , 'Veteran'          ])\nemp.head()",
  "generation": "emp['experience_level'] = pd.cut(emp['experience'], bins=3, labels=['Novice', 'Experienced', 'Veteran'])\nemp.head()"
 },
 {
  "nbid": "NB_191952",
  "file": "NB_191952,nbaavg-1.ipynb",
  "row_id": 52,
  "target_code": "nbaavg = nbaavg.reset_index()\nnbaavg",
  "generation": "nbaavg.reset_index(inplace=True)\nnbaavg.head()"
 },
 {
  "nbid": "NB_5038549",
  "file": "NB_5038549,train-1.ipynb",
  "row_id": 5,
  "target_code": "train = pd.concat([train['Date'      ], train['Total'       ], train['Count'       ]], axis = 1)\ntrain.set_index('Date'      , inplace=True)\ntrain.head(50)",
  "generation": "train['Total'] = train.sum(axis=1)\ntrain = train[['Total', 'Count']]\ntrain.head()"
 },
 {
  "nbid": "NB_5076058",
  "file": "NB_5076058,df_covid-1.ipynb",
  "row_id": 3,
  "target_code": "for col in df_covid.columns: \n    print(col) \ndf_covid = df_covid.drop(columns = ['Unnamed: 0'            ])\ndf_covid.head()",
  "generation": "df_covid = df_covid.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\ndf_covid.head()"
 },
 {
  "nbid": "NB_5076058",
  "file": "NB_5076058,df_covid-1.ipynb",
  "row_id": 5,
  "target_code": "df_covid = df_covid.reset_index(drop = True)\ndf_covid.head()",
  "generation": "df_covid = df_covid.drop(columns = ['Unnamed: 0'])\ndf_covid = df_covid.set_index('Country,Other')\ndf_covid.head()"
 },
 {
  "nbid": "NB_5079564",
  "file": "NB_5079564,features_matrix-1.ipynb",
  "row_id": 12,
  "target_code": "features_matrix = features_matrix.drop(['Cabin'       ], axis=1)\nfeatures_matrix",
  "generation": "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\nimp.fit(features_matrix[['Age']])\nfeatures_matrix[['Age']] = imp.transform(features_matrix[['Age']])\nfeatures_matrix.drop(['Cabin'], axis=1, inplace=True)\nfeatures_matrix.head()"
 },
 {
  "nbid": "NB_5083732",
  "file": "NB_5083732,df-1.ipynb",
  "row_id": 5,
  "target_code": "for col in df.columns:\n    if 'M'     not in col and col != 'Indicator'            :\n        df = df.drop(col, axis=1)\ndf.head(10)",
  "generation": "df = df.drop(['Unnamed: 1', 'Scale'], axis=1)\ndf = df.set_index('Indicator').T.reset_index().rename(columns={'index': 'date'})\ndf.head()"
 },
 {
  "nbid": "NB_5086629",
  "file": "NB_5086629,data-1.ipynb",
  "row_id": 9,
  "target_code": "data['continent'           ] = data['continent'           ].map({'North America'               :0,'Europe'        :1,'Asia'      :2,'Africa'        :3,'South America'               :4, 'Oceania'          :5,'Seven seas (open ocean)'                         :6, 'Antarctica'             :7})\ndata.head()",
  "generation": "continent_dict = {'North America': 0, 'Europe': 1, 'Asia': 2, 'Africa': 3, 'Oceania': 4, 'South America': 5, 'Antarctica': 6}\ndata['continent'] = data['continent'].map(continent_dict)\ndata_with_clusters = data\ndata_with_clusters['Cluster'] = identified_clusters\ndata_with_clusters.head()"
 },
 {
  "nbid": "NB_1251945",
  "file": "NB_1251945,train_df,y_train-1.ipynb",
  "row_id": 83,
  "target_code": "train_df = pd.get_dummies(train_df, columns=[\"Embarked\"          ])\ntrain_df.head()",
  "generation": "test_df[\"Sex\"] = [1 if i == \"male\" else 0 for i in test_df[\"Sex\"]]\ntest_df[\"Title\"] = test_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_df[\"Title\"] = test_df[\"Title\"].replace([\"Don\",\"Rev\",\"Dr\",\"Major\",\"Lady\",\"Sir\",\"Col\",\"Capt\",\"the Countess\",\"Jonkheer\",\"Dona\"],\"other\")\ntest_df[\"Title\"] = [0 if i == \"Master\" else 1 if i == \"Miss\" or i == \"Ms\" or i == \"Mlle\" or i == \"Mrs\" else 2 if i == \"Mr\" else 3 for i in test_df[\"Title\"]]\ntest_df = pd.get_dummies(test_df,columns=[\"Title\"])\ntest_df[\"family_size\"] = [1 if i < 5 else 0 for i in test_df[\"Parch\"] + test_df[\"SibSp\"] + 1]\ntest_df = pd.get_dummies(test_df, columns= [\"family_size\"])\ntest_df[\"Embarked_C\"] = [1 if i == \"C\" else 0 for i in test_df[\"Embarked\"]]\ntest_df[\"Embarked_Q\"] = [1 if i == \"Q\" else 0 for i in test_df[\"Embarked\"]]\ntest_df[\"Embarked_S\"] = [1 if i == \"S\" else 0 for i in test_df[\"Embarked\"]]\ntest_df = test_df.drop([\"Cabin\"],axis=1)\ntest_df = test_df.drop([\"Embarked\"],axis=1)\ntest_df = test_df.drop([\"Ticket\"],axis=1)\ntest_df = test_df.drop([\"Name\"],axis=1)\ntest_df = test_df.drop([\"PassengerId\"],axis=1)\ntest_df.head()"
 },
 {
  "nbid": "NB_1251945",
  "file": "NB_1251945,train_df,y_train-1.ipynb",
  "row_id": 88,
  "target_code": "train_df = pd.get_dummies(train_df, columns= [\"Ticket\"        ], prefix = \"T\"    )\ntrain_df.head(10)",
  "generation": "test_df = pd.get_dummies(test_df, columns=[\"Title\"])\ntest_df = pd.get_dummies(test_df, columns=[\"Embarked\"])\ntest_df[\"family_size\"] = [1 if i < 5 else 0 for i in test_df[\"Fsize\"]]\ntest_df = pd.get_dummies(test_df, columns= [\"family_size\"])"
 },
 {
  "nbid": "NB_1254165",
  "file": "NB_1254165,results_2019-1.ipynb",
  "row_id": 3,
  "target_code": "results_2019 = results_2019[['AgeOnRaceDay'              , 'Gender'         , 'CountryOfResName'                   , 'OfficialTime'               , 'RankOverall'              ]]\nresults_2019.Gender[results_2019.Gender == 'M'    ] = 1\nresults_2019.Gender[results_2019.Gender == 'F'    ] = 0\nresults_2019.head(2)",
  "generation": "results_2019 = results_2019[['AgeOnRaceDay', 'Gender', 'CountryOfResName', 'OfficialTime', 'RankOverall']]\nresults_2019.head(2)"
 },
 {
  "nbid": "NB_1255357",
  "file": "NB_1255357,df_dummy,no_internet,T,C-1.ipynb",
  "row_id": 21,
  "target_code": "df_dummy = df_dummy[df_dummy.columns.drop(list(df_dummy.filter(regex='_No internet service'                      )))]\ndf_dummy = df_dummy[df_dummy.columns.drop(list(df_dummy.filter(regex='_No phone service'                   )))]\ndf_dummy.head(10)",
  "generation": "df_dummy.drop(['MultipleLines_No phone service', 'OnlineSecurity_No internet service', 'OnlineBackup_No internet service', 'DeviceProtection_No internet service', 'TechSupport_No internet service', 'StreamingTV_No internet service', 'StreamingMovies_No internet service'], axis=1, inplace=True)\ndf_dummy.fillna(0, inplace=True)\ndf_dummy"
 },
 {
  "nbid": "NB_1273974",
  "file": "NB_1273974,df,norm_mgp,norm_pgp,norm_dgp,ax-1.ipynb",
  "row_id": 4,
  "target_code": "df = df[df.loc[:,'AIRPORT_COUNTRY_NAME'                      ] == 'United States'                ]\ndf.tail(7)",
  "generation": "df = df.loc[df['AIRPORT_COUNTRY_NAME'] == 'United States']\ndf"
 },
 {
  "nbid": "NB_1273974",
  "file": "NB_1273974,df,norm_mgp,norm_pgp,norm_dgp,ax-1.ipynb",
  "row_id": 6,
  "target_code": "df = df.groupby(['AIRPORT'         ,'DISPLAY_AIRPORT_CITY_NAME_FULL'                                ]).mean()\ndf.tail(6)",
  "generation": "df = df[df.loc[:,'AIRPORT_COUNTRY_NAME'] == 'United States']\ndf = df.groupby('AIRPORT').agg({'DISPLAY_AIRPORT_CITY_NAME_FULL': 'first', 'LATITUDE': 'mean', 'LONGITUDE': 'mean'}).reset_index()\ndf.rename(columns={\"AIRPORT\": \"Unnamed: 0_level_0\", \"DISPLAY_AIRPORT_CITY_NAME_FULL\": \"DISPLAY_AIRPORT_CITY_NAME_FULL\", \"LATITUDE\": \"LATITUDE\", \"LONGITUDE\": \"LONGITUDE\", \"Unnamed: 7\": \"Unnamed: 7\", \"AIRPORT_COUNTRY_NAME\": \"Unnamed: 1_level_0\", \"AIRPORT_STATE_NAME\": \"Unnamed: 2_level_1\", \"Unnamed: 3_level_1\": \"Unnamed: 3_level_1\", \"Unnamed: 4_level_1\": \"Unnamed: 4_level_1\"}, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_1279906",
  "file": "NB_1279906,df,ndf-1.ipynb",
  "row_id": 119,
  "target_code": "import statistics\ndf['age'     ] = ((df['age'     ]-df['age'     ].mean())/df['age'     ].mean())\ndf['trestbps'          ] = ((df['trestbps'          ]-df['trestbps'          ].mean())/df['trestbps'          ].mean())\ndf['chol'      ] = ((df['chol'      ]-df['chol'      ].mean())/df['chol'      ].mean())\ndf['thalach'         ] = ((df['thalach'         ]-df['thalach'         ].mean())/df['thalach'         ].mean())\ndf.head()",
  "generation": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']] = scaler.fit_transform(df[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']])\ndf.head()"
 },
 {
  "nbid": "NB_1287102",
  "file": "NB_1287102,cats-1.ipynb",
  "row_id": 9,
  "target_code": "cats=cats.drop(columns=['color1'        ])\ncats['spayed_neutered'                 ] = sex[0]\ncats=cats.dropna() \ncats=cats.drop(columns=['outcome'         ])\ncats",
  "generation": "# Label encode categorical variables\nle = LabelEncoder()\ncats['animal_type'] = le.fit_transform(cats['animal_type'])\ncats['breed'] = le.fit_transform(cats['breed'])\ncats['color'] = le.fit_transform(cats['color'])\ncats['spayed_neutered'] = le.fit_transform(cats['spayed_neutered'])\ncats['sex'] = le.fit_transform(cats['sex'])\ncats['outcome'] = le.fit_transform(cats['outcome'])\n\n# Standardize numerical variables\nscaler = StandardScaler()\ncats[['age', 'dob_month', 'dob_day', 'dob_dayofweek', 'month', 'day', 'dayofweek', 'hour', 'quarter']] = scaler.fit_transform(cats[['age', 'dob_month', 'dob_day', 'dob_dayofweek', 'month', 'day', 'dayofweek', 'hour', 'quarter']])\n\n# Drop unnecessary columns\ncats.drop(columns=['breed1', 'color1'], inplace=True)\n\n# Set index to Unnamed: 0\ncats.set_index('Unnamed: 0', inplace=True)\n\ncats.head()"
 },
 {
  "nbid": "NB_1289471",
  "file": "NB_1289471,df-1.ipynb",
  "row_id": 37,
  "target_code": "df.loc[df.exact_punish.str.contains('Jahr'      , na=False), 'cleaned_number_in_months'                           ] = df.number_punish.mul(12)\ndf.loc[df.exact_punish.str.contains('Tag'     , na=False), 'cleaned_number_in_months'                           ] = df.number_punish.div(30)\ndf.loc[df.exact_punish.str.contains('Monat'       , na=False), 'cleaned_number_in_months'                           ] = df.number_punish\ndf.head()",
  "generation": "df['punish'] = df.judgement2.str.extract(r'(\\d+ \\d*/\\d* Jahr)')\ndf['punish1'] = df.judgement2.str.extract(r'(\\d+ \\d*/\\d* Jahren)')\ndf['exact_punish'] = df.judgement2.str.extract(r'(\\d+ \\d*/\\d* Monaten)')\ndf['exact_punish1'] = df.judgement2.str.extract(r'(\\d+ \\d*/\\d* Monate)')\ndf['number_punish'] = df.exact_punish.str.extract(r'(\\d+)')\ndf['number_punish1'] = df.exact_punish1.str.extract(r'(\\d+)')\ndf['cleaned_number'] = df.number_punish.fillna(df.number_punish1)\ndf['cleaned_number_in_months'] = df.cleaned_number.astype(float) * 12\ndf['cleaned_number_in_months1'] = df.exact_punish.str.extract(r'(\\d+)').astype(float) * 12"
 },
 {
  "nbid": "NB_1300030",
  "file": "NB_1300030,tips-1.ipynb",
  "row_id": 102,
  "target_code": "tips = tips.filter('total_bill\ttip\tsex\tsmoker\tday\ttime\tsize'                                         .split())\ntips.head(3)",
  "generation": "tips = tips[['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']]\ntips.head(3)"
 },
 {
  "nbid": "NB_1300030",
  "file": "NB_1300030,tips-1.ipynb",
  "row_id": 105,
  "target_code": "tips['cost'      ] = tips['total_bill'            ] + tips['tip'     ]\ntips.head(3)",
  "generation": "tips['day_type'] = tips['day'].apply(lambda x: 'Weekend' if x in ['Sat', 'Sun'] else 'Weekday')\ntips['cost'] = tips['total_bill'] + tips['tip']\ntips.head()"
 },
 {
  "nbid": "NB_1300814",
  "file": "NB_1300814,chicago_community-1.ipynb",
  "row_id": 4,
  "target_code": "chicago_community = chicago_community.transpose().loc['1'   :'77'    , [0, 1, 2]].reset_index(drop=True)\nchicago_community.columns = ['Community'           , 'Population'             , 'Income'         ]\nchicago_community.head()",
  "generation": "chicago_community = pd.DataFrame({'Community': chicago_onehot['Community'].unique()})\nchicago_community[['Population', 'Income']] = latlng[['Population', 'Income']]\nchicago_community.head()"
 },
 {
  "nbid": "NB_1305239",
  "file": "NB_1305239,df_venues_count-1.ipynb",
  "row_id": 84,
  "target_code": "df_venues_count = df_venues_count.pivot(index='cinema_name'             , columns='category'          , values='count'       ).fillna(0)\ndf_venues_count.head()",
  "generation": "df_venues_count = df_venues_count.pivot(index='cinema_name', columns='category', values='count')\ndf_venues_count.head()"
 },
 {
  "nbid": "NB_1321391",
  "file": "NB_1321391,df_melt-1.ipynb",
  "row_id": 77,
  "target_code": "df_melt['year'      ] = df_melt['variable'          ].str.extract(r'([0-9]{4})'             ).astype('int'     )\ndf_melt.head()",
  "generation": "df_melt['year'] = df_melt['variable'].str.extract(r'(\\d+)').astype(int)\ndf_melt.head()"
 },
 {
  "nbid": "NB_1322316",
  "file": "NB_1322316,tips-1.ipynb",
  "row_id": 38,
  "target_code": "tips['tip_pct'         ] = tips['tip'     ]/(tips['total_bill'            ]-tips['tip'     ])\ntips.head()",
  "generation": "tips['tip_pct'] = tips['tip'] / tips['total_bill']\ntips.head()"
 },
 {
  "nbid": "NB_1336489",
  "file": "NB_1336489,data,hatred_words-1.ipynb",
  "row_id": 7,
  "target_code": "data.tweet = data.tweet.str.replace('@[a-zA-Z0-9_]+'                , ''   )\ndata.tweet = data.tweet.str.replace('#[a-zA-Z0-9_]+'                , ''   )\ndata.tweet = data.tweet.str.replace('[^a-zA-Z]'           , ' '    )\ndata.head()",
  "generation": "data['tweet'] = data['tweet'].str.replace('[^a-zA-Z\\s]', '')\ndata['tweet'] = data['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\ndata['tweet'] = data['tweet'].apply(lambda x: ' '.join([lemma.lemmatize(word) for word in x.split()]))\ndata.drop(['id', 'hashtags', 'mentions'], axis=1, inplace=True)\ndata"
 },
 {
  "nbid": "NB_1336803",
  "file": "NB_1336803,df-1.ipynb",
  "row_id": 11,
  "target_code": "df = pandas.concat([df, df3], axis=1)\ndf.tail()",
  "generation": "scaler = MinMaxScaler()\ndf_temp = df[['B_avg_BODY_att', 'B_avg_BODY_landed', 'B_avg_CLINCH_att', 'B_avg_CLINCH_landed']]\ndf_temp = scaler.fit_transform(df_temp)\ndf_temp = pandas.DataFrame(df_temp, columns=['B_avg_BODY_att', 'B_avg_BODY_landed', 'B_avg_CLINCH_att', 'B_avg_CLINCH_landed'])\ndf1 = pandas.DataFrame(df_temp, columns=['B_avg_BODY_att', 'B_avg_BODY_landed', 'B_avg_CLINCH_att', 'B_avg_CLINCH_landed'])\ndf1[i] = df_temp[i]\ndf2 = pandas.DataFrame(lis, columns=keys).astype('float')\ndf3 = pandas.DataFrame((df[B_status].to_numpy() - df[R_status].to_numpy()), columns=new_key)\ndf = df.drop(columns = [\"index\"])\ndf.title_bout = df.title_bout.astype(int)\ndf[\"Winner\"] = df[\"Winner\"].replace({\"Red\": 1, \"Blue\": 2})\ndf = df[df[\"Winner\"] != \"Draw\"].reset_index(drop=True)\ntemp_df = df\ndrop_list = [\"R_Stance\", \"B_Stance\", \"weight_class\"]\ndf = (pandas.concat([df, df1,df2], axis=1)).drop(columns = drop_list)\ndf = df.astype('float')\ndf = pandas.concat([df, df3], axis=1)"
 },
 {
  "nbid": "NB_1337879",
  "file": "NB_1337879,tmdb_df-1.ipynb",
  "row_id": 12,
  "target_code": "tmdb_df = tmdb_df.drop(['id'    , 'imdb_id'          , 'budget'         , 'revenue'          , \n                        'original_title'                                        , 'cast'       , 'homepage'           ,\n                        'director'                                  , 'tagline'          , 'keywords'           , \n                        'overview'                                  , 'runtime'          , 'production_companies'                       ,\n                        'release_date'                                      , 'vote_count'             , 'vote_average'               ], axis = 1)\ntmdb_df.head(20)",
  "generation": "# Selecting only the columns that are relevant to the questions\ntmdb_df = tmdb_df[['popularity', 'genres', 'budget', 'revenue', 'release_year']]\n\n# Removing rows with missing data\ntmdb_df.dropna(inplace=True)\n\n# Splitting the genres column into multiple columns\ndm = tmdb_df['genres'].str.get_dummies(sep='|')\n\n# Joining the new columns to the original dataframe\ntmdb_df = tmdb_df.join(dm)\n\n# Grouping by genres and release year, and calculating the mean popularity\npopularity_by_genre_year = tmdb_df.groupby(['genres', 'release_year'])['popularity'].mean()\n\n# Grouping by genres and calculating the mean budget and revenue\nbudget_revenue_by_genre = tmdb_df.groupby('genres')[['budget', 'revenue']].mean()\n\n# Creating a new dataframe with the relevant columns\ndf = pd.DataFrame({\n    'popularity': popularity_by_genre_year.values,\n    'genres': [elem[0] for elem in popularity_by_genre_year.index],\n    'release_year': [elem[1] for elem in popularity_by_genre_year.index],\n    'budget': budget_revenue_by_genre['budget'].values,\n    'revenue': budget_revenue_by_genre['revenue'].values,\n})\n\n# Selecting only the relevant columns\ndf = df[['popularity', 'genres', 'release_year', 'budget', 'revenue']]\n\n# Removing rows with missing data\ndf.dropna(inplace=True)\n\n# Converting release year to integer\ndf['release_year'] = df['release_year'].astype(int)\n\n# Displaying the first few rows of the dataframe\ndf.head(2)"
 },
 {
  "nbid": "NB_1337879",
  "file": "NB_1337879,tmdb_df-1.ipynb",
  "row_id": 19,
  "target_code": "tmdb_df = pd.concat([tmdb_df, dm], axis = 1)\ntmdb_df = tmdb_df.drop(['genres'        ], axis = 1)\ntmdb_df.head()",
  "generation": "tmdb_df = pd.concat([tmdb_df, dm], axis=1)\ntmdb_df = tmdb_df.drop('genres', axis=1)"
 },
 {
  "nbid": "NB_1337879",
  "file": "NB_1337879,tmdb_melt_df-1.ipynb",
  "row_id": 25,
  "target_code": "tmdb_melt_df = tmdb_melt_df[tmdb_melt_df.is_true == 1]\ntmdb_melt_df = tmdb_melt_df.drop(['is_true'         ], axis = 1)\ntmdb_melt_df.head(10)",
  "generation": "tmdb_melt_df = tmdb_melt_df[tmdb_melt_df['is_true'] == 1]\ntmdb_melt_df = tmdb_melt_df.drop(['is_true'], axis=1)\ntmdb_melt_df.head()"
 },
 {
  "nbid": "NB_1340180",
  "file": "NB_1340180,mail,test_data-1.ipynb",
  "row_id": 7,
  "target_code": "mail['label'       ] = mail['label'       ].map({'spam'      : 1, 'ham'      : 0})\nmail.head()",
  "generation": "mail['label'] = mail['v1'].map({'ham': 0, 'spam': 1})\nmail = mail[['label', 'v2']]\nmail.columns = ['label', 'message']\nmail.head()"
 },
 {
  "nbid": "NB_1351572",
  "file": "NB_1351572,df,df_non_profit,df_nsp-1.ipynb",
  "row_id": 26,
  "target_code": "df = df[~df['info_traffic_peer_participants'                                ].isin(['Not Disclosed'               , 0, ''   ])]\ndf",
  "generation": "xdf = df[df['info_traffic_peer_participants'] != '']"
 },
 {
  "nbid": "NB_1356952",
  "file": "NB_1356952,dataset-1.ipynb",
  "row_id": 28,
  "target_code": "dataset = dataset.loc[:, dataset.columns != 'name'       ]\ndataset.head(10)",
  "generation": "dataset = dataset.drop('name', axis=1)\ndataset.head()"
 },
 {
  "nbid": "NB_199030",
  "file": "NB_199030,df-1.ipynb",
  "row_id": 24,
  "target_code": "df.areastring=df.areastring.map(lambda x: x.split('.'   )[-1])\ndf.head()",
  "generation": "df['areastring'] = df['Unnamed: 0'].map(lambda x: x.split('.')[1])\ndf.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_205955",
  "file": "NB_205955,salary,max_coun-1.ipynb",
  "row_id": 18,
  "target_code": "salary=salary.merge(rates,left_on='CompensationCurrency'                      ,right_on='originCountry'               ,how='left'      )  \nsalary.head()",
  "generation": "salary['originCountry'] = salary['CompensationCurrency']\nsalary['exchangeRate'] = salary['CompensationCurrency'].map(rates.set_index('originCurrency')['exchangeRate'])\nsalary.head()"
 },
 {
  "nbid": "NB_208387",
  "file": "NB_208387,dt-1.ipynb",
  "row_id": 4,
  "target_code": "dt = pd.get_dummies(dt, drop_first=True)\ndt.head()",
  "generation": "dt = pd.get_dummies(dt, columns=['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])\ndt = dt.rename(columns={'age': 'age', 'trestbps': 'resting_blood_pressure', 'chol': 'cholesterol', 'thalach': 'max_heart_rate_achieved', 'oldpeak': 'st_depression', 'num': 'num_major_vessels', 'target': 'target'})\ndt.head()"
 },
 {
  "nbid": "NB_5126751",
  "file": "NB_5126751,star_wars,females-1.ipynb",
  "row_id": 9,
  "target_code": "star_wars = star_wars.rename(columns={\n        \"Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.\"                                                                                                                                                       : \"ranking_1\"            ,\n        \"Unnamed: 10\"                     : \"ranking_2\"            ,\n        \"Unnamed: 11\"                     : \"ranking_3\"            ,\n        \"Unnamed: 12\"                     : \"ranking_4\"            ,\n        \"Unnamed: 13\"                     : \"ranking_5\"            ,\n        \"Unnamed: 14\"                     : \"ranking_6\"            \n        })\nstar_wars.head(1)",
  "generation": "star_wars = star_wars.rename(columns={\n        \"Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.\": \"ranking_1\"\n    })\nstar_wars.head(1)"
 },
 {
  "nbid": "NB_5134454",
  "file": "NB_5134454,df-1.ipynb",
  "row_id": 7,
  "target_code": "df['d'   ] = df['Incongruent'             ] - df['Congruent'           ]\ndf.head()",
  "generation": "df['d'] = df['Incongruent'] - df['Congruent']\ndf.head()"
 },
 {
  "nbid": "NB_5148741",
  "file": "NB_5148741,dados,y-1.ipynb",
  "row_id": 2,
  "target_code": "a_renomear = {\n    'mileage_per_year'                      : 'milhas_por_ano'                 ,\n    'model_year'                :'ano_do_modelo'               ,\n    'price'           : 'preco'        ,\n    'sold'          :'vendido'         \n}\ndados = dados.rename(columns = a_renomear)\ndados.head()",
  "generation": "dados = dados.rename(columns={'mileage_per_year': 'milhas_por_ano', 'model_year': 'ano_do_modelo', 'price': 'preco', 'sold': 'vendido'})"
 },
 {
  "nbid": "NB_5148741",
  "file": "NB_5148741,dados,y-1.ipynb",
  "row_id": 4,
  "target_code": "from datetime import datetime\nano_atual = datetime.today().year\ndados['idade_do_modelo'                 ] = ano_atual - dados.ano_do_modelo \ndados.head()",
  "generation": "ano_atual = datetime.today().year\ndados['idade_do_modelo'] = ano_atual - dados.ano_do_modelo\ndados.head()"
 },
 {
  "nbid": "NB_5148741",
  "file": "NB_5148741,dados,y-1.ipynb",
  "row_id": 6,
  "target_code": "dados = dados.drop(columns = ['Unnamed: 0'            , 'milhas_por_ano'                 ,'ano_do_modelo'               ], axis =1)\ndados.head()",
  "generation": "dados = dados[['preco', 'vendido', 'idade_do_modelo', 'km_por_ano']]\ndados.head()"
 },
 {
  "nbid": "NB_5168659",
  "file": "NB_5168659,dados,y-1.ipynb",
  "row_id": 3,
  "target_code": "a_renomear = {\n    'mileage_per_year'                      : 'milhas_por_ano'                 ,\n    'model_year'                : 'ano_do_modelo'                ,\n    'price'           : 'preco'        ,\n    'sold'          : 'vendido'          \n}\ndados = dados.rename(columns = a_renomear)\ndados.head()",
  "generation": "dados.columns = ['Unnamed: 0', 'Unnamed: 0.1', 'milhas_por_ano', 'ano_do_modelo', 'preco', 'vendido']\ndados.head()"
 },
 {
  "nbid": "NB_5168659",
  "file": "NB_5168659,dados,y-1.ipynb",
  "row_id": 4,
  "target_code": "mapeamento = {\n    'yes'         : 1,\n    'no'        : 0\n}\ndados.vendido = dados.vendido.map(mapeamento)\ndados.head()",
  "generation": "dados['vendido'] = dados['vendido'].map({'yes': 1, 'no': 0})\ndados = dados.rename(columns={'mileage_per_year': 'milhas_por_ano', 'model_year': 'ano_do_modelo', 'price': 'preco', 'sold': 'vendido'})\ndados"
 },
 {
  "nbid": "NB_5168659",
  "file": "NB_5168659,dados,y-1.ipynb",
  "row_id": 7,
  "target_code": "dados = dados.drop(columns = [\"Unnamed: 0\"            , \"milhas_por_ano\"                 , \"ano_do_modelo\"                ], axis = 1)\ndados.head()",
  "generation": "dados = dados.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'milhas_por_ano', 'ano_do_modelo'])\ndados = dados[['preco', 'vendido', 'idade_do_modelo', 'km_por_ano']]\ndados.head()"
 },
 {
  "nbid": "NB_5175399",
  "file": "NB_5175399,graph_df-1.ipynb",
  "row_id": 4,
  "target_code": "graph_df.rename(columns={'index'       :'Household income'                  }, inplace=True)\ngraph_df['order'       ] = [0, 5, 6, 3, 4, 7, 2, 1]\ngraph_df.sort_values('order'       , inplace=True)\ngraph_df.reset_index(inplace=True)\ngraph_df.drop('index'       , axis=1, inplace=True)\ngraph_df.drop('order'       , axis=1, inplace=True)\ngraph_df = graph_df[['Household income'                  , '12 to 14'           , '15 to 17'           , '18 to 20'           , '21 to 24'           , '25 to 34'           , '35 to 49'           , '50 to 64'           , '65 or older'              ]]\ngraph_df",
  "generation": "df = pd.read_csv('../../clean_data/clean_victimization_data.csv')\ngraph_df = pd.DataFrame()\n\nfor age in df['Age'].unique():\n    age_df = df[df['Age'] == f\"{age}\"]\n    age_series = age_df['Household income'].value_counts()\n    by_income = age_series.to_frame().reset_index()\n    by_income.rename(columns={by_income.columns[1]:f\"{age}\"}, inplace=True)\n    \n    if graph_df.empty:\n        graph_df = by_income\n    else:\n        graph_df = graph_df.merge(by_income, on='index')\n\ngraph_df = graph_df.sort_values(by='index')\ngraph_df = graph_df.reset_index(drop=True)\ngraph_df.rename(columns={'index':'Household income'}, inplace=True)\ngraph_df.head(5)"
 },
 {
  "nbid": "NB_5190079",
  "file": "NB_5190079,vds-1.ipynb",
  "row_id": 22,
  "target_code": "import nltk \nfrom nltk.corpus import stopwords\nstop = stopwords.words(\"english\"         )\nvds['Text'      ] = vds['Text'      ].apply(lambda x: \" \"    .join([word for word in x.split() if word not in (stop)]))\nvds.head()",
  "generation": "stop_words = set(stopwords.words('english'))\nps = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\nvds['Text'] = vds['Text'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() if word not in stop_words]))\nvds['Text'] = vds['Text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split() if word not in stop_words]))\nvds.head()"
 },
 {
  "nbid": "NB_5190079",
  "file": "NB_5190079,vds-1.ipynb",
  "row_id": 23,
  "target_code": "from nltk.stem import PorterStemmer\nps = PorterStemmer()\nvds['Text'      ] = vds['Text'      ].apply(lambda x : ' '    .join([ps.stem(word) for word in x.split()]))\nvds.head()",
  "generation": "ps = PorterStemmer()\nvds['Text'] = vds['Text'].apply(lambda x: \" \".join([ps.stem(word) for word in x.split()]))\nvds.head()"
 },
 {
  "nbid": "NB_5195555",
  "file": "NB_5195555,df-1.ipynb",
  "row_id": 3,
  "target_code": "df[\"time\"      ] = pd.date_range(\"1880\"      , periods=len(df), freq=\"MS\"    )\ndf = df.set_index(\"time\"      )\ndel df[\"Year+Month\"            ]\ndf.tail()",
  "generation": "df = df.drop(['Station'], axis=1)\ndf.columns = ['time', 'Land+Ocean']\ndf['time'] = pd.to_datetime(df['time'], format='%Y.%m')\ndf.set_index('time', inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_1360560",
  "file": "NB_1360560,mails-1.ipynb",
  "row_id": 2,
  "target_code": "mails.dropna(axis = 1,inplace = True)\nmails.rename(index = str , columns = {'v2'    :'message'         ,'v1'    :'label'       },inplace = True)\nmails = mails[['message'         ,'label'       ]]\nmails.head()",
  "generation": "mails = mails[['v2', 'v1']]\nmails.columns = ['message', 'label']\nmails.head()"
 },
 {
  "nbid": "NB_1373313",
  "file": "NB_1373313,df-1.ipynb",
  "row_id": 3,
  "target_code": "df['Date'      ] = pd.to_datetime(df['Date/Time'           ])\ndf.head()",
  "generation": "df['Date'] = pd.to_datetime(df['Date/Time'])\ndf.head()"
 },
 {
  "nbid": "NB_1373313",
  "file": "NB_1373313,df-1.ipynb",
  "row_id": 5,
  "target_code": "df['hour'      ] = df['Date'      ].dt.hour\ndf.head()",
  "generation": "df['hour'] = df['Date'].dt.hour\ndf.head()"
 },
 {
  "nbid": "NB_1373313",
  "file": "NB_1373313,df-1.ipynb",
  "row_id": 6,
  "target_code": "df['day'     ] = df['Date'      ].dt.day\ndf.head()",
  "generation": "df['day'] = df['Date'].dt.day\ndf.head()"
 },
 {
  "nbid": "NB_1373313",
  "file": "NB_1373313,df-1.ipynb",
  "row_id": 7,
  "target_code": "df['month'       ] = df['Date'      ].dt.month\ndf.head()",
  "generation": "df['month'] = df['Date'].dt.month\ndf.head()"
 },
 {
  "nbid": "NB_1383403",
  "file": "NB_1383403,df-1.ipynb",
  "row_id": 9,
  "target_code": "df = df.merge(df_census, how='left'      , on='zip_code'          ) \ndf = df.dropna() \ndf.head()",
  "generation": "df = df_top.merge(df_zipcode, how='left', on='STATION')\ndf = df.merge(df_census, how='left', on='zip_code')\ndf.head()"
 },
 {
  "nbid": "NB_1393489",
  "file": "NB_1393489,df-1.ipynb",
  "row_id": 10,
  "target_code": "df = df.set_index('Year'      )\ndf.head()",
  "generation": "df = df.drop(['Year', 'Unnamed: 1_level_1', 'Unnamed: 2_level_1', 'Unnamed: 3_level_1', 'Unnamed: 4_level_1', 'Unnamed: 5_level_1', 'Unnamed: 6_level_1', 'Unnamed: 7_level_1', 'Unnamed: 8_level_1', 'Unnamed: 9_level_1', 'Unnamed: 10_level_1', 'Unnamed: 12_level_1', 'Unnamed: 13_level_1', 'Unnamed: 14_level_1', 'Unnamed: 15_level_1', 'Unnamed: 16_level_1', 'Unnamed: 17_level_1', 'Unnamed: 18_level_1', 'Unnamed: 19_level_1', 'Unnamed: 20_level_1', 'Unnamed: 21_level_1'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_1409789",
  "file": "NB_1409789,df,urban-1.ipynb",
  "row_id": 10,
  "target_code": "columns = [\n 'EVENT_DATE'             ,\n 'EVENT_TYPE'             ,\n 'ACTOR1'         ,\n 'ALLY_ACTOR_1'               ,\n 'INTER1'         ,\n 'ACTOR2'         ,\n 'ALLY_ACTOR_2'               ,\n 'INTER2'         ,\n 'INTERACTION'              ,\n 'COUNTRY'          ,\n 'LOCATION'           ,\n 'LATITUDE'           ,\n 'LONGITUDE'            ,\n 'SOURCE'         ,\n 'NOTES'        ,\n 'FATALITIES'             ]\ndf = df[df.COUNTRY=='South Africa'              ][columns]\ndf.head(3)",
  "generation": "df = df[df['COUNTRY'] == 'South Africa']\ndf = df[['EVENT_DATE', 'EVENT_TYPE', 'ACTOR1', 'ALLY_ACTOR_1', 'INTER1', 'ACTOR2', 'ALLY_ACTOR_2', 'INTER2', 'INTERACTION', 'COUNTRY', 'LOCATION', 'LATITUDE', 'LONGITUDE', 'SOURCE', 'NOTES', 'FATALITIES']]\ndf['INTER1'] = df['INTER1'].replace({1: 'Government', 2: 'Rebel', 3: 'Political Militia', 4: 'Identity Militia', 5: 'Communal Militia', 6: 'Protesters', 7: 'Civilians'})\ndf['INTER2'] = df['INTER2'].replace({1: 'Government', 2: 'Rebel', 3: 'Political Militia', 4: 'Identity Militia', 5: 'Communal Militia', 6: 'Protesters', 7: 'Civilians'})\ndf.columns = ['Unnamed: 0', 'EVENT_DATE', 'EVENT_TYPE', 'ACTOR1', 'ALLY_ACTOR_1', 'INTER1', 'ACTOR2', 'ALLY_ACTOR_2', 'INTER2', 'INTERACTION', 'COUNTRY', 'LOCATION', 'LATITUDE', 'LONGITUDE', 'SOURCE', 'NOTES', 'FATALITIES']\ndf.head(2)"
 },
 {
  "nbid": "NB_1409789",
  "file": "NB_1409789,df,urban-1.ipynb",
  "row_id": 14,
  "target_code": "column_names = {\n    'EVENT_DATE'                : 'Date'       ,\n    'EVENT_TYPE'                : 'Event Type'             ,\n    'ACTOR1'            : 'Actor 1'          ,\n    'ALLY_ACTOR_1'                  : 'Actor 1 Ally'               ,\n    'INTER1'            : 'Actor 1 Type'               ,\n    'ACTOR2'            : 'Actor 2'          ,\n    'ALLY_ACTOR_2'                  : 'Actor 2 Ally'               ,\n    'INTER2'            : 'Actor 2 Type'               ,\n    'INTERACTION'                 : 'Interaction Type'                   ,\n    'COUNTRY'             : 'Country'          ,\n    'LOCATION'              : 'Location'           ,\n    'LATITUDE'              : 'Latitude'           ,\n    'LONGITUDE'               : 'Longitude'            ,\n    'SOURCE'            : 'Source'         ,\n    'NOTES'           : 'Notes'        ,\n    'FATALITIES'                : 'Fatalities'             }\ndf = df.rename(columns = column_names)\ndf.head(3)",
  "generation": "df.rename(columns={'EVENT_DATE': 'Date',\n                   'EVENT_TYPE': 'Event Type',\n                   'ACTOR1': 'Actor 1',\n                   'ALLY_ACTOR_1': 'Actor 1 Ally',\n                   'INTER1': 'Actor 1 Type',\n                   'ACTOR2': 'Actor 2',\n                   'ALLY_ACTOR_2': 'Actor 2 Ally',\n                   'INTER2': 'Actor 2 Type',\n                   'INTERACTION': 'Interaction Type',\n                   'COUNTRY': 'Country',\n                   'LOCATION': 'Location',\n                   'LATITUDE': 'Latitude',\n                   'LONGITUDE': 'Longitude',\n                   'SOURCE': 'Source',\n                   'NOTES': 'Notes',\n                   'FATALITIES': 'Fatalities'}, inplace=True)\ndf['Actor 1 Type'] = df['Actor 1 Type'].replace({1: 'Government',\n                                                 2: 'Rebel',\n                                                 3: 'Political Militia',\n                                                 4: 'Identity Militia',\n                                                 5: 'Communal Militia',\n                                                 6: 'Protesters',\n                                                 7: 'Civilians',\n                                                 8: 'External',\n                                                 9: 'Other'})\ndf['Actor 2 Type'] = df['Actor 2 Type'].replace({1: 'Government',\n                                                 2: 'Rebel',\n                                                 3: 'Political Militia',\n                                                 4: 'Identity Militia',\n                                                 5: 'Communal Militia',\n                                                 6: 'Protesters',\n                                                 7: 'Civilians',\n                                                 8: 'External',\n                                                 9: 'Other'})\ndf.head(10)"
 },
 {
  "nbid": "NB_1409789",
  "file": "NB_1409789,df,urban-1.ipynb",
  "row_id": 61,
  "target_code": "df.drop_duplicates(subset=['Country'         , 'Time'       , 'Value'        ], keep='last'      )\nprint(df.shape)\ndf = df.pivot_table(index='Time'      , columns='Country'         , values='Value'       )\ndf.head()",
  "generation": "# pivot table to reshape data\ndf = df.pivot_table(index=['Time'], columns=['Country'], values=['Value'])\n\n# flatten column names\ndf.columns = ['_'.join(col).strip() for col in df.columns.values]\n\n# reset index\ndf = df.reset_index()\n\n# rename time column\ndf = df.rename(columns={'Time': 'Year'})\n\n# fill missing values with NaN\ndf = df.fillna(value=pd.np.nan)\n\n# round values to 3 decimal places\ndf = df.round(3)\n\ndf.head()"
 },
 {
  "nbid": "NB_1409789",
  "file": "NB_1409789,df,urban-1.ipynb",
  "row_id": 32,
  "target_code": "df = df_original.copy()\ndf = df[df.Indicator == 'Mortality rate, infant (per 1,000 live births)'                                                 ][['Country'         , 'Time'       , 'Value'        ]]\ndf.head()",
  "generation": "# select certain fields and manipulate dataset\ndf = df_original[df_original.Indicator == 'Infant mortality rate (probability of dying between birth and age 1 per 1000 live births)'][['Country', 'Time', 'Value']]\ndf.head()\n\n# remove duplicates\ndf.drop_duplicates(subset=['Country', 'Time', 'Value'], keep='last')\nprint(df.shape)\n\n# pivot table\ndf = df.pivot_table(index='Time', columns='Country', values='Value')\ndf = df.reset_index()\ndf = pd.melt(df, id_vars=['Time'], var_name='Country', value_name='Value')\ndf = df.rename(columns={'Time': 'Year'})\ndf = df.dropna(subset=['Value'])\ndf['Year'] = df['Year'].astype(int)\n\n# select South Africa\ndf = df[df['Country'] == 'South Africa']\n\n# visualize data\ntrace = go.Scatter(\n    x=df.Year,\n    y=df.Value,\n    mode='lines+markers',\n    name='South Africa'\n)\n\ndata = [trace]\nlayout = go.Layout(\n    title='Infant Mortality Rate in South Africa',\n    titlefont=dict(\n            size=22),\n    xaxis=dict(\n        title='Year',\n        titlefont=dict(\n            family='Courier New, monospace',\n            size=18,\n            color='#7f7f7f'\n        )\n    ),\n    yaxis=dict(\n        title='Infant Mortality Rate (per 1000 live births)',\n        titlefont=dict(\n            family='Courier New, monospace',\n            size=18,\n            color='#7f7f7f'\n        )\n    )\n)\n\n# create figure for offline use\nfig = dict( data=data, layout=layout )\n\n# plot inline\niplot(fig, show_link=False)\n\n# save figure offline\nmyChart = plotly.offline.plot(fig,output_type=\"div\", show_link=\"False\",include_plotlyjs=\"Flase\",link_text=\"\")\nwith open(\"images/infant_mortality.html\", \"w\") as text_file:\n    text_file.write(myChart)\n\n# select certain fields and manipulate dataset\ndf = df_original[df_original.Indicator == 'Fertility rate, total (births per woman)'][['Country', 'Time', 'Value']]\ndf.head()\n\n# remove duplicates\ndf.drop_duplicates(subset=['Country', 'Time', 'Value'], keep='last')\nprint(df.shape)\n\n# pivot table\ndf = df.pivot_table(index='Time', columns='Country', values='Value')\ndf = df.reset_index()\ndf = pd.melt(df, id_vars=['Time'], var_name='Country', value_name='Value')\ndf = df.rename(columns={'Time': 'Year'})\ndf = df.dropna(subset=['Value'])\ndf['Year'] = df['Year'].astype(int)\n\n# select South Africa\ndf = df[df['Country'] == 'South Africa']\n\n# visualize data\ntrace = go.Scatter(\n    x=df.Year,\n    y=df.Value,\n    mode='lines+markers',\n    name='South Africa'\n)\n\ndata = [trace]\nlayout = go.Layout(\n    title='Fertility Rate in South Africa',\n    titlefont=dict(\n            size=22),\n    xaxis=dict(\n        title='Year',\n        titlefont=dict(\n            family='Courier New, monospace',\n            size=18,\n            color='#7f7f7f'\n        )\n    ),\n    yaxis=dict(\n        title='Fertility Rate (births per woman)',\n        titlefont=dict(\n            family='Courier New, monospace',\n            size=18,\n            color='#7f7f7f'\n        )\n    )\n)\n\n# create figure for offline use\nfig = dict( data=data, layout=layout )\n\n# plot inline\niplot(fig, show_link=False)\n\n# save figure offline\nmyChart = plotly.offline.plot(fig,output_type=\"div\", show_link=\"False\",include_plotlyjs=\"Flase\",link_text=\"\")\nwith open(\"images/fertility_rate.html\", \"w\") as text_file:\n    text_file.write(myChart)"
 },
 {
  "nbid": "NB_1409789",
  "file": "NB_1409789,df,urban-1.ipynb",
  "row_id": 36,
  "target_code": "df = df_original.copy()\ndf = df[df.Indicator== 'Life expectancy at birth, total (years)'                                          ][['Country'         , 'Time'       , 'Value'        ]]\ndf.head()",
  "generation": "df = df_original.copy()\n\n# select certain fields and manipulate dataset\ndf = df[df.Indicator == 'Life expectancy at birth (years)'][['Country', 'Time', 'Value']]\ndf.head()\n\n# remove duplicates\ndf.drop_duplicates(subset=['Country', 'Time', 'Value'], keep='last')\nprint(df.shape)\n\n# pivot table\ndf = df.pivot_table(index='Time', columns='Country', values='Value')\ndf = df.reset_index()\ndf = pd.melt(df, id_vars=['Time'], value_vars=df.columns[1:], var_name='Country')\ndf = df.rename(columns={'Time': 'Year', 'value': 'Value'})\ndf = df[df.Country == 'Australia']\ndf = df[['Country', 'Year', 'Value']]\ndf.head()"
 },
 {
  "nbid": "NB_1409789",
  "file": "NB_1409789,df,urban-1.ipynb",
  "row_id": 40,
  "target_code": "df = df_original.copy()\ndf = df[df.Indicator== 'Prevalence of HIV, total (% of population ages 15-49)'                                                        ][['Country'         , 'Time'       , 'Value'        ]]\ndf.head()",
  "generation": ""
 },
 {
  "nbid": "NB_1409789",
  "file": "NB_1409789,df,urban-1.ipynb",
  "row_id": 48,
  "target_code": "df = df_original.copy()\ndf = df[df.Indicator == 'GDP (current US$)'                    ][['Country'         , 'Time'       , 'Value'        ]]\ndf.head()",
  "generation": "df = df_original.copy()\n\n# select certain fields and manipulate dataset\ndf = df[df.Indicator == 'GDP (current US$)'][['Country', 'Time', 'Value']]\ndf.head()\n\n# remove duplicates\ndf.drop_duplicates(subset=['Country', 'Time', 'Value'], keep='last')\nprint(df.shape)\n\n# pivot table\ndf = df.pivot_table(index='Time', columns='Country', values='Value')\ndf = df.reset_index()\ndf.columns.name = None\ndf.head()"
 },
 {
  "nbid": "NB_1409789",
  "file": "NB_1409789,df,urban-1.ipynb",
  "row_id": 52,
  "target_code": "df = df_original.copy()\ndf = df[df.Indicator == 'GDP per capita (current US$)'                               ][['Country'         , 'Time'       , 'Value'        ]]\ndf.head()",
  "generation": "df = df_original.copy()\n\n# select certain fields and manipulate dataset\ndf = df[df.Indicator == 'GDP per capita (current US$)'][['Country', 'Time', 'Value']]\ndf.head()\n # remove duplicates\ndf.drop_duplicates(subset=['Country', 'Time', 'Value'], keep='last')\nprint(df.shape)\ndf = df.pivot_table(index='Time', columns='Country', values='Value')\ndf = df.reset_index()\ndf = df[['Country', 'Time', 'Australia']]\ndf = df.rename(columns={'Australia': 'Value'})\ndf.head()"
 },
 {
  "nbid": "NB_1409789",
  "file": "NB_1409789,df,urban-1.ipynb",
  "row_id": 56,
  "target_code": "df = df_original.copy()\ndf = df[df.Indicator == 'Poverty headcount ratio at $3.10 a day (PPP) (% of population)'                                                                 ][['Country'         , 'Time'       , 'Value'        ]]\ndf.head()",
  "generation": "# select certain fields and manipulate dataset\ndf = df_original[df_original.Indicator == 'Poverty headcount ratio at $3.10 a day (2011 PPP) (% of population)'][['Country', 'Time', 'Value']]\n\n# remove duplicates\ndf.drop_duplicates(subset=['Country', 'Time', 'Value'], keep='last')\nprint(df.shape)\n\n# pivot table\ndf = df.pivot_table(index='Time', columns='Country', values='Value')\ndf = df.reset_index()\ndf = pd.melt(df, id_vars=['Time'], var_name='Country', value_name='Value')\ndf = df.dropna()\n\n# select certain fields and manipulate dataset\ndf = df[df.Country == 'South Africa'][['Country', 'Time', 'Value']]\ndf = df.rename(columns={'Time': 'Year'})\n\n# remove duplicates\ndf.drop_duplicates(subset=['Country', 'Year', 'Value'], keep='last')\nprint(df.shape)\n\n# convert year to int\ndf['Year'] = df['Year'].astype(int)\n\n# create figure\ntrace = go.Scatter(\n    x=df['Year'],\n    y=df['Value'],\n    mode='lines+markers',\n    name='Poverty Headcount Ratio'\n)\n\ndata = [trace]\nlayout = go.Layout(\n    title='Poverty Headcount Ratio in South Africa',\n    xaxis=dict(\n        title='Year',\n        titlefont=dict(\n            family='Courier New, monospace',\n            size=18,\n            color='#7f7f7f'\n        )\n    ),\n    yaxis=dict(\n        title='Poverty Headcount Ratio (%)',\n        titlefont=dict(\n            family='Courier New, monospace',\n            size=18,\n            color='#7f7f7f'\n        )\n    )\n)\n\n# create figure for offline use\nfig = dict( data=data, layout=layout )\n\n# plot inline\niplot(fig, show_link=False)\n\n# save figure offline\nmyChart = plotly.offline.plot(fig,output_type=\"div\", show_link=\"False\",include_plotlyjs=\"Flase\",link_text=\"\")\nwith open(\"images/poverty_headcount_ratio.html\", \"w\") as text_file:\n    text_file.write(myChart)"
 },
 {
  "nbid": "NB_1409789",
  "file": "NB_1409789,df,urban-1.ipynb",
  "row_id": 60,
  "target_code": "df = df_original.copy()\ndf = df[df.Indicator == 'Poverty headcount ratio at $1.90 a day (PPP) (% of population)'                                                                 ][['Country'         , 'Time'       , 'Value'        ]]\ndf.head()",
  "generation": "df = df[['Country', 'Time', 'Poverty headcount ratio at $3.10 a day (PPP) (% of population)']]\ndf = df.drop_duplicates(subset=['Country', 'Time', 'Poverty headcount ratio at $3.10 a day (PPP) (% of population)'], keep='last')\ndf = df.pivot_table(index='Time', columns='Country', values='Poverty headcount ratio at $3.10 a day (PPP) (% of population)')\ndf = df.reset_index()\ndf = df.melt(id_vars=['Time'], var_name='Country', value_name='Value')\ndf = df.dropna(subset=['Value'])\ndf = df.sort_values(by=['Country', 'Time'])\ndf = df.reset_index(drop=True)\ndf = df.rename(columns={'Poverty headcount ratio at $3.10 a day (PPP) (% of population)': 'Value'})\ndf.head()"
 },
 {
  "nbid": "NB_1409846",
  "file": "NB_1409846,y_test-1.ipynb",
  "row_id": 77,
  "target_code": "y_test = pd.merge(y_test, lang_groups, how='left'      , on='language'          )\ny_test.head(3)",
  "generation": "df = pd.merge(df, language_df, on='language', how='inner')\ndf['lang_encoded'] = encoder.transform(df['lang_encoded'])\ny_test = pd.merge(y_test, pd.DataFrame(df.language), right_index=True, left_index=True)"
 },
 {
  "nbid": "NB_1409846",
  "file": "NB_1409846,y_train-1.ipynb",
  "row_id": 75,
  "target_code": "y_train = pd.merge(y_train, lang_groups, how='left'      , on='language'          )\ny_train.head(3)",
  "generation": "encoder = LabelEncoder()\ndf['lang_encoded'] = encoder.fit_transform(df['language'])\ndf.head()"
 },
 {
  "nbid": "NB_1413752",
  "file": "NB_1413752,dfc-1.ipynb",
  "row_id": 37,
  "target_code": "dfc = dfc.rename(columns={0: 'Y'    })\ndfc.head()",
  "generation": "dfc = pd.concat([data_2df, Y], axis=1)\ndfc.columns = ['PC1', 'PC2', 'Y']\ndfc.head()"
 },
 {
  "nbid": "NB_1413752",
  "file": "NB_1413752,dfc-1.ipynb",
  "row_id": 45,
  "target_code": "dfc = pd.concat([dfc, y_hier], axis=1)\ndfc.head()",
  "generation": "dfc = dfc.rename(columns={0: 'cluster'})\ndfc.head()"
 },
 {
  "nbid": "NB_1419731",
  "file": "NB_1419731,country,x-1.ipynb",
  "row_id": 12,
  "target_code": "country['exports'         ] = country['exports'         ]*country['gdpp'      ]/100\ncountry['imports'         ] = country['imports'         ]*country['gdpp'      ]/100\ncountry['health'        ] = country['health'        ]*country['gdpp'      ]/100\ncountry.head()",
  "generation": "cols = ['exports', 'health', 'imports']\ncountry[cols] = country[cols] * country['gdpp'].values.reshape(-1, 1)\ncountry.head()"
 },
 {
  "nbid": "NB_1421396",
  "file": "NB_1421396,df-1.ipynb",
  "row_id": 2,
  "target_code": "df=df.astype('str'     )\ndf.apply(np.max)\nprint('\\n'    )\nd={'0'    : False, '1'     : True}\ndf[\"Survived\"          ]=df[\"Survived\"          ].map(d)\ndf.head()",
  "generation": "df['Survived'] = df['Survived'].astype(bool)\ndf.head()"
 },
 {
  "nbid": "NB_1428824",
  "file": "NB_1428824,df_full_data-1.ipynb",
  "row_id": 19,
  "target_code": "df_full_data['event_time'            ] = df_full_data['timestamp'           ].apply(lambda x: re.findall('[0-9]+:[0-9]+:[0-9]+'                      , x)[0])\ndf_full_data['event_hour'            ] = df_full_data['event_time'            ].apply(lambda x: x[0:2])\ndf_full_data.head()",
  "generation": "def getWeekday(date):\n    return pd.to_datetime(date).weekday()\n\ndf_full_data['event_time'] = df_full_data['timestamp'].apply(lambda x: x[11:19])\ndf_full_data['event_hour'] = df_full_data['event_time'].apply(lambda x: int(x[:2]))"
 },
 {
  "nbid": "NB_163177",
  "file": "NB_163177,party_count-1.ipynb",
  "row_id": 65,
  "target_code": "party_count = party_count.loc[:, 2:5]\nparty_count",
  "generation": "party_count = pd.crosstab(tips['day'], tips['size'])\nparty_count = party_count.iloc[:, 1:5]\nparty_count"
 },
 {
  "nbid": "NB_214156",
  "file": "NB_214156,Age_Penalties-1.ipynb",
  "row_id": 14,
  "target_code": "Age_Penalties = Age_Penalties.reset_index()\nAge_Penalties.head()",
  "generation": "couple_columns = data[['Skill Moves', 'Penalties', 'Age']]\nAge_Penalties = couple_columns.groupby(['Age' ,'Penalties'], as_index=False).mean()[['Age', 'Penalties', 'Skill Moves']]\nAge_Penalties.head()"
 },
 {
  "nbid": "NB_5217235",
  "file": "NB_5217235,glasses_df-1.ipynb",
  "row_id": 5,
  "target_code": "cutoff_value = 4\nmiscellaneous = 0 \nfor i in range(len(glasses_df)):\n    count = glasses_df.loc[i,\"Count\"       ]\n    if glasses_df.loc[i,\"Count\"       ] <= cutoff_value:\n        miscellaneous +=count\nglasses_df = glasses_df[glasses_df.Count > cutoff_value]\nglasses_df = glasses_df.append({\"Glass\"       :\"miscellaneous\"               , \"Count\"        :miscellaneous}, ignore_index = True)\nglasses_df",
  "generation": "cutoff_value = 40\nmiscellaneous = glasses_df[glasses_df['Count'] < cutoff_value]['Count'].sum()\nglasses_df = glasses_df[glasses_df['Count'] >= cutoff_value]\nglasses_df = glasses_df.append({'Glass': 'Miscellaneous', 'Count': miscellaneous}, ignore_index=True)\nglasses_df"
 },
 {
  "nbid": "NB_5307246",
  "file": "NB_5307246,cc_apps-1.ipynb",
  "row_id": 9,
  "target_code": "for col in cc_apps:\n    if cc_apps[col].dtypes == 'object'         :\n        cc_apps = cc_apps.fillna(cc_apps[col].value_counts().index[0])\ncc_apps.tail(17)",
  "generation": "for col in cc_apps.columns:\n    if cc_apps[col].dtype == 'object':\n        cc_apps = cc_apps.fillna(cc_apps[col].value_counts().index[0])\ncc_apps.tail(17)"
 },
 {
  "nbid": "NB_1451216",
  "file": "NB_1451216,df-1.ipynb",
  "row_id": 7,
  "target_code": "df.drop(columns=[\"id\"    ], axis=1, inplace=True)\ndf[\"date\"      ] = df[\"date\"      ].replace(to_replace=\"(2014|2015)(.*)\"                 , value='\\\\1'     , regex=True)\ndf[\"date\"      ] = pd.to_numeric(df[\"date\"      ], errors='coerce'        )\ndf.head()",
  "generation": "df.drop('id', axis=1, inplace=True)\ndf['date'] = pd.to_datetime(df['date']).dt.year\ndf.head()"
 },
 {
  "nbid": "NB_1455676",
  "file": "NB_1455676,df2,Y2-1.ipynb",
  "row_id": 11,
  "target_code": "athletes_df = pd.DataFrame(athletes, columns=['last_name'           , 'first_name'             ]) \ndf2 = pd.merge(df, athletes_df, on=['last_name'           , 'first_name'             ], how='inner'       )\ndf2 = df2.sort_values(by=['last_name'           , 'first_name'             ])\ndf2",
  "generation": "df = df.dropna(subset=['first_name'])\ngrouped_dict = df.groupby(['last_name', 'first_name']).groups\nathletes = [key for (key, value) in grouped_dict.items() if (len(value) > 1)]\ndf2 = pd.DataFrame()\nfor ln, fn in athletes:\n    df2 = pd.concat([df2, df[(df['last_name'] == ln) & (df['first_name'] == fn)]], axis=0)\ndf2 = df2.sort_values(by=['last_name', 'first_name'])\ndf2.reset_index(drop=True, inplace=True)\ndf2"
 },
 {
  "nbid": "NB_1459145",
  "file": "NB_1459145,df,y_train,X_test-1.ipynb",
  "row_id": 2,
  "target_code": "df.rename(columns={'title'       :'Track Name'            ,'artist'        :'Artist Name'             ,'bpm'     :'Beats Per Minute'                  ,'top genre'           :'Genre'       ,'nrgy'      :'Energy'        ,'dnce'      :'Danceability'              , 'dB'     :'Loudness dB'             ,'spch'      :'Speechiness'             ,'live'      :'Liveness'          ,'val'     :'Valence'         ,'dur'     :'Length'        ,'acous'       :'Acousticness'              ,'pop'     :'Popularity'            },inplace=True)\nf = df.drop(df.index[[50, 138, 267, 362, 442]])\ndf = df.reset_index()\ndf = df.drop(['Track Name'            ,'Artist Name'             ,'Genre'       ,'year'      ,'index'       ], axis =1)\ndf.head()",
  "generation": "df = df.rename(columns={'bpm': 'Beats Per Minute', 'nrgy': 'Energy', 'dnce': 'Danceability', 'dB': 'Loudness dB', 'live': 'Liveness', 'val': 'Valence', 'dur': 'Length', 'acous': 'Acousticness', 'spch': 'Speechiness', 'pop': 'Popularity'})\ndf.head()"
 },
 {
  "nbid": "NB_1461334",
  "file": "NB_1461334,data-1.ipynb",
  "row_id": 2,
  "target_code": "data = (data.sort_values(\"Very interested\"                 , ascending=False) / 2233).round(2)\ndata",
  "generation": "data = data.sort_values(by=\"Very interested\", ascending=False)\ndata = round((data / 2233) * 100, 2)\ndata.drop(\"Unnamed: 0\", axis=1, inplace=True)\ndata.head()"
 },
 {
  "nbid": "NB_1463084",
  "file": "NB_1463084,df_reviews-1.ipynb",
  "row_id": 14,
  "target_code": "df_reviews = df_reviews.dropna(subset=['Translated_Review'                   ])\ndf_reviews.head()",
  "generation": "df_reviews = df_reviews.dropna()\ndf_reviews.head()"
 },
 {
  "nbid": "NB_1463164",
  "file": "NB_1463164,df-1.ipynb",
  "row_id": 2,
  "target_code": "df['Date'      ] = pd.to_datetime(df['Date'      ])\ndf.head()",
  "generation": "df['Date'] = pd.to_datetime(df['Date'])\ndf.head()"
 },
 {
  "nbid": "NB_1467765",
  "file": "NB_1467765,global_summary-1.ipynb",
  "row_id": 25,
  "target_code": "global_summary = pandas.melt(global_summary, id_vars = ['index'       , 'region'         ], value_vars=['mean'      , 'median'         , 'sd'     ])\nglobal_summary",
  "generation": "global_summary = global_summary[['Unnamed: 0', 'index', 'region', 'mean']]\nglobal_summary = global_summary.rename(columns = {'Unnamed: 0': 'variable'})\nglobal_summary = global_summary.melt(id_vars=['variable', 'index', 'region'], value_vars=['mean'])\nglobal_summary = global_summary[['variable', 'index', 'region', 'variable', 'value']]\nglobal_summary = global_summary.rename(columns = {'variable': 'index'})\nglobal_summary.head()"
 },
 {
  "nbid": "NB_1471321",
  "file": "NB_1471321,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = pd.merge(df, dfsubject, on='subject'         )\ndf.head()",
  "generation": "df = pd.read_csv('randomization.csv')\ndf = pd.merge(df, dfsubject, on='subject')\ndf = pd.merge(df, dfefficacy, on='subject')\ndf.head()"
 },
 {
  "nbid": "NB_1471321",
  "file": "NB_1471321,df-1.ipynb",
  "row_id": 6,
  "target_code": "df = pd.merge(df, dfefficacy, on='subject'         )\ndf.head()",
  "generation": "df = pd.merge(df, dfefficacy, on='subject')\ndf['nosebleeds'] = df['nosebleeds'].fillna(0)\ndf['duration'] = df['duration'].fillna(365)\ndf.head()"
 },
 {
  "nbid": "NB_1471321",
  "file": "NB_1471321,dfactive-1.ipynb",
  "row_id": 11,
  "target_code": "dfactive['nosebleed_rate_last_year'                          ] = dfactive.apply(lambda row: (row['previous.year'               ]/365), axis=1)\ndfactive['nosebleed_rate'                ] = dfactive.apply(lambda row: (row['nosebleeds'            ]/row['duration'          ]), axis=1)\ndfactive.drop(['previous.year'               , 'nosebleeds'             , 'duration'           ], axis=1, inplace=True)\ndfactive.head()",
  "generation": "dfactive['nosebleed_rate'] = dfactive['nosebleeds'] / dfactive['duration']\ndfactive['nosebleed_rate_last_year'] = dfactive['previous.year'] / dfactive['duration']\ndfactive.head()"
 },
 {
  "nbid": "NB_1475637",
  "file": "NB_1475637,noshowdata_new-1.ipynb",
  "row_id": 10,
  "target_code": "noshowdata_new['Gender'        ] = noshowdata_new['Gender'        ].map({'M'   :1, 'F'    :0})\nnoshowdata_new.head(3)",
  "generation": "noshowdata_new['Hypertension'] = noshowdata_new['Hipertension']\nnoshowdata_new['Handicap'] = noshowdata_new['Handcap']\nnoshowdata_new = noshowdata_new.drop(['Hipertension', 'Handcap'], axis=1)\nnoshowdata_new['Gender'] = noshowdata_new['Gender'].map({'F': 0, 'M': 1})\nnoshowdata_new.head()"
 },
 {
  "nbid": "NB_1475637",
  "file": "NB_1475637,noshowdata_new-1.ipynb",
  "row_id": 11,
  "target_code": "noshowdata_new['No-show'         ] = noshowdata_new['No-show'         ].map({'Yes'     :1, 'No'     :0})\nnoshowdata_new.head(3)",
  "generation": "noshowdata_new['Gender'] = noshowdata_new['Gender'].map({'M':1, 'F':0})\nnoshowdata_new['No-show'] = noshowdata_new['No-show'].map({'No':0, 'Yes':1})\nnoshowdata_new.drop(columns=['SMS_received'], inplace=True)\nnoshowdata_new.head()"
 },
 {
  "nbid": "NB_1481513",
  "file": "NB_1481513,s_total_food_2018-1.ipynb",
  "row_id": 51,
  "target_code": "s_total_food_2018=s_total_food_2018[['2018'      ]]\ns_total_food_2018",
  "generation": "s_total_food_2018 = s_total_food_2018.drop('index', axis=1)\ns_total_food_2018 = s_total_food_2018.rename(columns={'Unnamed: 0': 'index'})\ns_total_food_2018 = s_total_food_2018.set_index('index')\ns_total_food_2018 = s_total_food_2018.reset_index(drop=True)\ns_total_food_2018.head()"
 },
 {
  "nbid": "NB_1481704",
  "file": "NB_1481704,matches_rank-1.ipynb",
  "row_id": 18,
  "target_code": "matches_rank = matches_rank.merge(rankings, \n                        left_on=['date'      , 'away_team'            ], \n                        right_on=['rank_date'           , 'country_full'               ], \n                        suffixes=('_home'       , '_away'        ))\nmatches_rank.head()",
  "generation": "matches['rank_date_home'] = matches['date']\nmatches['rank_date_away'] = matches['date']\nmatches_rank = matches.merge(rankings, left_on=['rank_date_home', 'home_team'], right_on=['rank_date', 'country_full'])\nmatches_rank = matches_rank.merge(rankings, left_on=['rank_date_away', 'away_team'], right_on=['rank_date', 'country_full'], suffixes=('_home', '_away'))\nmatches_rank = matches_rank.drop(['rank_date_home', 'country_full_home', 'country_abrv_home', 'rank_date_away', 'country_full_away', 'country_abrv_away'], axis=1)\nmatches_rank.head()"
 },
 {
  "nbid": "NB_1481704",
  "file": "NB_1481704,matches_rank-1.ipynb",
  "row_id": 19,
  "target_code": "matches_rank['rank_difference'                 ] = matches_rank['rank_home'           ] - matches_rank['rank_away'           ]\nmatches_rank['average_rank'              ] = (matches_rank['rank_home'           ] + matches_rank['rank_away'           ])/2\nmatches_rank['point_difference'                  ] = matches_rank['weighted_points_home'                      ] - matches_rank['weighted_points_away'                      ]\nmatches_rank['score_difference'                  ] = matches_rank['home_score'            ] - matches_rank['away_score'            ]\nmatches_rank['is_won'        ] = matches_rank['score_difference'                  ] > 0 \nmatches_rank['is_stake'          ] = matches_rank['tournament'            ] != 'Friendly'           \nmatches_rank.head()",
  "generation": "matches_rank['rank_difference'] = matches_rank['rank_home'] - matches_rank['rank_away']\nmatches_rank['average_rank'] = (matches_rank['rank_home'] + matches_rank['rank_away']) / 2\nmatches_rank['point_difference'] = matches_rank['weighted_points_home'] - matches_rank['weighted_points_away']\nmatches_rank['score_difference'] = matches_rank['home_score'] - matches_rank['away_score']\nmatches_rank['is_won'] = matches_rank['score_difference'] > 0\nmatches_rank['is_stake'] = matches_rank['tournament'] != 'Friendly'\nmatches_rank.head()"
 },
 {
  "nbid": "NB_1483983",
  "file": "NB_1483983,df-1.ipynb",
  "row_id": 3,
  "target_code": "df=100*df/2233\ndf=df.round(2)\ndf.head()",
  "generation": "df_perc = df.apply(lambda x: round(100 * x / sum(x), 2), axis=1)\ndf_perc = df_perc[['Very interested', 'Somewhat interested', 'Not interested']]\ndf_perc.head()"
 },
 {
  "nbid": "NB_1485583",
  "file": "NB_1485583,df-1.ipynb",
  "row_id": 26,
  "target_code": "df['season_cat'            ] = df.season.apply(lambda x: 'spring'          if x == 1 else \n                                           (\n                                            'summer'                                                     if x == 2 else (\n                                                'fall'                                                       if x == 3 else 'winter'         )\n                                           )\n                                      )\ndf.head()",
  "generation": "df['season_cat'] = df['season'].map({1:'spring', 2:'summer', 3:'fall', 4:'winter'})"
 },
 {
  "nbid": "NB_1486290",
  "file": "NB_1486290,housing_data-1.ipynb",
  "row_id": 10,
  "target_code": "furnishing_status = pd.get_dummies(housing_data['furnishingstatus'                  ], drop_first=True)\nhousing_data = pd.concat([housing_data, furnishing_status], axis=1)\nhousing_data.head()",
  "generation": "housing_data = pd.get_dummies(housing_data, columns=['furnishingstatus'], drop_first=True)"
 },
 {
  "nbid": "NB_1486290",
  "file": "NB_1486290,housing_data-1.ipynb",
  "row_id": 11,
  "target_code": "housing_data = housing_data.drop(['furnishingstatus'                  ], axis=1)\nhousing_data.head()",
  "generation": "housing_data.drop(['furnishingstatus'], axis=1, inplace=True)"
 },
 {
  "nbid": "NB_1486510",
  "file": "NB_1486510,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.drop(df.columns[0], axis=1)\ndf['Start Date'            ] = pd.to_datetime(df['Start Date'            ])\ndf.Opposition = df.Opposition.apply( lambda x: x[2:])\ndf = df[df['4s'    ] != '-'    ]\ndf['Runs'      ] = df['Runs'      ].apply( lambda x: x[:-1] if '*'     in x else x)\ndf.head()",
  "generation": "df['Start Date'] = pd.to_datetime(df['Start Date'])\ndf['Opposition'] = df['Opposition'].str.replace('v ','')\ndf = df[(df['Runs'] != '-') & (df['Runs'] != 'DNB') & (df['Runs'] != 'TDNB')]\ndf['Runs'] = df['Runs'].str.replace('*','').astype(int)\ndf.head()"
 },
 {
  "nbid": "NB_1491695",
  "file": "NB_1491695,df,m-1.ipynb",
  "row_id": 3,
  "target_code": "df=df.sort_values(by='Very interested'                 ,ascending=False)\ndf",
  "generation": "df.sort_values(by='Very interested', ascending=False, inplace=True)\ndf['Total'] = df.sum(axis=1)\ndf_perc = round((df.iloc[:, :-1]/2233)*100, 2)\ndf_perc = df_perc[['Very interested', 'Somewhat interested', 'Not interested']]\ndf_perc = df_perc.sort_values(by='Very interested', ascending=False)\ndf_perc"
 },
 {
  "nbid": "NB_1491695",
  "file": "NB_1491695,df,m-1.ipynb",
  "row_id": 4,
  "target_code": "df=(100*df/2233).round(2)\ndf",
  "generation": "df = pd.read_csv('Topic_Survey_Assignment.csv', index_col=0)\ndf = df.sort_values(by='Very interested', ascending=False)\ndf = round((df/2233)*100, 2)\ndf"
 },
 {
  "nbid": "NB_1498031",
  "file": "NB_1498031,df,normalized_selection-1.ipynb",
  "row_id": 7,
  "target_code": "toDrop = ['Year'      , 'OrderSum'           ]\ndf = df.drop(toDrop, axis=1)\ndf.head()",
  "generation": "df['Date'] = pd.to_datetime(df['Date'])\ndf['DayOfWeek'] = df['Date'].dt.dayofweek\ndf['DayOfYear'] = df['Date'].dt.dayofyear\ndf['StateHoliday'] = df['StateHoliday'].astype(int)\ndf['StateWeekend'] = df['StateWeekend'].astype(int)\ndf['Weekend'] = df['Weekend'].astype(int)\ndf['PromoYandex'] = df['PromoYandex'].astype(int)\ndf['PromoClub'] = df['PromoClub'].astype(int)\ndf['OrderCount'] = df['OrderCount'].astype(int)\ndf.head()"
 },
 {
  "nbid": "NB_1511795",
  "file": "NB_1511795,df-1.ipynb",
  "row_id": 11,
  "target_code": "df['income'        ] = df['AveragePrice'              ] * df['Total Volume'              ]\ndf.head()",
  "generation": "df['income'] = df['AveragePrice'] * df['Total Volume']\ndf.head()"
 },
 {
  "nbid": "NB_1512498",
  "file": "NB_1512498,murder_dataset-1.ipynb",
  "row_id": 41,
  "target_code": "murder_dataset['rate'      ] = murder_dataset['total'       ] / murder_dataset['population'            ] * 100000\nmurder_dataset.head()",
  "generation": "murder_dataset['rate'] = murder_dataset['total']/murder_dataset['population']*100000\nmurder_dataset"
 },
 {
  "nbid": "NB_169693",
  "file": "NB_169693,titanic-1.ipynb",
  "row_id": 95,
  "target_code": "titanic = titanic.merge(embarked_map).sort_values(by='PassengerId'             )\ntitanic.head()",
  "generation": "titanic = pd.merge(titanic, embarked_map, on='Embarked', how='left')\ntitanic.head()"
 },
 {
  "nbid": "NB_169724",
  "file": "NB_169724,merged_data-1.ipynb",
  "row_id": 15,
  "target_code": "merged_data = merged_data.drop(['Year'      , 'Office'         ],axis=1)\nmerged_data.head()",
  "generation": "merged_data.drop(['Year', 'Office'], axis=1, inplace=True)\nmerged_data.head()"
 },
 {
  "nbid": "NB_169724",
  "file": "NB_169724,merged_data-1.ipynb",
  "row_id": 19,
  "target_code": "merged_data = merged_data.drop(['Citizen Voting-Age Population'                               ],axis=1)\nmerged_data",
  "generation": "merged_data = merged_data.drop(['Citizen Voting-Age Population'],axis=1)\nmerged_data.head()"
 },
 {
  "nbid": "NB_169795",
  "file": "NB_169795,df-1.ipynb",
  "row_id": 14,
  "target_code": "df['mins_off'          ] = 1440 - df.mins_tot\ndf.head(5)",
  "generation": "df['mins_off'] = 1440 - df['mins_tot']\ndf"
 },
 {
  "nbid": "NB_4100835",
  "file": "NB_4100835,flood_parcel_df-1.ipynb",
  "row_id": 5,
  "target_code": "flood_parcel_df['Est_Foreclosed_Parcels'                        ] = flood_parcel_df['Percentage_Foreclosed_Parcels'                               ]*flood_parcel_df['ParcelCount'             ]\nflood_parcel_df['Est_Low_Value_Parcels'                       ] = flood_parcel_df['Share_Low_Value_Mean_05_15'                            ]*flood_parcel_df['ParcelCount'             ]\nflood_parcel_df['Est_Vacant_Parcels'                    ] = flood_parcel_df['Vacant_Percent_Mean_10_15'                           ]*flood_parcel_df['ParcelCount'             ]\nflood_parcel_df.head()",
  "generation": "flood_parcel_df['Est_Foreclosed_Parcels'] = flood_parcel_df['Percentage_Foreclosed_Parcels'] * flood_parcel_df['ParcelCount']\nflood_parcel_df['Est_Low_Value_Parcels'] = flood_parcel_df['Share_Low_Value_Mean_05_15'] * flood_parcel_df['ParcelCount']\nflood_parcel_df['Est_Vacant_Parcels'] = flood_parcel_df['Vacant_Percent_Mean_10_15'] * flood_parcel_df['ParcelCount']\nflood_parcel_df"
 },
 {
  "nbid": "NB_4129576",
  "file": "NB_4129576,df2a-1.ipynb",
  "row_id": 9,
  "target_code": "df2a[\"C_chyba\"         ] = df2a.C * 0.01\ndf2a.head()",
  "generation": "df2a['C_chyba']=df2a['C']*0.1\ndf2a"
 },
 {
  "nbid": "NB_4136264",
  "file": "NB_4136264,Xtest-1.ipynb",
  "row_id": 15,
  "target_code": "Xtest['impAge'        ] = pd.DataFrame(siage.transform(Xtest[['Age'     ]]), index=Xtest.index)\nXtest.head()",
  "generation": "Xtest['impAge'] = pd.DataFrame(siage.transform(Xtest[['Age']]), index=Xtest.index)  \nXtest.head()"
 },
 {
  "nbid": "NB_4136264",
  "file": "NB_4136264,Xtest-1.ipynb",
  "row_id": 16,
  "target_code": "Xtest['kbinAge'         ] = pd.DataFrame(kbdage.transform(Xtest[['impAge'        ]]), index=Xtest.index)\nXtest.head()",
  "generation": "Xtest['kbinAge'] = pd.DataFrame(kbdage.transform(Xtest[['impAge']]), index=Xtest.index).astype(int)"
 },
 {
  "nbid": "NB_4136264",
  "file": "NB_4136264,Xtest-1.ipynb",
  "row_id": 17,
  "target_code": "Xcat = pd.DataFrame(ohe.transform(Xtest[categoricalvars]), columns=ohe.get_feature_names(), index=Xtest.index)\nXtest = pd.concat([Xtest, Xcat], axis=1)\nXtest.head()",
  "generation": "Xtest = pd.DataFrame(ohe.transform(Xtest[categoricalvars]), index=Xtest.index, columns=ohe.get_feature_names(categoricalvars))\nXtest.head()"
 },
 {
  "nbid": "NB_4151576",
  "file": "NB_4151576,df3-1.ipynb",
  "row_id": 83,
  "target_code": "df3['intercept'           ] = 1\ndf3[['US'    ,'CA'    ]] = pd.get_dummies(df3['country'         ])[['US'    ,'CA'    ]]\ndf3.head()",
  "generation": "df3[['US', 'CA']] = pd.get_dummies(df3['country'])\ndf3.head()"
 },
 {
  "nbid": "NB_4188340",
  "file": "NB_4188340,df2-1.ipynb",
  "row_id": 74,
  "target_code": "df2['intercept'           ] = 1\ndf2[['control'         , 'treatment'            ]] = pd.get_dummies(df2['group'       ])\ndf2.head()",
  "generation": "df['intercept'] = 1\ndf[['control', 'treatment']] = pd.get_dummies(df['group'])\ndf.drop('control', axis=1, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_4203227",
  "file": "NB_4203227,df_confirmed-1.ipynb",
  "row_id": 9,
  "target_code": "df_confirmed.fillna(method='pad'     , inplace=True)\ndf_confirmed[\"CH_cumul_conf\"               ] = df_confirmed.loc[:,'AG'    :'ZH'    ].sum(axis = 1)\ndf_confirmed.tail()",
  "generation": "df_confirmed = df_confirmed.fillna(method='ffill')\ndf_confirmed['CH_cumul_conf'] = df_confirmed.sum(axis=1)\ndf_confirmed.tail()"
 },
 {
  "nbid": "NB_146628",
  "file": "NB_146628,df,cols-1.ipynb",
  "row_id": 5,
  "target_code": "from sklearn import preprocessing\npreprocessing_cols = [\n    'source.bytes'                  , \n    'source.packets'                    ,\n    'destination.bytes'                       , \n    'destination.packets'                         , \n    'network.packets'                     , \n    'network.bytes'                   , \n    'event.duration'                    \n]\nfor col in preprocessing_cols:\n    df[col] = df[col].astype(str).str.replace(','   , ''   ).astype(float)\ndf.head()",
  "generation": "df['destination.packets'] = df['destination.packets'].astype(float)\ndf['destination.port'] = df['destination.port'].astype(float)\ndf['destination.bytes'] = df['destination.bytes'].astype(float)\ndf['network.packets'] = df['network.packets'].astype(float)\ndf['network.bytes'] = df['network.bytes'].astype(float)"
 },
 {
  "nbid": "NB_1517743",
  "file": "NB_1517743,data-1.ipynb",
  "row_id": 106,
  "target_code": "data.date = data.date.apply(lambda d: datetime.strptime(d, \"%Y-%m-%d\"           ))\ndata.head()",
  "generation": "data = data.rename(columns={\"time\":\"date\"})\ndata.head(2)"
 },
 {
  "nbid": "NB_1521731",
  "file": "NB_1521731,df-1.ipynb",
  "row_id": 7,
  "target_code": "df['Total_Benefited'                 ] = df[df.columns[2:6]].sum(axis=1)\ndf.head()",
  "generation": "df[\"Total_Benefited\"] = df.iloc[:, 3:].sum(axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_1526115",
  "file": "NB_1526115,data-1.ipynb",
  "row_id": 2,
  "target_code": "data = data.drop('NMHC(GT)'          , axis=1)\ndata.head(5)",
  "generation": "data.drop('NMHC(GT)', axis=1, inplace=True)\ndata.head(5)"
 },
 {
  "nbid": "NB_1526603",
  "file": "NB_1526603,dfKatoLowBuy-1.ipynb",
  "row_id": 44,
  "target_code": "dfKatoLowBuy.columns = dfKatoLowBuy.columns.droplevel()\ndfKatoLowBuy = dfKatoLowBuy.drop([\"full\"      ], axis=1)\ndfKatoLowBuy",
  "generation": "dfKatoLowBuy.columns = dfKatoLowBuy.columns.droplevel()\ndfKatoLowBuy = dfKatoLowBuy.drop(columns=[\"full\"])\ndfKatoLowBuy.head()"
 },
 {
  "nbid": "NB_1526603",
  "file": "NB_1526603,dfKatoLowBuy-1.ipynb",
  "row_id": 50,
  "target_code": "dfKatoLowBuy = pd.merge(dfKatoLowBuy, dfKatoRankings, on=[\"Team\"      ])\ndfKatoLowBuy",
  "generation": "dfKatoLowBuy = dfKatoRoundType.reset_index()\ndfKatoLowBuy[\"Mean\"] = dfKatoLowBuy.mean(axis=1)\ndfKatoLowBuy = dfKatoLowBuy.merge(dfKatoRankings, on=\"Team\")\ndfKatoLowBuy = dfKatoLowBuy.reset_index()\ndfKatoLowBuy = dfKatoLowBuy.drop(columns=[\"index\"])\ndfKatoLowBuy = dfKatoLowBuy.rename(columns={\"Won Round\": \"Ranking\"})"
 },
 {
  "nbid": "NB_1529673",
  "file": "NB_1529673,dfT-1.ipynb",
  "row_id": 72,
  "target_code": "dfT = dfT.reset_index()\ndfT.head()",
  "generation": "dfT.reset_index(inplace=True)\ndfT.head()"
 },
 {
  "nbid": "NB_1549327",
  "file": "NB_1549327,train-1.ipynb",
  "row_id": 21,
  "target_code": "train['Phrase'        ]=train['Phrase'        ].apply(lambda x:words_with_more_than_three_chars(x))\ntrain.head()",
  "generation": "test['Phrase'] = test['Phrase'].apply(lambda x: remove_punctuation(x) if isinstance(x, str) else x)\ntest['Phrase'] = test['Phrase'].apply(lambda x: remove_stopwords(x) if isinstance(x, str) else x)\ntest['Phrase'] = test['Phrase'].apply(lambda x: lemmatize_text(x) if isinstance(x, str) else x)\ntest.head()"
 },
 {
  "nbid": "NB_1549327",
  "file": "NB_1549327,train-1.ipynb",
  "row_id": 23,
  "target_code": "stop_words=stopwords.words('english'         )\ntrain['Phrase'        ]=train['Phrase'        ].apply(lambda x: ' '    .join([word for word in x.split() if word not in (stop_words)]))\ntrain.head()",
  "generation": "stop_words = set(stopwords.words('english'))\ntrain['Phrase'] = train['Phrase'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\ntrain.head()"
 },
 {
  "nbid": "NB_1559548",
  "file": "NB_1559548,df,g-1.ipynb",
  "row_id": 11,
  "target_code": "df = df[['id'    ,'popularity'            ,'budget'        ,'revenue'         ,'genres'        ,'release_year'              ]]\ndf.head()",
  "generation": "df = df[['id', 'popularity', 'budget', 'revenue', 'genres', 'release_year']]\ndf.head(2)"
 },
 {
  "nbid": "NB_1559548",
  "file": "NB_1559548,df,g-1.ipynb",
  "row_id": 20,
  "target_code": "df = df[['genres'        ,'revenue'         ,'budget'        ]]\ndf.head()",
  "generation": "df = df[['Unnamed: 0', 'genres', 'revenue', 'budget']]\ndf = df.rename(columns={'Unnamed: 0': 'id'})\ndf.head()"
 },
 {
  "nbid": "NB_1560931",
  "file": "NB_1560931,pass_df_pc-1.ipynb",
  "row_id": 14,
  "target_code": "pass_df_pc = pass_df_num_n0.join(pass_df_pc)\npass_df_pc = pass_df_pc.join(pass_df['Survived'          ])\npass_df_pc",
  "generation": "pass_df_num_n0 = pass_df_num_n.fillna(value={'Age': pass_df_num_n['Age'].mean()})\npass_df = pass_df.join(pd.get_dummies(pass_df['Sex']))\npass_df_pc=pd.DataFrame(pca.transform(pass_df_num_n0))\npass_df_pc.set_index(pass_df.index, inplace=True)\npass_df_pc.rename(columns=lambda x: 'PC' + str(int(x)+1), inplace=True)\npass_df = pass_df.join(pass_df_pc)\npass_df.rename(columns={0: 'Survived'}, inplace=True)\npass_df.head()"
 },
 {
  "nbid": "NB_1565232",
  "file": "NB_1565232,temp_df-1.ipynb",
  "row_id": 5,
  "target_code": "temp_df['Time'      ] = temp_df[['Year'      , 'Month'        ]].apply(lambda row: pd.Period(year=row['Year'      ],month=row['Month'       ],freq='M'   ) , axis=1)\ntemp_df.head()",
  "generation": "temp_df['Time'] = pd.to_datetime(temp_df['Year'].astype(int).astype(str) + '-' + temp_df['Month'].astype(int).astype(str))\ntemp_df = temp_df.drop(['Unnamed: 0'], axis=1)\ntemp_df.head()"
 },
 {
  "nbid": "NB_1565232",
  "file": "NB_1565232,temp_df-1.ipynb",
  "row_id": 7,
  "target_code": "temp_df = temp_df.set_index('Time'      )\ntemp_df = temp_df.drop(['Year'      ,\n                        'Month'                               ,\n                        'Ten-year Anomaly'                                          ,\n                        'T.y. Unc.'                                   ,\n                        'Twenty-year Anomaly'                                             ,'Tw.y. Unc.'            ],\n                         axis=1)\ntemp_df.head()",
  "generation": "temp_df['Time'] = temp_df[['Year', 'Month']].apply(lambda row: pd.Period(year=row['Year'],month=row['Month'],freq='M') , axis=1)\ntemp_df.set_index('Time', inplace=True)\ntemp_df.drop(['Year', 'Month'], axis=1, inplace=True)\ntemp_df.columns = pd.MultiIndex.from_product([[''], temp_df.columns])\ntemp_df.head()"
 },
 {
  "nbid": "NB_4218841",
  "file": "NB_4218841,df3-1.ipynb",
  "row_id": 73,
  "target_code": "df3[['CA'    , 'UK'     , 'US'     ]] = pd.get_dummies(df3['country'         ])\ndf3 = df3.drop('CA'    , axis=1)\ndf3.head()",
  "generation": "df3[['CA', 'UK', 'US']] = pd.get_dummies(df3['country'])\ndf3 = df3.drop('CA', axis=1)\ndf3.head()"
 },
 {
  "nbid": "NB_4218864",
  "file": "NB_4218864,df_new-1.ipynb",
  "row_id": 85,
  "target_code": "df_new[['CA'    , 'US'     ]] = pd.get_dummies(df_new['country'         ])[['CA'    ,'US'    ]]\ndf_new.head()",
  "generation": "df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])\nlogit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']])\nresults = logit_mod.fit()\nresults.summary()"
 },
 {
  "nbid": "NB_4221817",
  "file": "NB_4221817,predict_data-1.ipynb",
  "row_id": 23,
  "target_code": "predict_data[\"now_date\"          ] = pd.to_datetime(predict_data[\"\u5e74\u6708\"    ], format=\"%Y%m\"      )\npredict_data[\"start_date\"            ] = pd.to_datetime(predict_data[\"start_date\"            ])\nfrom dateutil.relativedelta import relativedelta\npredict_data[\"period\"        ] = None\nfor i in range(len(predict_data)):\n    delta = relativedelta(predict_data[\"now_date\"          ][i], predict_data[\"start_date\"            ][i])\n    predict_data[\"period\"        ][i] = delta.years*12 + delta.months\npredict_data.head()",
  "generation": "predict_data['now_date'] = '2019-03-31'\npredict_data['now_date'] = pd.to_datetime(predict_data['now_date'])\npredict_data['start_date'] = pd.to_datetime(predict_data['start_date'])\npredict_data['period'] = None\nfor i in range(len(predict_data)):\n    delta = relativedelta(predict_data['now_date'][i], predict_data['start_date'][i])\n    predict_data['period'][i] = int(delta.years*12 + delta.months)\npredict_data.head()"
 },
 {
  "nbid": "NB_4248912",
  "file": "NB_4248912,data-1.ipynb",
  "row_id": 4,
  "target_code": "data['body_len'          ]=data['body_text'           ].apply(lambda x:len(x)-x.count(' '   ))\ndata['punct%'        ] =data['body_text'           ].apply(lambda x:count_punct(x))\ndata.head()",
  "generation": "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\ndata['punct%'] = data['body_text'].apply(lambda x: round(x.count(\"%\")/(len(x) - x.count(\" \"))*100, 2))\ndata.head()"
 },
 {
  "nbid": "NB_4256732",
  "file": "NB_4256732,df,df1,df2-1.ipynb",
  "row_id": 7,
  "target_code": "scaler = MinMaxScaler()\nscaler.fit(df[['Income'        ]])\ndf['Income'        ] = scaler.transform(df[['Income'        ]])\nscaler = MinMaxScaler()\nscaler.fit(df[['Age'     ]])\ndf['Age'     ] = scaler.transform(df[['Age'     ]])\ndf.head()",
  "generation": "scaler = MinMaxScaler()\ndf[['Age', 'Income']] = scaler.fit_transform(df[['Age', 'Income']])\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(df[['Age', 'Income']])\ndf['Cluster'] = kmeans.labels_\ndf.head(5)"
 },
 {
  "nbid": "NB_4259218",
  "file": "NB_4259218,data-1.ipynb",
  "row_id": 9,
  "target_code": "data = data.set_index('Date'      )\ndata.head()",
  "generation": "data = data.set_index('Date')\ndata.head()"
 },
 {
  "nbid": "NB_4259218",
  "file": "NB_4259218,data-1.ipynb",
  "row_id": 16,
  "target_code": "data = data.resample(rule = 'M'    ).mean() \ndata.head(10)",
  "generation": "data.index=pd.to_datetime(data.index)\ndata_monthly=data.resample('M').mean()\ndata_monthly.head()"
 },
 {
  "nbid": "NB_4259458",
  "file": "NB_4259458,df,vect,feature_names-1.ipynb",
  "row_id": 3,
  "target_code": "df= df.rename(columns={'Department Name'                 : 'department_name'                  , 'Review Text'              : 'review_text'              })\ndf['Rating'        ] = df['Rating'        ].map({1: 'negative'           , 2: 'negative'           , 3: 'negative'           , 4: 'positive'           , 5:'positive'          })\ndf = df[(df.department_name==\"Dresses\"         )]\ndf.head()",
  "generation": "df = df.rename(columns={'Review Text': 'review_text', 'Department Name': 'department_name'})\ndf['Rating'] = df['Rating'].apply(lambda x: 'positive' if x > 3 else 'negative')\ndf = df[df['department_name'] == 'Dresses']"
 },
 {
  "nbid": "NB_4259458",
  "file": "NB_4259458,df,vect,feature_names-1.ipynb",
  "row_id": 4,
  "target_code": "df = df[['review_text'             , 'Rating'         ]]\ndf = df.dropna()\ndf.head()",
  "generation": "relevant_columns = ['review_text', 'Rating']\ndf = df[relevant_columns].dropna()\ndf.head()"
 },
 {
  "nbid": "NB_4264769",
  "file": "NB_4264769,df_cb,df3-1.ipynb",
  "row_id": 25,
  "target_code": "from sklearn.preprocessing import LabelEncoder\ndf_cb['negara'        ] = LabelEncoder().fit_transform(df_cb['negara'        ])\ndf_cb['genre'       ] = LabelEncoder().fit_transform(df_cb['genre'       ])\ndf_cb.head()",
  "generation": "df_cb = df_cb.reset_index()\ndf_cb = df_cb[['movie', 'negara', 'rating', 'durasi', 'genre', 'tahun']]\ndf_cb['negara'] = 0\ndf_cb['genre'] = df_cb['genre'].replace({'Action': 0, 'Drama': 3})\ndf_cb = df_cb.set_index('movie')\ndf_cb.head()"
 },
 {
  "nbid": "NB_4268308",
  "file": "NB_4268308,df_new-1.ipynb",
  "row_id": 85,
  "target_code": "df_new['CA_page'         ] = df_new['CA'    ] * df_new['ab_page'         ]\ndf_new['UK_page'         ] = df_new['UK'    ] * df_new['ab_page'         ]\ndf_new.head()",
  "generation": "df2['intercept'] = 1\ncountries_df = pd.read_csv('./countries.csv')\nnew_df = df2.set_index('user_id').join(countries_df.set_index('user_id'))\ndummies_df = pd.get_dummies(new_df['country'])\nnewer_df = new_df.join(dummies_df)\nnewer_df['CA_page'] = newer_df['CA'] * newer_df['ab_page']\nnewer_df['UK_page'] = newer_df['UK'] * newer_df['ab_page']\nnewer_df.head()"
 },
 {
  "nbid": "NB_4277329",
  "file": "NB_4277329,df-1.ipynb",
  "row_id": 2,
  "target_code": "df.experience = df.experience.fillna('zero'      )\ndf",
  "generation": "df['experience'] = df['experience'].apply(lambda x: w2n.word_to_num(x) if isinstance(x, str) else x)\ndf['experience'] = df['experience'].fillna(0)\ndf['experience'] = df['experience'].astype(int)\ndf['experience'] = df['experience'].apply(lambda x: 'zero' if x == 0 else x)\ndf"
 },
 {
  "nbid": "NB_4287965",
  "file": "NB_4287965,CreditData-1.ipynb",
  "row_id": 6,
  "target_code": "RaceDummy = pd.get_dummies(CreditData.Ethnicity, prefix = 'Race'       )\ndel RaceDummy['Race_African American'                       ]\nGenderDummy = pd.get_dummies(CreditData.Gender, prefix = 'Gender'         )\ndel GenderDummy['Gender_ Male'              ]\nMarriedDummy = pd.get_dummies(CreditData.Married, prefix = 'Married'          )\ndel MarriedDummy['Married_No'            ]\nStudentDummy = pd.get_dummies(CreditData.Student, prefix = 'Student'          )\ndel StudentDummy['Student_No'            ]\nCreditData = pd.concat([CreditData, RaceDummy,GenderDummy,MarriedDummy,StudentDummy], axis=1)\nCreditData.head()",
  "generation": "CreditData['Gender'] = CreditData['Gender'].map({' Male': 1, 'Female': 0})\nCreditData['Student'] = CreditData['Student'].map({'Yes': 1, 'No': 0})\nCreditData['Married'] = CreditData['Married'].map({'Yes': 1, 'No': 0})\nCreditData = pd.get_dummies(CreditData, columns=['Ethnicity'])\nCreditData.head()"
 },
 {
  "nbid": "NB_4319900",
  "file": "NB_4319900,df,X-1.ipynb",
  "row_id": 2,
  "target_code": "df=df.drop(\"Date\"      ,axis=1,inplace=False);\ndf=df.drop(\"Vol.\"      ,axis=1,inplace=False);\ndf=df.drop(\"Change %\"          ,axis=1,inplace=False);\ndf=df.drop(\"Low\"     ,axis=1,inplace=False);\ndf=df.drop(\"High\"      ,axis=1,inplace=False);\ndf=df.drop(\"Open\"      ,axis=1,inplace=False);\ny =  df[\"Price\"       ]\nX = df.drop([\"Price\"       ],axis=1,inplace=False)\ndf.head(10)",
  "generation": "df = df[['Price', 'Date_m']]\ndf = df.reset_index(drop=True)\ndf.head()"
 },
 {
  "nbid": "NB_4330450",
  "file": "NB_4330450,df2-1.ipynb",
  "row_id": 86,
  "target_code": "df2 = df2.set_index('user_id'         ).join(df_countries.set_index('user_id'         ))\ndf2.reset_index(inplace=True)\ndf2.head()",
  "generation": "df2 = pd.merge(df2, df_countries, on='user_id', how='left')\ndf2.head()"
 },
 {
  "nbid": "NB_4330450",
  "file": "NB_4330450,df2-1.ipynb",
  "row_id": 88,
  "target_code": "df2[['CA'    ,'UK'    ,'US'    ]] = pd.get_dummies(df2.country)\ndf2.head()",
  "generation": "df2[['CA', 'UK', 'US']] = pd.get_dummies(df2['country'])\ndf2.drop(['country'], axis=1, inplace=True)\ndf2.head()"
 },
 {
  "nbid": "NB_4333643",
  "file": "NB_4333643,df_new-1.ipynb",
  "row_id": 84,
  "target_code": "df_new[['UK'    , 'US'     , 'CA'     ]] = pd.get_dummies(df_new['country'         ])\ndf_new.head()",
  "generation": "df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])\ndf_new['intercept'] = 1\nlogit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK']])\nresults = logit_mod.fit()\nresults.summary()"
 },
 {
  "nbid": "NB_4353500",
  "file": "NB_4353500,iris_df-1.ipynb",
  "row_id": 7,
  "target_code": "iris_class = iris_df['Class'       ]\niris_dfcopy = iris_df.copy\niris_df = iris_df.drop(iris_df.columns[[4]], axis=1)\niris_df.head()",
  "generation": "labels = iris_df['Class']\niris_df = iris_df.drop('Class', axis=1)\niris_df.head()"
 },
 {
  "nbid": "NB_1000025",
  "file": "NB_1000025,train,X_test-1.ipynb",
  "row_id": 37,
  "target_code": "train[\"weekday\"         ] = train[\"datetime\"          ].dt.dayofweek\ntrain.head()\ntest[\"weekday\"         ] = test[\"datetime\"          ].dt.dayofweek\ntrain.head()",
  "generation": "train[\"weekday\"] = train[\"datetime\"].dt.weekday\ntrain.head()\n\ntest[\"weekday\"] = test[\"datetime\"].dt.weekday\ntest.head()"
 },
 {
  "nbid": "NB_1002394",
  "file": "NB_1002394,df2-1.ipynb",
  "row_id": 16,
  "target_code": "df2['Active'        ] = df2['Confirmed'           ] - df2['Deaths'        ] - df2['Recovered'           ]\ndf2.head(2)",
  "generation": "df2['Active'] = df2['Confirmed'] - df2['Deaths'] - df2['Recovered']\ndf2.head(2)"
 },
 {
  "nbid": "NB_1004882",
  "file": "NB_1004882,train,Y-1.ipynb",
  "row_id": 96,
  "target_code": "train[\"SalePrice\"           ]=train[\"Sale Price\"            ]\ntrain.drop([\"Sale Price\"            ,\"2ndFlrSF\"          ,\"TypeOfSale\"            ,\"Condition\"           ],axis=1,inplace=True)\ntrain.head()",
  "generation": "train[\"TotalBsmtSF\"           ] = train[\"TotalBsmtSF\"           ].fillna(0)\ntrain[\"MSZone\"               ] = train[\"MSZoning\"              ]\ntrain[\"Neighbor\"             ] = train[\"Neighborhood\"          ]\ntrain[\"BuildingType\"         ] = train[\"BldgType\"              ]\ntrain[\"HouseStyle(Story)\"    ] = train[\"HouseStyle\"            ].apply(lambda x: x.split(\"St\")[0])\ntrain[\"Roof\"                 ] = train[\"RoofStyle\"             ]\ntrain[\"Exterior\"             ] = train[\"Exterior1st\"           ]\ntrain[\"FoundationStructure\"  ] = train[\"Foundation\"            ]\ntrain[\"Electricity\"          ] = train[\"Electrical\"            ]\ntrain[\"Garage\"               ] = train[\"GarageType\"            ]\ntrain.drop([\"MSZoning\",\"Neighborhood\",\"BldgType\",\"HouseStyle\",\"RoofStyle\",\"Exterior1st\",\"Foundation\",\"Electrical\",\"GarageType\"],axis=1,inplace=True)\ntrain.head()"
 },
 {
  "nbid": "NB_1010437",
  "file": "NB_1010437,train_df,y-1.ipynb",
  "row_id": 22,
  "target_code": "train_df = train_df.replace({'Game ID'         :{'Sudoku #1'           : '1'    ,'Sudoku #2'           : '2'    ,'Sudoku #3'           : '3'    ,\n                                       'Sudoku #4'                                                  : '4'    ,'Sudoku #5'           : '5'    ,'Sudoku #6'           : '6'    ,\n                                       'Sudoku #7'                                                  : '7'    ,'Sudoku #8'           : '8'    ,'Sudoku #9'           : '9'    ,\n                                       'Sudoku #10'                                                   : '10'     ,'Sudoku #11'            : '11'     ,'Sudoku #12'            : '12'     ,\n                                       'Sudoku #13'                                                   : '13'     ,'Sudoku #14'            : '14'     ,'Sudoku #15'            : '15'     ,\n                                       'Sudoku #16'                                                   : '16'     ,'Sudoku #17'            : '17'     ,'Sudoku #18'            : '18'     ,\n                                       'Sudoku #19'                                                   : '19'     ,'Sudoku #20'            : '20'     ,'Sudoku #21'            : '21'     ,\n                                       'Sudoku #22'                                                   : '22'     ,'Sudoku #23'            : '23'     ,'Sudoku #24'            : '24'     ,\n                                       'Sudoku #25'                                                   : '25'     ,'Sudoku #26'            : '26'     }})\ntrain_df",
  "generation": "train_df = pd.merge(test_df, df, on='Batch ID')\ntrain_df = train_df.reset_index()\ntrain_df['Game ID'] = train_df.groupby('Batch ID').cumcount() + 1\ntrain_df['Game ID'] = train_df['Game ID'].astype(str)\ntrain_df['Game ID'] = '1'\ntrain_df = train_df.set_index('Batch ID')\ntrain_df"
 },
 {
  "nbid": "NB_1576738",
  "file": "NB_1576738,df_wine,wino_df-1.ipynb",
  "row_id": 63,
  "target_code": "df_wine['qual/alc ratio'                ] = df_wine['quality'         ]/df_wine['alcohol'         ]\ndf_wine.head(2)",
  "generation": "df_wine['qual/alc ratio'] = df_wine['quality'] / df_wine['alcohol']\ndf_wine.head(2)"
 },
 {
  "nbid": "NB_1579124",
  "file": "NB_1579124,df-1.ipynb",
  "row_id": 40,
  "target_code": "df = df.drop('Code'      , axis=1)\ndf",
  "generation": "df.drop('Code', axis=1, inplace=True)\ndf.columns = ['Entity', 'Year', 'Electricity consumption']\ndf.head()"
 },
 {
  "nbid": "NB_1579124",
  "file": "NB_1579124,df-1.ipynb",
  "row_id": 44,
  "target_code": "df = df.drop('Year'      , axis=1)\ndf",
  "generation": "df = df.drop(['Year'], axis=1)\ndf = df[['Entity', 'Electricity consumption']]\ndf.head()"
 },
 {
  "nbid": "NB_1579124",
  "file": "NB_1579124,df-1.ipynb",
  "row_id": 45,
  "target_code": "df = df.pivot(columns='Entity'        , values='Electricity consumption'                         )\ndf",
  "generation": "df = df.drop(['Unnamed: 0_level_0', 'Unnamed: 1_level_1', 'Unnamed: 2_level_1'], axis=1)\ndf = df.set_index('Year')\ndf.head()"
 },
 {
  "nbid": "NB_1581112",
  "file": "NB_1581112,df,y-1.ipynb",
  "row_id": 19,
  "target_code": "df = df[df['lower'       ]!='NaN'     ]\ndf",
  "generation": "df = df[df['lower'].apply(lambda x: len(x) >= 3)]\ndf.reset_index(drop=True, inplace=True)\ndf"
 },
 {
  "nbid": "NB_1581112",
  "file": "NB_1581112,df,y-1.ipynb",
  "row_id": 45,
  "target_code": "df['comp_score'            ] = df['compound'          ].apply(lambda c: 'pos'       if c >=0 else 'neg'      )\ndf.head(20)",
  "generation": "df['comp_score'] = df['compound'].apply(lambda c: 'pos' if c > 0 else 'neg')\ndf.head()"
 },
 {
  "nbid": "NB_1583769",
  "file": "NB_1583769,data_v3-1.ipynb",
  "row_id": 19,
  "target_code": "data_v3['name_length'             ] = data_v3.name.apply(lambda x: len(x))\ndata_v3['name_num_words'                ] = data_v3.name.apply(lambda x: len(x.split()))\ndata_v3.head(2)",
  "generation": "data_v3['name_length'] = data_v3['name'].apply(len)\ndata_v3['name_num_words'] = data_v3['name'].apply(lambda x: len(x.split()))\ndata_v3.head(2)"
 },
 {
  "nbid": "NB_1583769",
  "file": "NB_1583769,data_v3-1.ipynb",
  "row_id": 21,
  "target_code": "data_v3 = data_v3.assign(\n    launch_month = data_v3.launched.dt.month,\n    duration_launched_month = ((data_v3.deadline - data_v3.launched)/np.timedelta64(1, 'M'    )),\n    launch_year = data_v3.launched.dt.year,\n    deadline_year = data_v3.deadline.dt.year,\n    deadline_month = data_v3.deadline.dt.month\n)\ndata_v3.head(2)",
  "generation": "data_v3['deadline'] = pd.to_datetime(data_v3['deadline'])\ndata_v3['launched'] = pd.to_datetime(data_v3['launched'])\ndata_v3['launch_month'] = data_v3['launched'].dt.month\ndata_v3['duration_launched_month'] = (data_v3['deadline'] - data_v3['launched']) / np.timedelta64(1, 'M')\ndata_v3['launch_year'] = data_v3['launched'].dt.year\ndata_v3['deadline_year'] = data_v3['deadline'].dt.year\ndata_v3['deadline_month'] = data_v3['deadline'].dt.month\ndata_v3.head(2)"
 },
 {
  "nbid": "NB_1583769",
  "file": "NB_1583769,data_v3-1.ipynb",
  "row_id": 23,
  "target_code": "data_v3['usd_goal_per_backer'                     ] = np.where(data_v3['backers'         ]>0, \n                                          data_v3['usd_goal_real'               ]/data_v3['backers'         ],\n                                         0)\ndata_v3.head(2)",
  "generation": "data_v3['usd_goal_per_backer'] = data_v3['usd_goal_real'] / data_v3['backers']\ndata_v3"
 },
 {
  "nbid": "NB_1583769",
  "file": "NB_1583769,data_v3-1.ipynb",
  "row_id": 25,
  "target_code": "data_v3 = data_v3.assign(\n    usd_goal_per_month = data_v3.usd_goal_real/data_v3.duration_launched_month,\n    backers_per_month = data_v3.backers/data_v3.duration_launched_month\n)\ndata_v3.head(2)",
  "generation": "data_v3['usd_goal_per_month'] = data_v3['usd_goal_real']/data_v3['duration_launched_month']\ndata_v3['backers_per_month'] = data_v3['backers']/data_v3['duration_launched_month']\ndata_v3.head()"
 },
 {
  "nbid": "NB_1583769",
  "file": "NB_1583769,data_v3-1.ipynb",
  "row_id": 27,
  "target_code": "data_v3['usd_goal_per_month_per_backer'                               ] = np.where(data_v3['backers'         ]>0, \n                                                    ((data_v3['usd_goal_real'               ]/data_v3['duration_launched_month'                         ])\n                                                     /data_v3['backers'         ])\n                                                    ,0)\ndata_v3.head(2)",
  "generation": "data_v3['name_num_words'] = data_v3.name.apply(lambda x: len(x.split()))\ndata_v3 = data_v3.assign(\n    launch_month = data_v3.launched.dt.month,\n    duration_launched_month = ((data_v3.deadline - data_v3.launched)/np.timedelta64(1, 'M')),\n    launch_year = data_v3.launched.dt.year,\n    deadline_year = data_v3.deadline.dt.year,\n    deadline_month = data_v3.deadline.dt.month,\n    usd_goal_per_backer = np.where(data_v3['backers']>0, data_v3['usd_goal_real']/data_v3['backers'], 0),\n    usd_goal_per_month = data_v3.usd_goal_real/data_v3.duration_launched_month,\n    backers_per_month = data_v3.backers/data_v3.duration_launched_month,\n    usd_goal_per_month_per_backer = np.where(data_v3['backers']>0, data_v3['usd_goal_real']/(data_v3.duration_launched_month*data_v3['backers']), 0)\n)\ndata_v3.head(2)"
 },
 {
  "nbid": "NB_1583769",
  "file": "NB_1583769,data_v3-1.ipynb",
  "row_id": 29,
  "target_code": "data_v3['main_category-category'                        ] = data_v3[['main_category'               , 'category'           ]].apply(lambda x: '-'    .join(x), axis=1)\ndata_v3.head(2)",
  "generation": "data_v3 = data_v3.assign(main_category_category=data_v3.main_category + '-' + data_v3.category)"
 },
 {
  "nbid": "NB_1583867",
  "file": "NB_1583867,df-1.ipynb",
  "row_id": 8,
  "target_code": "df['date'      ] = pd.to_datetime(df['Date of Launch'                ])\ndf['year'      ], df['month'       ] = df['date'      ].dt.year, df['date'      ].dt.month\ndf.head(2).T",
  "generation": "df['Date of Launch'] = pd.to_datetime(df['Date of Launch'])\ndf"
 },
 {
  "nbid": "NB_1586291",
  "file": "NB_1586291,data,state,confirmed-1.ipynb",
  "row_id": 10,
  "target_code": "data=data.drop(['State_Notes'             ,'Delta_Deaths'              ,'Delta_Recovered'                 ,'Delta_Confirmed'                 ,\n                'State_code'                            , 'Migrated_Other'                 , 'Last_Updated_Time'                    ],axis=1)\ndata=data.drop([0])\ndata.head()",
  "generation": "data = data.drop(labels=['Last_Updated_Time', 'Migrated_Other', 'State_code', 'Delta_Confirmed', 'Delta_Recovered', 'Delta_Deaths', 'State_Notes'], axis=1)\ndata = data.drop(index=0)\ndata = data.reset_index(drop=True)\ndata.head()"
 },
 {
  "nbid": "NB_1593268",
  "file": "NB_1593268,tips-1.ipynb",
  "row_id": 21,
  "target_code": "tips = tips.drop('Unnamed: 0'            , 1)\ntips.head()",
  "generation": "tips.drop('Unnamed: 0', axis=1, inplace=True)\ntips.head()"
 },
 {
  "nbid": "NB_1598251",
  "file": "NB_1598251,abalone_data,sizes-1.ipynb",
  "row_id": 4,
  "target_code": "abalone_data['Age'     ] = abalone_data['Rings'       ] + 1.5\nabalone_data.head(5)",
  "generation": "abalone_data['Age'] = abalone_data['Rings'] + 1.5\nabalone_data.drop('Rings', axis=1, inplace=True)\nabalone_data.head()"
 },
 {
  "nbid": "NB_1602096",
  "file": "NB_1602096,df-1.ipynb",
  "row_id": 12,
  "target_code": "columns = [\"CO_MUNICIPIO_RESIDENCIA\"                         , \"CO_UF_RESIDENCIA\"                   , \"NO_MUNICIPIO_RESIDENCIA\"                          , \"CO_MUNICIPIO_NASCIMENTO\"                          ,\n           \"NO_MUNICIPIO_NASCIMENTO\"                                    ]\ndf = df.drop(columns, axis=1)\ndf.head()",
  "generation": "df = df.drop(\"CO_MUNICIPIO_RESIDENCIA\", axis=1)\ndf = df.drop(\"NO_MUNICIPIO_RESIDENCIA\", axis=1)\ndf = df.drop(\"CO_UF_RESIDENCIA\", axis=1)\ndf = df.drop(\"CO_UF_NASCIMENTO\", axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_1602096",
  "file": "NB_1602096,df-1.ipynb",
  "row_id": 14,
  "target_code": "remove_other_types = df[(df.TP_NACIONALIDADE!=1)].index\ndf = df.drop(remove_other_types, axis=0)\ndf = df.drop(\"TP_NACIONALIDADE\"                  , axis=1)\ndf.head()",
  "generation": "df = df_data.copy()\ndf = df.drop(['Unnamed: 0', 'CO_MUNICIPIO_RESIDENCIA', 'CO_UF_RESIDENCIA', 'NO_MUNICIPIO_RESIDENCIA', \n              'CO_MUNICIPIO_NASCIMENTO', 'NO_MUNICIPIO_NASCIMENTO'], axis=1)\ndf['TP_ANO_CONCLUIU'] = df['TP_ANO_CONCLUIU'].replace(0, df['TP_ST_CONCLUSAO'] - 1)\ndf.head()"
 },
 {
  "nbid": "NB_1602096",
  "file": "NB_1602096,df-1.ipynb",
  "row_id": 16,
  "target_code": "columns_drop = [\"CO_UF_NASCIMENTO\"                  ,\"CO_ESCOLA\"           , \"CO_MUNICIPIO_ESC\"                   , \"NO_MUNICIPIO_ESC\"                   , \"CO_UF_ESC\"            , \"SG_UF_ESC\"            ]\ndf = df.drop(columns_drop, axis=1)\ndf.head()",
  "generation": "df[\"TP_ESCOLA\"] = df[\"TP_ESCOLA\"].fillna(1)\ndf"
 },
 {
  "nbid": "NB_1602152",
  "file": "NB_1602152,car_df-1.ipynb",
  "row_id": 6,
  "target_code": "car_df['carcompany'            ] = car_df['CarName'         ].apply(lambda x: x.split(' '   )[0])\ncar_df.drop(['CarName'         ],axis=1,inplace=True)\ncar_df.rename(columns={'carcompany'            : 'CarName'          }, inplace=True)\ncar_df.drop(['car_ID'        ],axis=1,inplace=True)\ncar_df.reset_index()\ncar_df.head()",
  "generation": "car_df.loc[:, 'CarName'    ] = car_df.loc[:, 'CarName'                  ].apply(lambda x: x.split(' ')[0])\ncar_df.drop(columns=['car_ID', 'CarName'], inplace=True)\ncar_df.rename(columns={'carcompany': 'CarName'}, inplace=True)\ncar_df.head()"
 },
 {
  "nbid": "NB_4375952",
  "file": "NB_4375952,df3-1.ipynb",
  "row_id": 89,
  "target_code": "df3[['CA'    ,'US'    ,'UK'    ]]=pd.get_dummies(df3['country'         ])\ndf3.head()",
  "generation": "df_countries[['CA', 'UK', 'US']] = pd.get_dummies(df_countries['country'])\ndf3 = df_countries.join(df2.set_index('user_id'), on='user_id')\ndf3.head()"
 },
 {
  "nbid": "NB_4376624",
  "file": "NB_4376624,train,labels,X_train,y_train,predictions-1.ipynb",
  "row_id": 9,
  "target_code": "sex = pd.get_dummies(train['Sex'     ], drop_first = True)\nembark = pd.get_dummies(train['Embarked'          ],drop_first = True)\ntrain.drop(['Sex'     ,'Embarked'          ,'Name'      ,'Ticket'        ], axis =1, inplace = True)\ntrain = pd.concat([train,sex, embark],axis = 1)\ntrain.head()",
  "generation": "train['Sex'] = train['Sex'].apply(lambda x: 1 if x == 'male' else 0)\ntrain['Embarked'] = train['Embarked'].map({'S': 1, 'C': 0, 'Q': 0})\ntrain = train.drop(['Name', 'Ticket', 'Cabin'], axis=1)\ntrain = train.dropna()\ntrain = pd.get_dummies(train, columns=['Pclass'])\ntrain.head()"
 },
 {
  "nbid": "NB_4407134",
  "file": "NB_4407134,df3-1.ipynb",
  "row_id": 41,
  "target_code": "df3.head()",
  "generation": "df3['incshr90'] = 100 * df3['inexp90'] / (df3['inexp90'] + df3['chexp90'])\ndf3['incshr88'] = 100 * df3['inexp88'] / (df3['inexp88'] + df3['chexp88'])\ndf3.head()"
 },
 {
  "nbid": "NB_4457599",
  "file": "NB_4457599,train-1.ipynb",
  "row_id": 10,
  "target_code": "train[\"text\"      ] = train[\"text\"      ].map(sms_to_words)\ntrain.head()",
  "generation": "vectorizer = CountVectorizer(stop_words='english')\ntrain[\"text\"] = train[\"text\"].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ntrain[\"text\"] = train[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x.isalpha()))\ntrain[\"text\"] = vectorizer.fit_transform(train[\"text\"]).toarray()\ntrain.head()"
 },
 {
  "nbid": "NB_4466843",
  "file": "NB_4466843,car-1.ipynb",
  "row_id": 14,
  "target_code": "label = []\nfor index, row in car.iterrows():\n    if row[\"Fuel_Type\"           ] == \"Petrol\"         :\n        label.append(0)\n    elif row[\"Fuel_Type\"           ] == \"Diesel\"         :\n        label.append(1)\n    elif row[\"Fuel_Type\"           ] == \"Electric\"           :\n        label.append(2)\n    elif row[\"Fuel_Type\"           ] == \"CNG\"      :\n        label.append(3)\n    else:\n        label.append(4)\ncar[\"Fuel_Type1\"            ] = label\ncar = car.drop(columns=['Fuel_Type'           ])\ncar.tail()",
  "generation": "car['Fuel_Type1'] = pd.get_dummies(car['Fuel_Type'], drop_first=True)\ncar.head()"
 },
 {
  "nbid": "NB_4466843",
  "file": "NB_4466843,car-1.ipynb",
  "row_id": 15,
  "target_code": "label = []\nfor index, row in car.iterrows():\n    if row[\"Owner_Type\"            ] == \"First\"        :\n        label.append(1)\n    elif row[\"Owner_Type\"            ] == \"Second\"         :\n        label.append(2)\n    elif row[\"Owner_Type\"            ] == \"Thrid\"        :\n        label.append(3)\n    else:\n        label.append(4)\ncar[\"Owner_Type1\"             ] = label\ncar = car.drop(columns=['Owner_Type'            ])\ncar.tail()",
  "generation": "car['Owner_Type1'] = car['Owner_Type'].replace({'First': 1, 'Second': 2, 'Third': 3, 'Fourth & Above': 4})\ncar = car.drop(columns=['Owner_Type'])\ncar.tail()"
 },
 {
  "nbid": "NB_4467006",
  "file": "NB_4467006,df1-1.ipynb",
  "row_id": 3,
  "target_code": "df1['mainroad'          ]=df1['mainroad'          ].apply(cat)\ndf1['guestroom'           ]=df1['guestroom'           ].apply(cat)\ndf1['basement'          ]=df1['basement'          ].apply(cat)\ndf1['hotwaterheating'                 ]=df1['hotwaterheating'                 ].apply(cat)\ndf1['airconditioning'                 ]=df1['airconditioning'                 ].apply(cat)\ndf1['prefarea'          ]=df1['prefarea'          ].apply(cat)\ndf1.head()",
  "generation": "scaler = MinMaxScaler()\ndf1[['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'parking']] = scaler.fit_transform(df1[['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'parking']])\ndf1['mainroad'] = df1['mainroad'].map({'yes': 1, 'no': 0})\ndf1['guestroom'] = df1['guestroom'].map({'yes': 1, 'no': 0})\ndf1['basement'] = df1['basement'].map({'yes': 1, 'no': 0})\ndf1['hotwaterheating'] = df1['hotwaterheating'].map({'yes': 1, 'no': 0})\ndf1['airconditioning'] = df1['airconditioning'].map({'yes': 1, 'no': 0})\ndf1['prefarea'] = df1['prefarea'].map({'yes': 1, 'no': 0})\ndf1['furnishingstatus'] = df1['furnishingstatus'].map({'furnished': 2, 'semi-furnished': 1, 'unfurnished': 0})\ndf1.head()"
 },
 {
  "nbid": "NB_4469462",
  "file": "NB_4469462,df_new-1.ipynb",
  "row_id": 87,
  "target_code": "df_new = df_new.join(pd.get_dummies(df_new['country'         ]))\ndf_new.head()",
  "generation": "df_new[[\"CA\", \"UK\", \"US\"]] = pd.get_dummies(df_new[\"country\"])\ndf_new.head()"
 },
 {
  "nbid": "NB_4482622",
  "file": "NB_4482622,patsiendi_df-1.ipynb",
  "row_id": 26,
  "target_code": "patsiendi_df = pd.get_dummies(patsiendi_df)\npatsiendi_df",
  "generation": "patsiendi_df = pd.DataFrame({\"veregrupp\": [\"A\", \"AB\", \"A\", \"O\", \"O\"], \n                             \"valu\": [\"puudub\", \"puudub\", \"kerge\", \"intensiivne\", \"kerge\"]})\npatsiendi_df                            \nvalu_map = {\"puudub\": 0, \"kerge\": 1, \"intensiivne\": 2}\npatsiendi_df[\"valu\"] = patsiendi_df[\"valu\"].map(valu_map)\npatsiendi_df\npatsiendi_df = pd.get_dummies(patsiendi_df, columns=[\"veregrupp\"])\npatsiendi_df"
 },
 {
  "nbid": "NB_4503057",
  "file": "NB_4503057,df4-1.ipynb",
  "row_id": 88,
  "target_code": "df4[['CA'    , 'UK'     , 'US'     ]] = pd.get_dummies(df4['country'         ])\ndf4.head()",
  "generation": "df4 = pd.get_dummies(df4, columns=['country'])\ndf4.head()"
 },
 {
  "nbid": "NB_4505655",
  "file": "NB_4505655,df,Y-1.ipynb",
  "row_id": 14,
  "target_code": "df = df.drop(columns=['result1'         ])\ndf.head()",
  "generation": "df = df.drop(columns=['result1', 'date'])"
 },
 {
  "nbid": "NB_4505655",
  "file": "NB_4505655,df,Y-1.ipynb",
  "row_id": 17,
  "target_code": "label_encoder = preprocessing.LabelEncoder() \nlabel_encoder.fit(df['team1'       ].append(df['team2'       ])) \ndf['team1'       ] = label_encoder.transform(df['team1'       ]) \ndf['team2'       ] = label_encoder.transform(df['team2'       ])\ndf.head()",
  "generation": "le = preprocessing.LabelEncoder()\ndf['team1'] = le.fit_transform(df['team1'])\ndf['team2'] = le.transform(df['team2'])\ndf.head()"
 },
 {
  "nbid": "NB_4512415",
  "file": "NB_4512415,df,labels,tf_idf-1.ipynb",
  "row_id": 14,
  "target_code": "df = df.reset_index() \ndf = df.drop(columns = ['index'       ])\ndf.head()",
  "generation": "df.reset_index(drop=True, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_1033915",
  "file": "NB_1033915,df-1.ipynb",
  "row_id": 3,
  "target_code": "df.columns = [x.lower().replace('.'   , '_'    ) for x in df.columns]\ndf.head()",
  "generation": "df = df.rename(columns={'Sepal.Length': 'sepal_length', 'Sepal.Width': 'sepal_width', 'Petal.Length': 'petal_length', 'Petal.Width': 'petal_width', 'Species': 'species'})\ndf.head()"
 },
 {
  "nbid": "NB_1038302",
  "file": "NB_1038302,dfHormonas-1.ipynb",
  "row_id": 11,
  "target_code": "dfHormonas = dfHormonas.loc[dfHormonas.Progesterona>1.04]\ndfHormonas",
  "generation": "dfVizcacha['Tiempo'] = dfVizcacha['Tiempo'].astype(float)\ndfHormonas=dfVizcacha.dropna(subset=[\"Edad\",\"Progesterona\"])\ndfHormonas=dfHormonas.drop(74)\ndfHormonas"
 },
 {
  "nbid": "NB_1038302",
  "file": "NB_1038302,dfHormonas-1.ipynb",
  "row_id": 12,
  "target_code": "dfHormonas=dfHormonas.sort_values(by=\"Edad\"      , ascending=False)\ndfHormonas",
  "generation": "dfHormonas = dfVizcacha.dropna(subset=[\"Edad\",\"Progesterona\"])\ndfHormonas = dfHormonas.drop(74)\ndfHormonas = dfHormonas.loc[dfHormonas.Progesterona>1.04]"
 },
 {
  "nbid": "NB_1039431",
  "file": "NB_1039431,exp_design-1.ipynb",
  "row_id": 16,
  "target_code": "exp_design['exhaustion'            ] = ['multipep'           in ind for ind in exp_design['treatment'           ]]\nexp_design.head()",
  "generation": "le = preprocessing.LabelEncoder()\ndf['treatment'] = le.fit_transform(exp_design['treatment'])\npca = decomposition.PCA(n_components=2)\npca.fit(df.T)\npca_df = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=df.columns)\npca_df['treatment'] = exp_design['treatment']\npca_df['exhaustion'] = ((df == 0).sum(axis=1) >= 9)\npca_df.head()"
 },
 {
  "nbid": "NB_1044112",
  "file": "NB_1044112,covid_census_merged-1.ipynb",
  "row_id": 43,
  "target_code": "covid_census_merged['Covid_Deaths_Percent_Total_Population'                                       ] = covid_census_merged.apply(percent_population_deaths_covid, axis=1)\ncovid_census_merged.head()",
  "generation": "covid_census_merged['Covid_Deaths_Percent_Total_Population'] = covid_census_merged.apply(percent_population_deaths_covid, axis=1)\ncovid_census_merged = covid_census_merged[['State', 'Covid_Recovered', 'Covid_Deaths', 'Covid_Confirmed', 'id', 'Total population', 'RACE!!One race', 'RACE!!Two or more races', 'RACE!!One race!!White', 'RACE!!One race!!Black or African American', '85 years and over', 'Median age (years)', 'Under 18 years', '16 years and over', '18 years and over', '21 years and over', '62 years and over', '65 years and over', 'Covid_Confirmed_Percent_Total_Population', 'Covid_Deaths_Percent_Total_Population']]\ncovid_census_merged"
 },
 {
  "nbid": "NB_1052006",
  "file": "NB_1052006,ChinaBank-1.ipynb",
  "row_id": 9,
  "target_code": "ChinaBank['Close_diff_1'              ] = ChinaBank['Close'       ].diff(1)\nChinaBank['Close_diff_2'              ] = ChinaBank['Close_diff_1'              ].diff(1)\nChinaBank.head()",
  "generation": "ChinaBank['Close_diff_1'] = ChinaBank['Close'].diff(1)\nChinaBank['Close_diff_2'] = ChinaBank['Close'].diff(2)\nChinaBank.head()"
 },
 {
  "nbid": "NB_4529020",
  "file": "NB_4529020,star_wars,females-1.ipynb",
  "row_id": 6,
  "target_code": "star_wars = star_wars.rename(columns={\n        \"Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.\"                                                                                                                                                       : \"ranking_1\"            ,\n        \"Unnamed: 10\"                     : \"ranking_2\"            ,\n        \"Unnamed: 11\"                     : \"ranking_3\"            ,\n        \"Unnamed: 12\"                     : \"ranking_4\"            ,\n        \"Unnamed: 13\"                     : \"ranking_5\"            ,\n        \"Unnamed: 14\"                     : \"ranking_6\"            \n        })\nstar_wars.head()",
  "generation": "star_wars = star_wars.rename(columns={\n        \"Which of the following Star Wars films have you seen? Please select all that apply.\": \"seen_1\",\n        \"Unnamed: 4\": \"seen_2\",\n        \"Unnamed: 5\": \"seen_3\",\n        \"Unnamed: 6\": \"seen_4\",\n        \"Unnamed: 7\": \"seen_5\",\n        \"Unnamed: 8\": \"seen_6\"\n        })\n\nstar_wars.head()"
 },
 {
  "nbid": "NB_4529137",
  "file": "NB_4529137,newer_df-1.ipynb",
  "row_id": 86,
  "target_code": "newer_df = newer_df.drop(['US'    ], axis=1)\nnewer_df.head()",
  "generation": "newer_df = pd.get_dummies(new_df, columns=['country'], drop_first=True)\nnewer_df.head()"
 },
 {
  "nbid": "NB_4538389",
  "file": "NB_4538389,iris_df,y-1.ipynb",
  "row_id": 26,
  "target_code": "iris_df = pd.concat([iris_df,species_dummies],axis=1)\niris_df.head()",
  "generation": "iris_df = pd.read_csv('MachineLearning/LinearRegression/IrisData/iris.csv')\nspecies_dummies = pd.get_dummies(iris_df['Species'], prefix='Species')\niris_df = pd.concat([iris_df, species_dummies], axis=1)\niris_df.head()"
 },
 {
  "nbid": "NB_4547667",
  "file": "NB_4547667,df3-1.ipynb",
  "row_id": 91,
  "target_code": "df3[['CA'    , 'UK'     , 'US'     ]] = pd.get_dummies(df3['country'         ])\ndf3.head(3)",
  "generation": "df_c = pd.read_csv('countries.csv')\ndf2[['ab_page', 'old_page']] = pd.get_dummies(df2['landing_page'])\ndf3 = df2.join(df_c.set_index('user_id'), on='user_id', how='left')\ndf3 = pd.get_dummies(df3, columns=['country'], drop_first=True)\ndf3.head()"
 },
 {
  "nbid": "NB_4551589",
  "file": "NB_4551589,datesDf-1.ipynb",
  "row_id": 25,
  "target_code": "datesDf['Indice'        ] = list(range(len(datesDf.index)))\ndatesDf['Indice'        ] = datesDf['Indice'        ] + 1\ndatesDf.head(5)",
  "generation": "import pandas as pd\nimport matplotlib.pyplot as plt\ndates = ['2018-01-01', '2017-02-01', '2019-10-01']\nevento = ['A', 'B', 'C']\nvalores = [200, 300, 400]\ncategoria = ['XL', 'XL', 'XS']\nx = 300\n\n#tranforma o arrays em colunas no mesmo dataframe\ndatesDf = pd.DataFrame({'Data': dates, 'Evento': evento, 'Valores': valores, 'Categoria': categoria})\n\n#Criando colunas de datas\ndatesDf['Date'] = pd.to_datetime(datesDf['Data']) #padr\u00e3o dateTime\ndatesDf['ano'] = datesDf['Date'].map(lambda x: x.strftime('%Y'))\ndatesDf['Indice'] = datesDf.index + 1\n\ndatesDf.head()"
 },
 {
  "nbid": "NB_4551589",
  "file": "NB_4551589,datesDf-1.ipynb",
  "row_id": 37,
  "target_code": "datesDf['Valores10Perc'               ] = datesDf['Valores'         ] * 1.1 \ndatesDf['Valores20Perc'               ] = datesDf['Valores'         ].apply(lambda x: x * 1.2) \ndef doubleData(x):\n   return x * 2\ndatesDf['ValoresDobro'              ] = datesDf['Valores'         ].apply(doubleData) \ndef Status(x):\n  if x >= 300:\n    return 'faixa1'         \n  else:\n    return 'faixa0'         \ndatesDf['Status'        ] = datesDf['Valores'         ].apply(Status) \ndatesDf.head(5)",
  "generation": "datesDf['Valores10Perc'] = datesDf['Valores'] * 1.1\ndatesDf['Valores20Perc'] = datesDf['Valores'].apply(lambda x: x * 1.2)\ndatesDf['ValoresDobro'] = datesDf['Valores'].apply(lambda x: x * 2)\n\ndef classify(x):\n    if x < 300:\n        return 'faixa0'\n    else:\n        return 'faixa1'\n\ndatesDf['Status'] = datesDf['Valores'].apply(classify)\n\ndatesDf"
 },
 {
  "nbid": "NB_4551589",
  "file": "NB_4551589,datesDf-1.ipynb",
  "row_id": 41,
  "target_code": "datesDf['exemploNull'             ] = datesDf['exemploNull'             ].fillna(0.0) \ndatesDf.head(5)",
  "generation": "datesDf['exemploNull'] = None\ndatesDf['exemploNull'] = datesDf['exemploNull'].fillna(0)\ndatesDf.head()"
 },
 {
  "nbid": "NB_4592174",
  "file": "NB_4592174,data2-1.ipynb",
  "row_id": 8,
  "target_code": "data2 = data2[data2['country'         ].notna()]\ndata2 = data2[data2['province'          ].notna()]\ndata2 = data2[data2['winery'        ].notna()]\ndata2 = data2[data2['review_title'              ].notna()]\ndata2 = data2[data2['designation'             ].notna()]\ndata2 = data2[data2['points'        ].notna()]\ndata2 = data2[data2['price'       ].notna()]\nprint(data2.shape)\nprint(data2.info())\ndata2.head()",
  "generation": "data2 = data2[['country', 'review_title', 'designation', 'points', 'price', 'province', 'winery']]\ndata2.head()"
 },
 {
  "nbid": "NB_4593483",
  "file": "NB_4593483,joined_df-1.ipynb",
  "row_id": 86,
  "target_code": "joined_df['US_ab_page'            ] = joined_df['US'    ] * joined_df['ab_page'         ]\njoined_df['UK_ab_page'            ] = joined_df['UK'    ] * joined_df['ab_page'         ]\njoined_df.head()",
  "generation": "joined_df['US_ab_page'] = joined_df['US'] * joined_df['ab_page']\njoined_df['UK_ab_page'] = joined_df['UK'] * joined_df['ab_page']\njoined_df.head()"
 },
 {
  "nbid": "NB_4604476",
  "file": "NB_4604476,df_new-1.ipynb",
  "row_id": 86,
  "target_code": "country_dummies = pd.get_dummies(df_new.country)\ndf_new = df_new.join(country_dummies)\ndf_new['intercept'           ] = 1\ndf_new.head()",
  "generation": "df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])\ndf_new['intercept'] = 1\nlogit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US']])\nresults = logit_mod.fit()\nresults.summary()"
 },
 {
  "nbid": "NB_4634617",
  "file": "NB_4634617,df2-1.ipynb",
  "row_id": 62,
  "target_code": "df2['ab_page'         ] = df2['old_page'          ]\ndf2 = df2.drop(['old_page'          , 'new_page'           ], axis=1)\ndf2.head()",
  "generation": "df2['ab_page'] = df2['new_page']\ndf2['ab_page'] = df2['ab_page'].replace({0: 1, 1: 0})\ndf2 = df2.drop(['old_page', 'new_page'], axis=1)\ndf2.head()"
 },
 {
  "nbid": "NB_4645953",
  "file": "NB_4645953,df-1.ipynb",
  "row_id": 4,
  "target_code": "labelled_species = [\n    'Iris-setosa'                 ,\n    'Iris-versicolor'                     ,\n    'Iris-virginica'                    ,\n]\nfor idx, label in enumerate(labelled_species):\n    df.Species = df.Species.replace(label, idx)\ndf.head()",
  "generation": "species_dict = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\ndf['Species'] = df['Species'].replace(species_dict)\ndf.head()"
 },
 {
  "nbid": "NB_4662540",
  "file": "NB_4662540,df_new-1.ipynb",
  "row_id": 89,
  "target_code": "df_new[['CA'    , 'UK'     , 'US'     ]] = pd.get_dummies(df_new['country'         ])\ndf_new = df_new.drop('CA'    , axis=1)\ndf_new.head()",
  "generation": "df_new = pd.get_dummies(df_new, columns=['country'])\ndf_new"
 },
 {
  "nbid": "NB_8274616",
  "file": "NB_8274616,df-1.ipynb",
  "row_id": 3,
  "target_code": "df.Bedrooms = df.Bedrooms.fillna(median_bedroom)\ndf",
  "generation": "import math\nmedian_bedroom = math.floor(df.Bedrooms.median())\ndf['Bedrooms'] = df['Bedrooms'].fillna(median_bedroom)\ndf"
 },
 {
  "nbid": "NB_8311135",
  "file": "NB_8311135,df-1.ipynb",
  "row_id": 3,
  "target_code": "df.bedrooms = df.bedrooms.fillna(median_bed)\ndf",
  "generation": "df['bedrooms'] = df['bedrooms'].fillna(median_bed)\ndf.head()"
 },
 {
  "nbid": "NB_1062876",
  "file": "NB_1062876,drop_out_rate_df-1.ipynb",
  "row_id": 36,
  "target_code": "dropIndexs = drop_out_rate_df.index[[0,-1]]\ndrop_out_rate_df = drop_out_rate_df.drop(dropIndexs)\ndrop_out_rate_df = drop_out_rate_df.set_index('Level of Education - State'                            )\ndrop_out_rate_df",
  "generation": "drop_out_rate_df = drop_out_rate_df[['Level of Education - State', 'Primary - 2012-2013', 'Primary - 2014-2015', 'Primary - 2014-2015.1', 'Upper Primary - 2012-2013', 'Upper Primary - 2013-2014', 'Upper Primary - 2014-2015', 'Secondary - 2012-2013', 'Secondary - 2013-2014', 'Secondary - 2014-2015', 'Senior Secondary - 2012-2013', 'Senior Secondary - 2013-2014', 'Senior Secondary - 2014-2015']]\ndrop_out_rate_df.set_index('Level of Education - State', inplace=True)\ndrop_out_rate_df.index.name = 'Level of Education - State'\ndrop_out_rate_df.columns.name = 'Unnamed: 0_level_0'\ndrop_out_rate_df.reset_index(inplace=True)\ndrop_out_rate_df.head()"
 },
 {
  "nbid": "NB_1062876",
  "file": "NB_1062876,drop_out_rate_df-1.ipynb",
  "row_id": 39,
  "target_code": "drop_out_rate_df = drop_out_rate_df.dropna()\ndrop_out_rate_df",
  "generation": "drop_out_rate_df.loc['Goa'] = [0.2, 0.07, 11.15, 89607.0]"
 },
 {
  "nbid": "NB_1072034",
  "file": "NB_1072034,result_diff,y-1.ipynb",
  "row_id": 19,
  "target_code": "result = pd.merge(result, right=unemployment, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=unemployment_diff2, left_index=True, right_index=True)  \nresult_diff",
  "generation": "unemployment_diff2['Unemp_rate'] = unemployment_diff2['Unemp_rate'].apply(pd.to_numeric, errors='coerce')\nresult_diff = pd.merge(car_sales_diff2, right=population_diff2, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=unemployment_diff2, left_index=True, right_index=True)\nresult_diff"
 },
 {
  "nbid": "NB_1072034",
  "file": "NB_1072034,result_diff,y-1.ipynb",
  "row_id": 20,
  "target_code": "result = pd.merge(result, right=gold, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=gold_diff2, left_index=True, right_index=True)  \nresult_diff",
  "generation": "result = pd.merge(result, right=gold, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=gold_diff2, left_index=True, right_index=True)\nresult_diff"
 },
 {
  "nbid": "NB_1072034",
  "file": "NB_1072034,result_diff,y-1.ipynb",
  "row_id": 21,
  "target_code": "result = pd.merge(result, right=wti, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=wti_diff2, left_index=True, right_index=True)  \nresult_diff",
  "generation": "result = pd.merge(result, right=wti, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=wti_diff2, left_index=True, right_index=True)  \nresult_diff"
 },
 {
  "nbid": "NB_1072034",
  "file": "NB_1072034,result_diff,y-1.ipynb",
  "row_id": 22,
  "target_code": "result = pd.merge(result, right=djia, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=djia_diff2, left_index=True, right_index=True)  \nresult_diff",
  "generation": "djia_diff2 = djia_diff1.diff().fillna(djia_diff1)\nresult = pd.merge(car_sales, right=population, left_index=True, right_index=True)\nresult_diff = pd.merge(car_sales_diff2, right=population_diff2, left_index=True, right_index=True)\nresult = pd.merge(result, right=unemployment, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=unemployment_diff2, left_index=True, right_index=True)  \nresult = pd.merge(result, right=gold, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=gold_diff2, left_index=True, right_index=True)  \nresult = pd.merge(result, right=wti, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=wti_diff2, left_index=True, right_index=True)  \nresult = pd.merge(result, right=djia_diff2, left_index=True, right_index=True)\nresult_diff"
 },
 {
  "nbid": "NB_1072034",
  "file": "NB_1072034,result_diff,y-1.ipynb",
  "row_id": 24,
  "target_code": "result = pd.merge(result, right=house, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=house_diff2, left_index=True, right_index=True)  \nresult_diff",
  "generation": "result = pd.merge(car_sales, right=population, left_index=True, right_index=True)\nresult_diff = pd.merge(car_sales_diff2, right=population_diff2, left_index=True, right_index=True)\nresult = pd.merge(result, right=unemployment, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=unemployment_diff2, left_index=True, right_index=True)  \nresult = pd.merge(result, right=gold, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=gold_diff2, left_index=True, right_index=True)  \nresult = pd.merge(result, right=wti, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=wti_diff2, left_index=True, right_index=True)  \nresult = pd.merge(result, right=djia, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=djia_diff2, left_index=True, right_index=True)  \nresult = pd.merge(result, right=gdp, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=gdp_diff2, left_index=True, right_index=True)  \nresult = pd.merge(result, right=house, left_index=True, right_index=True)\nresult_diff = pd.merge(result_diff, right=house_diff2, left_index=True, right_index=True)  \nresult_diff"
 },
 {
  "nbid": "NB_1077949",
  "file": "NB_1077949,lift_df-1.ipynb",
  "row_id": 20,
  "target_code": "lift_df = lift_df.replace(0, .01)\nlift_df = 1/lift_df\nlift_df = lift_df.replace(np.inf, 0)\nnp.fill_diagonal(lift_df.values, 0)\nlift_df",
  "generation": "lift_df = pd.DataFrame(0, index=top_ten, columns=top_ten)\n\nfor lift in lift_sort:\n    lift_df.loc[lift[0][0], lift[0][1]] = lift[1]\n    lift_df.loc[lift[0][1], lift[0][0]] = lift[1]\n\nlift_df.div(lift_df.sum(axis=1), axis=0)"
 },
 {
  "nbid": "NB_1081250",
  "file": "NB_1081250,df,no_yr_df-1.ipynb",
  "row_id": 28,
  "target_code": "df['date'      ] = pd.to_datetime(df.date)\ndf['date'      ]= df['date'      ].dt.month\ndf.head()",
  "generation": "df['date'] = pd.to_datetime(df['date']).dt.to_period('M')\ndf = df.rename(columns={'date': 'month'})\ndf = df.set_index(['month', 'id'])\ndf.head()"
 },
 {
  "nbid": "NB_1081796",
  "file": "NB_1081796,dropped_cyclone_df,y_axis-1.ipynb",
  "row_id": 25,
  "target_code": "dropped_cyclone_df[\"DECADE\"        ]=dropped_cyclone_df[\"DECADE\"        ].astype({\"DECADE\"        :int})\ndropped_cyclone_df.drop(dropped_cyclone_df[dropped_cyclone_df[\"DECADE\"        ] <= 1940].index, inplace = True)\ndropped_cyclone_df.head()",
  "generation": "dropped_cyclone_df['DECADE'] = dropped_cyclone_df['DECADE'].astype(int)\ndropped_cyclone_df.head(2)"
 },
 {
  "nbid": "NB_1085688",
  "file": "NB_1085688,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.drop(['Serial No.'            ], axis=1)\ndf.head(n=10)",
  "generation": "df.drop('Serial No.', axis=1, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_1089532",
  "file": "NB_1089532,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.merge(facility_reference, left_on='Top position'              ,right_on='Height'        , how='left'      ).drop(['Top position'              , 'Height'         ], axis=1)\ndf.head()",
  "generation": "df['Facility'] = df['Top position'].apply(lambda x: facility_reference[facility_reference['Top position'] == x]['Facility'].values[0])\ndf = df[['Booking Time', 'Booking Status', 'Booking Reference Number', 'Booked for User Name', 'Booked for User Org Unit', 'Booked for User Email Address', 'Use Type', 'Purpose of Booking', 'Booking Date', 'Facility']]\ndf.head()"
 },
 {
  "nbid": "NB_1089532",
  "file": "NB_1089532,df-1.ipynb",
  "row_id": 6,
  "target_code": "df['Booking Time'              ] = df['Booking Time'              ].str.replace('-00:00'        ,'-23:59'        )\ndf['Booking Start'               ] = pd.to_datetime(df['Booking Date'              ] + ' '     + df['Booking Time'              ].str.split('-'   ).str[0])\ndf['Booking End'             ] = pd.to_datetime(df['Booking Date'              ] + ' '     + df['Booking Time'              ].str.split('-'   ).str[1])\ndf['Hours Booked'              ] = df['Booking End'             ] - df['Booking Start'               ]\ndf['Facility'          ] = df['Facility'          ].str.replace(u'\\xa0'       , u' '     )\ndf.head()",
  "generation": "df['Booking Start'] = pd.to_datetime(df['Booking Time'].str.split('-',expand=True)[0] + ':00 ' + df['Booking Date'],format='%H:%M %d/%m/%Y')\ndf['Booking End'] = pd.to_datetime(df['Booking Time'].str.split('-',expand=True)[1] + ':00 ' + df['Booking Date'],format='%H:%M %d/%m/%Y')\ndf['Hours Booked'] = df['Booking End'] - df['Booking Start']\ndf.head()"
 },
 {
  "nbid": "NB_1089532",
  "file": "NB_1089532,df-1.ipynb",
  "row_id": 7,
  "target_code": "df = df[pd.to_datetime(df['Booking Date'              ], dayfirst=True).dt.dayofweek.isin([6,5,4])]\ndf = df[~ (pd.to_datetime(df['Booking Date'              ], dayfirst=True).dt.dayofweek.isin([4])) & df['Booking Start'               ].dt.hour < 18]\ndf",
  "generation": "df['Booking Time'] = df['Booking Time'].str.replace('-00:00','-23:59')\ndf['Booking Start'] = pd.to_datetime(df['Booking Date'] + ' ' + df['Booking Time'].str.split('-').str[0])\ndf['Booking End'] = pd.to_datetime(df['Booking Date'] + ' ' + df['Booking Time'].str.split('-').str[1])\ndf['Hours Booked'] = df['Booking End'] - df['Booking Start']\ndf['Facility'] = df['Facility'].str.replace(u'\\xa0', u' ')\ndf = df.merge(facility_reference, left_on='Top position',right_on='Height', how='left').drop(['Top position', 'Height'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_1090268",
  "file": "NB_1090268,toplot-1.ipynb",
  "row_id": 31,
  "target_code": "toplot=toplot.unstack(level=0)\ntoplot",
  "generation": "df = df.groupby(['\u00e9v', 'szint']).count().unstack()['Unnamed: 1_level_0']\ndf.head()"
 },
 {
  "nbid": "NB_1090445",
  "file": "NB_1090445,df_bank,grouped_education,grouped_default,grouped_housing,grouped_loan-1.ipynb",
  "row_id": 4,
  "target_code": "df_bank['age'     ].astype('int64'       )\nlabels=['10-19'       ,'20-29'       ,'30-39'       ,'40-49'       ,'50-59'       ,'60-69'       ,'70-79'       ,'80-89'       ,'90-99'       ]\ndf_bank['age'     ] = pd.cut(df_bank.age,range(10,100,9),labels=labels)\ndf_bank.head()",
  "generation": "df_bank['age'] = pd.cut(df_bank['age'], [0, 19, 29, 39, 49, 59, 69, 100], labels=['10-19', '20-29', '30-39', '40-49', '50-59', '60-69', '70-100'])\ndf_bank.head()"
 },
 {
  "nbid": "NB_1090445",
  "file": "NB_1090445,df_bank,grouped_education,grouped_default,grouped_housing,grouped_loan-1.ipynb",
  "row_id": 7,
  "target_code": "pdays_labels = ['0-5'     ,'6-10'      ,'11-15'       ,'16-20'       ,'21-25'       ,'26-30'       ,'\u8fd1\u671f\u65e0\u8054\u7cfb'       ]\ndf_bank['pdays'       ] = pd.cut(df_bank.pdays,bins=[-1,5,10,15,20,25,30,1000],labels=pdays_labels)\ndf_bank.head()",
  "generation": "df_bank['pdays'] = np.where(df_bank['pdays'] == 999, '\u8fd1\u671f\u65e0\u8054\u7cfb', '\u6709\u8fc7\u8054\u7cfb')\ndf_bank.head()"
 },
 {
  "nbid": "NB_138947",
  "file": "NB_138947,df-1.ipynb",
  "row_id": 38,
  "target_code": "df=df.replace({\"voice mail plan\"                 :d})\ndf.head()",
  "generation": "d={\"no\":False,\"yes\":True}\ndf[\"international plan\"]=df[\"international plan\"].map(d)\ndf.head()"
 },
 {
  "nbid": "NB_3749152",
  "file": "NB_3749152,dados_bosch,y-1.ipynb",
  "row_id": 4,
  "target_code": "dados_bosch['MaterialClass'               ]=pd.factorize(dados_bosch['MaterialClass'               ])[0]\ndados_bosch.head()",
  "generation": "dados_bosch = pd.read_csv('./dados/ToolDataSheetVsNQuebras.csv', sep=';')\ndados_bosch = dados_bosch[dados_bosch.PropellerAngle <= 360]\ndados_bosch['MaterialClass'] = dados_bosch['MaterialClass'].astype('category').cat.codes\ndados_bosch['Refrigeration'] = dados_bosch['Refrigeration'].astype('category').cat.codes\ndados_bosch"
 },
 {
  "nbid": "NB_3757350",
  "file": "NB_3757350,df3-1.ipynb",
  "row_id": 85,
  "target_code": "df3[['CA'    ,'UK'    , 'US'     ]] = pd.get_dummies(df3['country'         ])\ndf3 = df3.drop('CA'    , axis = 1)\ndf3.head()",
  "generation": "df3[['UK', 'US']] = pd.get_dummies(df3['country'])\ndf3.head()"
 },
 {
  "nbid": "NB_3803946",
  "file": "NB_3803946,df,nor_df-1.ipynb",
  "row_id": 12,
  "target_code": "df=df.dropna()\nprint(df.shape)\ndf.head()",
  "generation": "data_1=pd.merge(bb,ma, how='outer', left_index=True, right_index=True)\ndata_1=pd.merge(data_1,macd, how='outer', left_index=True, right_index=True)\ndata_1=pd.merge(data_1,rsi, how='outer', left_index=True, right_index=True)\ndata_1=pd.merge(data_1,sto, how='outer', left_index=True, right_index=True)\ndata_1=pd.merge(data_1,roc, how='outer', left_index=True, right_index=True)\n\n#include date\ndf=pd.merge(data_1, data['Date'],left_on=None, right_on=None, left_index=True, right_index=True)\ndf = df[['Date','mbb','ubb','lbb','ma_5','ma_10','ma_15','ma_20','ma_30','ma_60','macd','fast','slow','signal',\n                   'osc','rsi','open','high','low','fast_K','fast_D','slow_D','roc']]\n\nprint(df.shape)\ndf.head()"
 },
 {
  "nbid": "NB_3896471",
  "file": "NB_3896471,df2-1.ipynb",
  "row_id": 75,
  "target_code": "df2['intercept'           ] = 1\ndf2[['old_page'          , 'ab_page'          ]] = pd.get_dummies(df2['group'       ])\ndf2.drop('old_page'          , axis=1, inplace=True)\ndf2.head()",
  "generation": "df2['intercept'] = 1\ndf2['ab_page'] = pd.get_dummies(df2['group'])['treatment']\ndf2.head()"
 },
 {
  "nbid": "NB_3896471",
  "file": "NB_3896471,df_new-1.ipynb",
  "row_id": 87,
  "target_code": "df_country = pd.get_dummies(df_new['country'         ])\ndf_new = df_new.join(df_country)\ndf_new.head()",
  "generation": "df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])\ndf_new['intercept'] = 1\nlogit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK']])\nresults = logit_mod.fit()\nresults.summary()"
 },
 {
  "nbid": "NB_3902538",
  "file": "NB_3902538,occupancy,predictions-1.ipynb",
  "row_id": 2,
  "target_code": "occupancy.date = pd.to_datetime(occupancy.date) \noccupancy.index = pd.DatetimeIndex(occupancy.date, freq='H'   ) \noccupancy.head(3)",
  "generation": "occupancy['date'] = pd.to_datetime(occupancy['date'])\noccupancy = occupancy.set_index('date')\noccupancy.head()"
 },
 {
  "nbid": "NB_3914421",
  "file": "NB_3914421,dfp-1.ipynb",
  "row_id": 4,
  "target_code": "dfp['Avg'     ] = dfp.mean(axis=1).round(0)\ndfp.head()",
  "generation": "dfp['Avg'] = dfp.mean(axis=1)\ndfp = dfp.reset_index()\ndfp.head()"
 },
 {
  "nbid": "NB_1098760",
  "file": "NB_1098760,df-1.ipynb",
  "row_id": 3,
  "target_code": "d1 = {'Yes'      : 1, 'No'      : 0}\ndf['International plan'                    ] = df['International plan'                    ].map(d1)\ndf['Voice mail plan'                 ] = df['Voice mail plan'                 ].map(d1)\ndf.head()",
  "generation": "df['International plan'] = df['International plan'].map({'Yes': 1, 'No': 0})\ndf['Voice mail plan'] = df['Voice mail plan'].map({'Yes': 1, 'No': 0})\ndf"
 },
 {
  "nbid": "NB_1106263",
  "file": "NB_1106263,raw_data-1.ipynb",
  "row_id": 22,
  "target_code": "raw_data[\"LastUpdated\"             ] = pd.to_datetime(raw_data['Last Update'             ])\nraw_data.head()",
  "generation": "raw_data[\"LastUpdated\"] = pd.to_datetime(raw_data[\"Last Update\"])\nraw_data = raw_data[[\"Sno\", \"Province/State\", \"Country\", \"Last Update\", \"Confirmed\", \"Deaths\", \"Recovered\", \"LastUpdated\"]]\nraw_data.head()"
 },
 {
  "nbid": "NB_1106263",
  "file": "NB_1106263,raw_data-1.ipynb",
  "row_id": 23,
  "target_code": "raw_data['date'      ] = pd.DatetimeIndex(raw_data['LastUpdated'             ]).date\nraw_data['year'      ] = pd.DatetimeIndex(raw_data['LastUpdated'             ]).year\nraw_data['month'       ] = pd.DatetimeIndex(raw_data['LastUpdated'             ]).month\nraw_data['day'     ] = pd.DatetimeIndex(raw_data['LastUpdated'             ]).day\nraw_data['time'      ] = pd.DatetimeIndex(raw_data['LastUpdated'             ]).time\nraw_data['dayofweek'           ] = pd.DatetimeIndex(raw_data['LastUpdated'             ]).dayofweek\nraw_data['day_name'          ] = pd.DatetimeIndex(raw_data['LastUpdated'             ]).day_name()\nraw_data['month_name'            ] = pd.DatetimeIndex(raw_data['LastUpdated'             ]).month_name()\nraw_data.head()",
  "generation": "raw_data[\"LastUpdated\"] = pd.to_datetime(raw_data['Last Update'])\nraw_data['year'] = raw_data['LastUpdated'].dt.year\nraw_data['month'] = raw_data['LastUpdated'].dt.month\nraw_data['day'] = raw_data['LastUpdated'].dt.day\nraw_data['time'] = raw_data['LastUpdated'].dt.time\nraw_data['dayofweek'] = raw_data['LastUpdated'].dt.dayofweek\nraw_data['day_name'] = raw_data['LastUpdated'].dt.day_name()\nraw_data['month_name'] = raw_data['LastUpdated'].dt.month_name()\nraw_data['date'] = raw_data['LastUpdated'].dt.date\nraw_data.head()"
 },
 {
  "nbid": "NB_1107994",
  "file": "NB_1107994,y_train_pred_final-1.ipynb",
  "row_id": 98,
  "target_code": "y_train_pred_final['predicted'           ] = y_train_pred_final.Converted_prob.map(lambda x: 1 if x > 0.8 else 0)\ny_train_pred_final.head()",
  "generation": "y_train_pred_final['predicted'] = y_train_pred_final['Converted_prob'].apply(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()"
 },
 {
  "nbid": "NB_1111834",
  "file": "NB_1111834,df-1.ipynb",
  "row_id": 8,
  "target_code": "df = df[df['SUMLEV'        ] == 50]\ndf",
  "generation": "df['Cost'] = df['Cost'] * 0.8\ndf"
 },
 {
  "nbid": "NB_1115578",
  "file": "NB_1115578,df4-1.ipynb",
  "row_id": 63,
  "target_code": "df4['\uad11\uc5ed\uc2dc'     ] = ['\uc11c\uc6b8'    ,'\ubd80\uc0b0'    ,'\ub300\uad6c'    ,'\uc778\ucc9c'    ,'\uad11\uc8fc'    ,'\ub300\uc804'    ,'\uc6b8\uc0b0'    ]\ndf4 = df4.set_index('\uad11\uc5ed\uc2dc'     )\ndf4",
  "generation": "df3['\uad11\uc5ed\uc2dc'] = df3.index.str.split(' ').str[0]\ndf4 = df3.iloc[:, [0,1,2,6]]\ndf4.columns = ['\uad11\uc5ed\uc2dc', '\ucd1d \uc778\uad6c\uc218', '\uc138\ub300\uc218', 'Unnamed']\ndf4 = df4.iloc[:, :-1]\ndf4.set_index(['\uad11\uc5ed\uc2dc'], inplace=True)\ndf4.index.name = 'Unnamed: 0_level_0'\ndf4.columns = pd.MultiIndex.from_product([[''], df4.columns])\ndf4"
 },
 {
  "nbid": "NB_1117698",
  "file": "NB_1117698,data-1.ipynb",
  "row_id": 14,
  "target_code": "data['year'      ] = np.repeat(2017,data.shape[0])\ndata.head()",
  "generation": "data = data.rename(columns={'Id': 'id', 'Street': 'street', 'Weekday': 'weekday', 'Month': 'month', 'Day': 'day', 'Hour': 'hour', 'Mild injuries': 'mild_injuries', 'Serious injuries': 'serious_injuries', 'Victims': 'victims', 'Vehicles involved': 'vehicles_involved', 'Longitude': 'longitude', 'Latitude': 'latitude'})\ndata['year'] = 2017\ndata = data[['id', 'street', 'weekday', 'month', 'day', 'hour', 'mild_injuries', 'serious_injuries', 'victims', 'vehicles_involved', 'longitude', 'latitude', 'year']]\ndata.head()"
 },
 {
  "nbid": "NB_1117698",
  "file": "NB_1117698,data-1.ipynb",
  "row_id": 19,
  "target_code": "data['date'      ] = pd.to_datetime(data[['year'      ,'month'       ,'day'     ,'hour'      ]])\ndata.head()",
  "generation": "data['date'] = pd.to_datetime(data['year'].astype(str) + '-' + data['month'].astype(str) + '-' + data['day'].astype(str) + ' ' + data['hour'].astype(str) + ':00:00')\ndata.head()"
 },
 {
  "nbid": "NB_1121220",
  "file": "NB_1121220,data-1.ipynb",
  "row_id": 46,
  "target_code": "data[\"msg_no_stopword\"                 ] = data['msg_clean_tokenized'                     ].apply(lambda x:remove_stopwords(x))\ndata.head()",
  "generation": "stop_words = stopwords.words('english')\ndata['msg_no_stopword'] = data['msg_clean_tokenized'].apply(lambda x: [word for word in x if word not in stop_words])\ndata.head()"
 },
 {
  "nbid": "NB_1121220",
  "file": "NB_1121220,data-1.ipynb",
  "row_id": 56,
  "target_code": "data['msg_stemmed'             ] = data['msg_no_stopword'                 ].apply(lambda x: stemming(x))\ndata.head()",
  "generation": "data['msg_stemmed'] = data['msg_no_stopword'].apply(lambda x:stemming(x)) \ndata.head()"
 },
 {
  "nbid": "NB_1121220",
  "file": "NB_1121220,data-1.ipynb",
  "row_id": 69,
  "target_code": "data['msg_nostop'            ] = data['msg'     ].apply(lambda x: clean_txt(x))\ndata.head()",
  "generation": "data['msg_nostop'] = data['msg_clean_tokenized'].apply(lambda x: remove_stopwords(x))\ndata[['label', 'msg', 'msg_nostop']]"
 },
 {
  "nbid": "NB_1121220",
  "file": "NB_1121220,data-1.ipynb",
  "row_id": 71,
  "target_code": "data['msg_lemmatized'                ] = data['msg_nostop'            ].apply(lambda x : lemmatization(x))\ndata.head()",
  "generation": "data['msg_lemmatized'] = data['msg_nostop'].apply(lambda x: lemmatize(x))\ndata.head()"
 },
 {
  "nbid": "NB_1123446",
  "file": "NB_1123446,rdu_temps-1.ipynb",
  "row_id": 25,
  "target_code": "rdu_temps['wind_speed'            ] = rdu_temps.pop('AWND'      ) / 10\nrdu_temps['precipitation'               ] = rdu_temps.pop('PRCP'      ) / 10\nrdu_temps.head()",
  "generation": "rdu_temps = rdu_temps.replace((- 9999), np.nan)\nrdu_temps = rdu_temps[['SNOW', 'temp_max', 'temp_min', 'AWND', 'PRCP']].copy()\nrdu_temps.columns = ['SNOW', 'temp_max', 'temp_min', 'wind_speed', 'precipitation']\nrdu_temps.head()"
 },
 {
  "nbid": "NB_1123446",
  "file": "NB_1123446,rdu_temps-1.ipynb",
  "row_id": 27,
  "target_code": "rdu_temps = rdu_temps.rename(columns={'SNOW'      : 'snowfall'           })\nrdu_temps.head()",
  "generation": "def c10_to_f(celsius):\n    return (celsius * 1.8) + 32\n\nrdu_weather = rdu_weather.replace((- 9999), np.nan)\nrdu_temps = rdu_weather[['TMAX', 'TMIN', 'AWND', 'PRCP', 'SNOW']].copy()\nrdu_temps.columns = ['temp_max', 'temp_min', 'wind_speed', 'precipitation', 'snowfall']\nrdu_temps['temp_max'] = c10_to_f(rdu_temps.pop('temp_max'))\nrdu_temps['temp_min'] = c10_to_f(rdu_temps.pop('temp_min'))\nrdu_temps['wind_speed'] = rdu_temps.pop('wind_speed') / 10\nrdu_temps['precipitation'] = rdu_temps.pop('precipitation') / 10\nrdu_temps.head()"
 },
 {
  "nbid": "NB_1125154",
  "file": "NB_1125154,carseats,y,score-1.ipynb",
  "row_id": 7,
  "target_code": "carseats['High'      ] = (carseats.Sales > 8).astype('int'     )\ncarseats['ShelveLoc'           ] = carseats.ShelveLoc.map({'Bad'     :0, 'Good'       :1, 'Medium'         :2}) \ncarseats['Urban'       ] = (carseats.Urban == 'Yes'      ).astype('int'     ) \ncarseats['US'    ] = (carseats.US == 'Yes'      ).astype('int'     ) \ncarseats.head()",
  "generation": "carseats[\"High\"] = carseats[\"Sales\"] > 8.0\ncarseats[\"ShelveLoc\"] = pd.factorize(carseats[\"ShelveLoc\"])[0]\ncarseats[\"Urban\"] = carseats[\"Urban\"] == \"Yes\"\ncarseats[\"US\"] = carseats[\"US\"] == \"Yes\"\ncarseats.head()"
 },
 {
  "nbid": "NB_144642",
  "file": "NB_144642,longTable-1.ipynb",
  "row_id": 10,
  "target_code": "longTable[\"Old\"     ] = longTable[\"Age\"     ] > 60\nlongTable.head()",
  "generation": "longTable['Old'] = longTable['Age'] >= 60\nlongTable.head()"
 },
 {
  "nbid": "NB_144642",
  "file": "NB_144642,wideTable-1.ipynb",
  "row_id": 27,
  "target_code": "wideTable = wideTable.reset_index()\nwideTable",
  "generation": "wideTable.reset_index(inplace=True)\nwideTable = wideTable.drop(columns=['Unnamed: 0_level_0', 'Unnamed: 1_level_0', 'Unnamed: 2_level_0', 'Unnamed: 3_level_0', 'Unnamed: 5_level_1', 'Unnamed: 6_level_1', 'Unnamed: 7_level_1', 'Unnamed: 8_level_1', 'Unnamed: 9_level_1', 'Unnamed: 10_level_1', 'Unnamed: 11_level_1', 'Unnamed: 12_level_1', 'Unnamed: 13_level_1', 'Unnamed: 14_level_1', 'Unnamed: 15_level_1'])\nwideTable"
 },
 {
  "nbid": "NB_3562015",
  "file": "NB_3562015,dataframe-1.ipynb",
  "row_id": 43,
  "target_code": "dataframe = dataframe.replace({'Voice mail plan'                 : dic})\ndataframe.head()",
  "generation": "dic = {'No' : False, 'Yes' : True}\ndataframe['International plan'] = dataframe['International plan'].map(dic)\ndataframe['Voice mail plan'] = dataframe['Voice mail plan'].map(dic)\ndataframe.head()"
 },
 {
  "nbid": "NB_3563175",
  "file": "NB_3563175,tornado_data-1.ipynb",
  "row_id": 4,
  "target_code": "cols = ['EF#'     , 'State'        , 'Path length'              , 'Max width'            ]\ntornado_data = tornado_data[cols]\ntornado_data.head()",
  "generation": "tornado_data = tornado_data.groupby(['State', 'EF#'])[['Path length', 'Max width']].max().reset_index()\ntornado_data.head()"
 },
 {
  "nbid": "NB_3564217",
  "file": "NB_3564217,PRISM_RhondasHill-1.ipynb",
  "row_id": 12,
  "target_code": "PRISM_RhondasHill['Date'      ] =  pd.to_datetime(PRISM_RhondasHill['Date'      ])\nPRISM_RhondasHill.head()",
  "generation": "PRISM_RhondasHill['Date'] = pd.to_datetime(PRISM_RhondasHill['Date'])\nPRISM_RhondasHill.head()"
 },
 {
  "nbid": "NB_3564854",
  "file": "NB_3564854,mushrooms,y-1.ipynb",
  "row_id": 5,
  "target_code": "from sklearn.preprocessing import LabelEncoder\nlabelencoder=LabelEncoder()\nfor col in mushrooms.columns:\n    mushrooms[col] = labelencoder.fit_transform(mushrooms[col])\nmushrooms.head()",
  "generation": "# Encode categorical variables\nlabelencoder=LabelEncoder()\nfor column in mushrooms.columns:\n    mushrooms[column] = labelencoder.fit_transform(mushrooms[column])\n\n# Scale the data\nscaler = StandardScaler()\nmushrooms = pd.DataFrame(scaler.fit_transform(mushrooms), columns=mushrooms.columns) \n\nmushrooms"
 },
 {
  "nbid": "NB_3566338",
  "file": "NB_3566338,df-1.ipynb",
  "row_id": 28,
  "target_code": "df['AgeBand'         ] = pd.cut(df['Age'     ],[0,5,15,30,50,80],labels = ['1'   ,'2'   ,'3'   ,'4'   ,'5'   ])\ndf.head()",
  "generation": "df['AgeBand'] = pd.cut(df['Age'], bins=[0,5,15,30,50,80], labels=['1','2','3','4','5'])\ndf.head()"
 },
 {
  "nbid": "NB_3566338",
  "file": "NB_3566338,df-1.ipynb",
  "row_id": 30,
  "target_code": "df['AgeBand'         ] = pd.qcut(df['Age'     ],[0,0.1,0.3,0.5,0.7,0.9],labels = ['1'   ,'2'   ,'3'   ,'4'   ,'5'   ])\ndf.head()",
  "generation": "df['AgeBand'] = pd.qcut(df['Age'], [0, 0.1, 0.3, 0.5, 0.7, 0.9], labels=['1', '2', '3', '4', '5'])\ndf.head()"
 },
 {
  "nbid": "NB_3566338",
  "file": "NB_3566338,df-1.ipynb",
  "row_id": 40,
  "target_code": "df['Sex_num'         ] = df['Sex'     ].replace(['male'      ,'female'        ],[1,2])\ndf.head()",
  "generation": "le = LabelEncoder()\ndf['Sex_num'] = le.fit_transform(df['Sex'])\ndf.head()"
 },
 {
  "nbid": "NB_3568723",
  "file": "NB_3568723,X-1.ipynb",
  "row_id": 11,
  "target_code": "X['Fare'      ] = X['Fare'      ]/X['Fare'      ].max()\nX.head()",
  "generation": "logreg = LogisticRegression()\ndf = pd.concat([df, dummy1, dummy2], axis=1)\nX = df[['Pclass', 'Fare', 'Sex_male', 'Embarked_Q', 'Embarked_S']]\ny = df['Survived']\nX['Fare'] = (X['Fare'] - X['Fare'].min()) / (X['Fare'].max() - X['Fare'].min())\nX.head()"
 },
 {
  "nbid": "NB_3579277",
  "file": "NB_3579277,data,X-1.ipynb",
  "row_id": 4,
  "target_code": "for columnName in data:\n    data[columnName] = data[columnName].apply(lambda x : 0 if (str(x).count('.'   ) == 2) else x)\n    data[columnName] = data[columnName].apply(lambda x : 0 if ('Gram'       in str(x)) else x)\n    data[columnName] = data[columnName].apply(lambda x : 0 if ('Mikrogram'            in str(x)) else x) \ndata    ",
  "generation": "data[['sfat', 'cholesterol', 'folat', 'pantotenik', 'cinko', 'selenium', 'manganese']] = data[['sfat', 'cholesterol', 'folat', 'pantotenik', 'cinko', 'selenium', 'manganese']].apply(lambda x: x.str.replace('.', '0').astype(float)) \ndata"
 },
 {
  "nbid": "NB_3931733",
  "file": "NB_3931733,sales-1.ipynb",
  "row_id": 2,
  "target_code": "sales['sale price'            ].value_counts(dropna=False)\nsales['sale price'            ] = sales['sale price'            ].replace(to_replace=[' -   '       ], value=[None])\nsales['sale price'            ].value_counts(dropna=False)\nsales=sales.dropna()\nprint(sales.shape)\nsales.head()",
  "generation": "sales = sales[sales['land square feet'].notna()]"
 },
 {
  "nbid": "NB_3931733",
  "file": "NB_3931733,sales-1.ipynb",
  "row_id": 3,
  "target_code": "sales[['zip code'          ,'borough'         ]] = sales[['zip code'          ,'borough'         ]].astype(str)\nsales['sale price'            ] = sales['sale price'            ].apply(lambda x: x.replace(','   ,''  ))\nsales.head()\nsales[['sale price'            ]] = sales[['sale price'            ]].astype(float)\nsales.head()",
  "generation": "sales['sale price'] = pd.to_numeric(sales['sale price'], errors='coerce')\nsales['land square feet'] = sales['land square feet'].replace(to_replace=[' -   ', '           . '], value=[None, None])\nsales['land square feet'] = pd.to_numeric(sales['land square feet'], errors='coerce')\nsales = sales.dropna()\nsales[['sale price','land square feet']] = sales[['sale price','land square feet']].astype(float)"
 },
 {
  "nbid": "NB_3941769",
  "file": "NB_3941769,df_test-1.ipynb",
  "row_id": 6,
  "target_code": "df_test['height'        ] = df_test.height\ndf_test['height3'         ] = df_test.height**3\ndf_test['bias'      ] = 1\ndf_test.head(3)",
  "generation": ""
 },
 {
  "nbid": "NB_3957668",
  "file": "NB_3957668,df3-1.ipynb",
  "row_id": 74,
  "target_code": "df3[['group_1'         , 'group_2'           ]] = pd.get_dummies(df2.group)\ndf3[['lp1'     , 'lp2'       ]] = pd.get_dummies(df2.landing_page)\ndf3 = df3.rename(columns={'group_2'         : 'ab_page'          , 'lp1'      : 'treatment'            })\ndf3 = df3.drop(['timestamp'           ,'group'       , 'landing_page'               , 'lp2'      , 'group_1'          , 'treatment'            ], axis=1)\ndf3.head()",
  "generation": "df2['intercept'] = 1\ndf2['ab_page'] = pd.get_dummies(df2['landing_page'])['new_page']\ndf2.drop('landing_page', axis=1, inplace=True)\ndf2.head()"
 },
 {
  "nbid": "NB_3958310",
  "file": "NB_3958310,df_new-1.ipynb",
  "row_id": 83,
  "target_code": "df_new[['country_CA'            ,'country_UK'            ,'country_US'            ]] = pd.get_dummies(df_new['country'         ])\ndf_new.head()",
  "generation": "df_new[['country_CA', 'country_UK', 'country_US']] = pd.get_dummies(df_new['country'])\ndf_new"
 },
 {
  "nbid": "NB_3962726",
  "file": "NB_3962726,dataset-1.ipynb",
  "row_id": 1,
  "target_code": "dataset[\"Attrition\"           ]=dataset[\"Attrition\"           ].map({\"Yes\"     :1,\"No\"    :0})\ndataset.head()",
  "generation": "dataset['Attrition'] = dataset['Attrition'].apply(lambda x: 1 if x == 'Yes' else 0)\ndataset"
 },
 {
  "nbid": "NB_3969929",
  "file": "NB_3969929,df-1.ipynb",
  "row_id": 8,
  "target_code": "df = df.drop('Day'     , axis = 1)\ndf = df.drop('Year'      , axis = 1)\ndf.head()",
  "generation": "df = pd.read_csv('/Users/chidera/Desktop/All Project Files/storesalesdata.csv')\ndf = df.groupby(['Store']).sum().drop(['Day', 'Month', 'Year'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_4032138",
  "file": "NB_4032138,new_countries-1.ipynb",
  "row_id": 48,
  "target_code": "new_countries[['CA'    , 'US'     , 'UK'     ]] = pd.get_dummies(new_countries['country'         ])\nnew_countries.head()",
  "generation": "new_countries[['CA', 'US', 'UK']] = pd.get_dummies(new_countries['country'])\nnew_countries.head()"
 },
 {
  "nbid": "NB_4041192",
  "file": "NB_4041192,grouped_df-1.ipynb",
  "row_id": 27,
  "target_code": "grouped_df = grouped_df[::-1]\ngrouped_df",
  "generation": "grouped_df = grouped_df.iloc[::-1]\ngrouped_df"
 },
 {
  "nbid": "NB_3584730",
  "file": "NB_3584730,Apple-1.ipynb",
  "row_id": 10,
  "target_code": "Apple = Apple[['prime_genre'             , 'user_rating'              , 'rating_count_tot'                   , 'price'        ]]\nApple.head(3)",
  "generation": "Apple = Apple[['prime_genre', 'user_rating', 'rating_count_tot', 'price']]\nApple.head(3)"
 },
 {
  "nbid": "NB_3587438",
  "file": "NB_3587438,df_new-1.ipynb",
  "row_id": 97,
  "target_code": "df_new = df_new.drop('CA'    ,axis=1)\ndf_new.head()",
  "generation": "country_dummies = pd.get_dummies(df2['country'])\ndf2 = df2.join(country_dummies)\ndf2 = df2.drop('country', axis=1)\ndf2.head()"
 },
 {
  "nbid": "NB_3588096",
  "file": "NB_3588096,train-1.ipynb",
  "row_id": 15,
  "target_code": "train = pd.concat([train, embarked_dummy],axis=1)\ntrain.drop('Embarked_new'              ,axis=1,inplace = True)\ntrain.head()",
  "generation": "train['Age_new'] = train['Age'].fillna(train['Age'].mean())\ndf = pd.concat([train, sex_dummy, embarked_dummy],axis=1)\ndf.drop(['Sex', 'Embarked', 'Cabin'],axis=1,inplace = True)\ndf.head()"
 },
 {
  "nbid": "NB_3588543",
  "file": "NB_3588543,raw_data-1.ipynb",
  "row_id": 9,
  "target_code": "from datetime import datetime, date\nraw_data.quit_date = raw_data.quit_date.fillna(date(year = 2015, month = 12, day = 13))\nraw_data.head()",
  "generation": "raw_data['quit_date'].fillna(pd.to_datetime('2015-12-13'), inplace=True)\nraw_data.head()"
 },
 {
  "nbid": "NB_3588543",
  "file": "NB_3588543,raw_data-1.ipynb",
  "row_id": 13,
  "target_code": "raw_data['salary/seniority'                  ] = raw_data['salary'        ] / raw_data['seniority'           ]\nraw_data.head()",
  "generation": "raw_data['salary/seniority'] = raw_data['salary'] / raw_data['seniority']\nraw_data.head()"
 },
 {
  "nbid": "NB_3593456",
  "file": "NB_3593456,dd-1.ipynb",
  "row_id": 2,
  "target_code": "dd = dd.drop('id'    ,axis=1)\ndd.head()",
  "generation": "dd = dd.drop(columns=['id'])\ndd.head()"
 },
 {
  "nbid": "NB_3599514",
  "file": "NB_3599514,data-1.ipynb",
  "row_id": 24,
  "target_code": "data['volume'        ] = data['x'   ] * data['y'   ] * data['x'   ]\ndata.head()",
  "generation": "data['volume'] = data['x'] * data['y'] * data['z']\ndata"
 },
 {
  "nbid": "NB_3606998",
  "file": "NB_3606998,leaf_data-1.ipynb",
  "row_id": 17,
  "target_code": "leaf_data['eccentricity'              ] = (leaf_data['eccentricity'              ] - leaf_data['eccentricity'              ].mean()) / leaf_data['eccentricity'              ].std()\nleaf_data.head()",
  "generation": "leaf_data['eccentricity'] = leaf_data['eccentricity'].fillna(leaf_data['eccentricity'].mean())\nleaf_data['eccentricity'] = leaf_data['eccentricity'] - leaf_data['eccentricity'].mean()\nleaf_data"
 },
 {
  "nbid": "NB_3610726",
  "file": "NB_3610726,df_new-1.ipynb",
  "row_id": 99,
  "target_code": "df_new[['CA'    , 'US'     , 'UK'     ]] = pd.get_dummies(df_new['country'         ])[['CA'    , 'US'     , 'UK'     ]]\ndf_new.head()",
  "generation": "df_new[['CA', 'US', 'UK']] = pd.get_dummies(df_new['country'])\ndf_new.head()"
 },
 {
  "nbid": "NB_3619061",
  "file": "NB_3619061,sms_df-1.ipynb",
  "row_id": 11,
  "target_code": "sms_df[\"label\"       ] = sms_df[\"label\"       ].apply(lambda x: 1 if x==\"spam\"       else 0)\nsms_df.head()",
  "generation": "sms_df[\"label\"] = sms_df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)\nsms_df.head()"
 },
 {
  "nbid": "NB_3619061",
  "file": "NB_3619061,sms_df-1.ipynb",
  "row_id": 14,
  "target_code": "sms_df[\"len_char\"          ] = sms_df[\"message\"         ].apply(len)\nsms_df.head()",
  "generation": "sms_df['len_char'] = sms_df['message'].apply(len)\nsms_df.head()"
 },
 {
  "nbid": "NB_3632504",
  "file": "NB_3632504,autos-1.ipynb",
  "row_id": 8,
  "target_code": "autos['price'       ] = autos['price'       ].str.replace('$'   ,''  ).str.replace(','   ,''  ).astype(int)\nautos['odometer_km'             ] = autos['odometer_km'             ].str.replace('km'    ,''  ).str.replace(','   ,''  ).astype(int)\nautos.head()",
  "generation": "autos['price'] = autos['price'].str.replace('$','').str.replace(',','').astype(float)\nautos['odometer'] = autos['odometer'].str.replace('km','').str.replace(',','').astype(float)\nautos = autos.rename(columns={'dateCrawled': 'date_crawled', 'yearOfRegistration': 'registration_year', 'powerPS': 'power_ps', 'monthOfRegistration': 'registration_month', 'fuelType': 'fuel_type', 'notRepairedDamage': 'unrepaired_damage', 'dateCreated': 'ad_created', 'nrOfPictures': 'nr_of_pictures', 'postalCode': 'postal_code', 'lastSeen': 'last_seen'})\nautos.head()"
 },
 {
  "nbid": "NB_3633112",
  "file": "NB_3633112,diamonds,x-1.ipynb",
  "row_id": 4,
  "target_code": "from sklearn.preprocessing import LabelEncoder\ncategorical_features = ['cut'     , 'color'        , 'clarity'          ]\nle = LabelEncoder()\nfor i in categorical_features:\n    diamonds[i] = le.fit_transform(diamonds[i])\ndiamonds.head()",
  "generation": "le = LabelEncoder()\ndiamonds['cut'] = le.fit_transform(diamonds['cut'])\ndiamonds['color'] = le.fit_transform(diamonds['color'])\ndiamonds['clarity'] = le.fit_transform(diamonds['clarity'])\nscaler = StandardScaler()\ndiamonds[['carat', 'depth', 'table', 'x', 'y', 'z']] = scaler.fit_transform(diamonds[['carat', 'depth', 'table', 'x', 'y', 'z']])\ndiamonds.head()"
 },
 {
  "nbid": "NB_3633382",
  "file": "NB_3633382,df_new-1.ipynb",
  "row_id": 91,
  "target_code": "df_new[['CA'    ,'US'    ,'UK'    ]] = pd.get_dummies(df_new['country'         ])[['CA'    ,'US'    ,'UK'    ]]\ndf_new.head()",
  "generation": "df_new = pd.get_dummies(df_new, columns=['country'])"
 },
 {
  "nbid": "NB_3666096",
  "file": "NB_3666096,new-1.ipynb",
  "row_id": 94,
  "target_code": "new['US_ab_page'            ] = new['US'    ]*new['ab_page'         ]\nnew.head()",
  "generation": "df['US_ab_page'] = df['US'] * df['ab_page']\ndf[['CA','UK','US']] = pd.get_dummies(df['country'])\ndf.head()"
 },
 {
  "nbid": "NB_3666096",
  "file": "NB_3666096,new-1.ipynb",
  "row_id": 95,
  "target_code": "new['UK_ab_page'            ] = new['UK'    ]*new['ab_page'         ]\nnew.head()",
  "generation": "df[['CA', 'UK', 'US']] = pd.get_dummies(df['country'])\ndf['US_ab_page'] = df['US'] * df['ab_page']\ndf['UK_ab_page'] = df['UK'] * df['ab_page']\ndf.head()"
 },
 {
  "nbid": "NB_3671986",
  "file": "NB_3671986,df_final-1.ipynb",
  "row_id": 86,
  "target_code": "df_final[['US'    , 'UK'     , 'CA'     ]] = pd.get_dummies(df_final['country'         ])\ndf_final.head()",
  "generation": "one_hot_encoded = pd.get_dummies(df_final['country'])\ndf_final = df_final.join(one_hot_encoded)\ndf_final.head(5)"
 },
 {
  "nbid": "NB_3675553",
  "file": "NB_3675553,np_data-1.ipynb",
  "row_id": 17,
  "target_code": "np_data[['Badlands'          , \n         'GrandCanyon'                      ,\n         'BryceCanyon'                      ]] = np_data[['Badlands'          , \n                                    'GrandCanyon'                                                 ,\n                                    'BryceCanyon'                                                 ]].diff()\nnp_data.head()",
  "generation": "np_data[['Badlands', 'GrandCanyon', 'BryceCanyon']] = np_data[['Badlands', 'GrandCanyon', 'BryceCanyon']].diff()\nnp_data.head()"
 },
 {
  "nbid": "NB_3675794",
  "file": "NB_3675794,turkcell_data-1.ipynb",
  "row_id": 10,
  "target_code": "turkcell_data['TextSize'          ] = [len(t) for t in turkcell_data.Tweets]\nturkcell_data.head()",
  "generation": "turkcell_data['TextSize'] = turkcell_data['Tweets'].apply(len)\nturkcell_data.head()"
 },
 {
  "nbid": "NB_3689746",
  "file": "NB_3689746,df3-1.ipynb",
  "row_id": 88,
  "target_code": "df3[['CA'    , 'UK'     , 'US'     ]] = pd.get_dummies(df3['country'         ])\ndf3.head()",
  "generation": "df3[['CA', 'UK', 'US']] = pd.get_dummies(df3['country'])\ndf3.head()"
 },
 {
  "nbid": "NB_3695976",
  "file": "NB_3695976,sms,spam_words_df-1.ipynb",
  "row_id": 12,
  "target_code": "sms['text_length'             ] = sms['text'      ].apply(lambda col: len(col))\nsms.head()",
  "generation": "sms['text_length'] = sms['text'].apply(len)\nsms.head()"
 },
 {
  "nbid": "NB_3698221",
  "file": "NB_3698221,fb-1.ipynb",
  "row_id": 5,
  "target_code": "fb['close_ontem'             ] = fb['Close'       ].shift(1)\nfb.head()",
  "generation": "fb['close_ontem'] = fb['Close'].shift(1)\nfb.head()"
 },
 {
  "nbid": "NB_3698221",
  "file": "NB_3698221,fb-1.ipynb",
  "row_id": 7,
  "target_code": "fb[\"precodiff\"           ] = fb['Close'       ] - fb['close_ontem'             ]\nfb.head()",
  "generation": "fb['precodiff'] = fb['Close'] - fb['close_ontem']\nfb.head()"
 },
 {
  "nbid": "NB_3698221",
  "file": "NB_3698221,fb-1.ipynb",
  "row_id": 9,
  "target_code": "fb['retorno_dia'             ] = fb['precodiff'           ] / fb['Close'       ]\nfb.head()",
  "generation": "fb['retorno_dia'] = fb['precodiff'] / fb['Close']\nfb.head()"
 },
 {
  "nbid": "NB_3698221",
  "file": "NB_3698221,fb-1.ipynb",
  "row_id": 11,
  "target_code": "fb['direcao'         ] = [1 if fb.loc[x, 'precodiff'            ] > 0 else -1 for x in fb.index]\nfb.head()",
  "generation": "fb['close_ontem'] = fb['Close'].shift(1)\nfb[\"precodiff\"] = fb['Close'] - fb['close_ontem']\nfb['retorno_dia'] = fb['precodiff'] / fb['Close']\nfb['direcao'] = fb['retorno_dia'].apply(lambda x: 1 if x > 0 else -1)"
 },
 {
  "nbid": "NB_3698221",
  "file": "NB_3698221,fb-1.ipynb",
  "row_id": 13,
  "target_code": "fb['media3'        ] = (fb['Close'       ] + fb['Close'       ].shift(1) + fb['Close'       ].shift(2)) / 3\nfb.head()",
  "generation": "fb['close_ontem'] = fb['Close'].shift(1)\nfb[\"precodiff\"] = fb['Close'] - fb['close_ontem']\nfb['retorno_dia'] = fb['precodiff'] / fb['Close']\nfb['direcao'] = [1 if fb.loc[x, 'precodiff'] > 0 else -1 for x in fb.index]\nfb['media3'] = fb['Close'].rolling(window=3).mean()\nfb.head()"
 },
 {
  "nbid": "NB_3698221",
  "file": "NB_3698221,fb-1.ipynb",
  "row_id": 17,
  "target_code": "fb['media45'         ] = fb['Close'       ].rolling(45).mean()\nfb['media180'          ] = fb['Close'       ].rolling(200).mean()\nfb.head()",
  "generation": "fb['close_ontem'] = fb['Close'].shift(1)\nfb[\"precodiff\"] = fb['Close'] - fb['close_ontem']\nfb['retorno_dia'] = fb['precodiff'] / fb['Close']\nfb['direcao'] = [1 if fb.loc[x, 'precodiff'] > 0 else -1 for x in fb.index]\nfb['media3'] = fb['Close'].rolling(3).mean()\nfb['media15'] = fb['Close'].rolling(15).mean()\nfb['media45'] = fb['Close'].rolling(45).mean()\nfb['media180'] = fb['Close'].rolling(180).mean()\nfb.head()"
 },
 {
  "nbid": "NB_3699464",
  "file": "NB_3699464,test_data-1.ipynb",
  "row_id": 8,
  "target_code": "test_data.head()\ntest_data=test_data.drop(\"Title\"       ,axis=1)\ntest_data.head()",
  "generation": "test_data = test_data.drop(columns=['Title'])\ntest_data_clustered = loaded_model.predict(test_data)\ntest_data_clustered = pd.DataFrame(test_data_clustered, columns=['Cluster'])\ntest_data_clustered = pd.concat([test_data.iloc[:,0], test_data_clustered], axis=1)\ntest_data_clustered = pd.get_dummies(test_data_clustered, columns=['Cluster'])\ntest_data_clustered = test_data_clustered.drop(columns=['Unnamed: 0'])\ntest_data_clustered.head()"
 },
 {
  "nbid": "NB_943813",
  "file": "NB_943813,df_predictions-1.ipynb",
  "row_id": 175,
  "target_code": "df_predictions = df_predictions.query('isdog_1 == True or isdog_2 == True or isdog_3 == True'                                                       )\ndf_predictions",
  "generation": "df_predictions = df_predictions[df_predictions['isdog_1'] == True]\ndf_predictions = df_predictions.reset_index(drop=True)\ndf_predictions.to_csv('output.csv')"
 },
 {
  "nbid": "NB_952638",
  "file": "NB_952638,co2df-1.ipynb",
  "row_id": 10,
  "target_code": "co2df['date'      ]=pd.to_datetime(dict(year=co2df['year'      ], month=co2df['month'       ], day=co2df['day'     ]))\nch4df['date'      ]=pd.to_datetime(dict(year=ch4df['year'      ], month=ch4df['month'       ], day=ch4df['day'     ]))\nco2df.head()",
  "generation": "co2df['date'] = pd.to_datetime(dict(year=co2df['year'], month=co2df['month'], day=co2df['day']))\nco2df = co2df[['year', 'month', 'day', 'value', 'value_std_dev', 'nvalue', 'date']]\nco2df.head()"
 },
 {
  "nbid": "NB_953071",
  "file": "NB_953071,obesity_datavalue_df-1.ipynb",
  "row_id": 11,
  "target_code": "obesity_datavalue_df= pd.merge(obesity_datavalue_df, HBP_df, on=\"CityFIPS\"          )\nobesity_datavalue_df= obesity_datavalue_df.rename(columns={\"City_x\"        :\"City\"      ,\"State_x\"         :\"State\"       , \"Population_x\"               :\"Population\"            })\nobesity_datavalue_df= obesity_datavalue_df[[\"City\"      , \"State\"        ,\"CityFIPS\"          ,\"Population\"            ,\"Obesity %\"           ,\"CHD %\"       , \"Diabetes %\"             , \"HBP %\"         ]]\nobesity_datavalue_df.head()",
  "generation": "obesity_datavalue_df= pd.merge(obesity_datavalue_df, HBP_df, on=\"CityFIPS\")\nobesity_datavalue_df= obesity_datavalue_df.rename(columns={\"State_x\":\"State\", \"Population_x\":\"Population\"})\nobesity_datavalue_df= obesity_datavalue_df[[\"City\", \"State\",\"CityFIPS\",\"Population\",\"Obesity %\",\"CHD %\", \"Diabetes %\", \"HBP %\"]]\nobesity_datavalue_df.head()"
 },
 {
  "nbid": "NB_956979",
  "file": "NB_956979,articles,y-1.ipynb",
  "row_id": 5,
  "target_code": "articles['claps'       ] = claps_number \nfiltered_columns = ['author'        ,'claps'       ,'title'       ,'text'      ] \narticles = articles[filtered_columns]\narticles.head()",
  "generation": "articles['claps'] = articles['claps'].apply(format_claps)\narticles = articles[['Unnamed: 0', 'author', 'claps', 'title', 'text']]\narticles.head(3)"
 },
 {
  "nbid": "NB_961824",
  "file": "NB_961824,income_county_agg-1.ipynb",
  "row_id": 13,
  "target_code": "income_county_agg['avg_income'            ] = round(income_county_agg.total_inc_amt * 1000 / income_county_agg.returns_with_total_inc, 0)\nincome_county_agg.head(3)",
  "generation": "income_county_agg['avg_income'] = income_county_agg['agi'] / income_county_agg['return_count']\nincome_county_agg.head(2)"
 },
 {
  "nbid": "NB_965657",
  "file": "NB_965657,data,data_new-1.ipynb",
  "row_id": 6,
  "target_code": "data = data.drop(columns=['Element'         ,'Unit'      ,'Value Footnotes'                 ])\ndata.head(5)",
  "generation": "data = data.drop(['Value Footnotes', 'Element', 'Unit'], axis=1)\ndata = data.rename(columns={'Country or Area': 'Country'})\ndata = data[['Country', 'Year', 'Value']]\ndata.head()"
 },
 {
  "nbid": "NB_966822",
  "file": "NB_966822,sp-1.ipynb",
  "row_id": 6,
  "target_code": "sp[\"trend\"       ] = ((sp['Open'      ]-sp[\"Close\"       ])/sp[\"Open\"      ])*100\nsp.head()",
  "generation": "sp['trend'] = np.log(sp['Close']/sp['Close'].shift(1))"
 },
 {
  "nbid": "NB_966822",
  "file": "NB_966822,sp-1.ipynb",
  "row_id": 7,
  "target_code": "sp[\"Date\"      ] = pd.to_datetime(np.array(sp[\"Date\"      ]))\nsp = sp.loc[sp[\"Date\"      ] >= \"2015\"       ]\nsp.head()",
  "generation": "sp = pd.read_csv('s&p.csv',header='infer')\nsp[\"trend\"] = ((sp['Open']-sp[\"Close\"])/sp[\"Open\"])*100\nsp.tail()"
 },
 {
  "nbid": "NB_976101",
  "file": "NB_976101,data-1.ipynb",
  "row_id": 6,
  "target_code": "data=data.rename(columns={'1. Name'         :'Name'      ,'2. Email ID'             :'Email_ID'          ,'3. Highest Education'                      :'Highest_Education'                   ,'4. Year of Passing'                    :'Year_Of_Passing'                 ,'5. Native Place'                 :'Native_Place'              ,\n                          '6. Aggregate SSC Marks (in Percentage)'                                                                  :'SSC_Marks'           ,'7. Aggregate HSSC Marks(in Percentage)'                                        :'HSSC_Marks'            ,\n                          '8. Aggregate UG Marks(in Percentage)'                                                                :'UG_Marks'          ,'9. Aggregate PG Marks(in Percentage)'                                      :'PG_Marks'          ,\n                          '10. Are you having active backlogs?'                                                               :'Active_Backlogs'                 ,\n                          '11. Has your college placement cell provided you the information about the placement activity?'                                                                                                                          \n                          :'Placement_Notification'                        ,'12. Was the college providing training to the students in order to answer aptitude, coding tests and interview?'                                                                                                                 \n                          :'Training'          ,'13. Communication skills'                          :'Communication_Skills'                      ,'14. Analytical Reasoning'                          :'Analytical_Reasoning'                      ,'15. Logical Reasoning'                       :'Logical_Reasoning'                   ,'16. The company you were placed'                                 :'Company_Placed'                ,'17. Which location were you placed?'                                     \n                          :'Location'          ,'18. What position were you interviewed for?'                                             :'Position'          ,'19. CTC Received'                  :'CTC'     })\ndata.head()",
  "generation": "data=data.rename(columns={'1. Name':'Name','2. Email ID':'Email_ID','3. Highest Education':'Highest_Education','4. Year of Passing':'Year_Of_Passing','5. Native Place':'Native_Place','6. Aggregate SSC Marks (in Percentage)':'SSC_Marks','7. Aggregate HSSC Marks(in Percentage)':'HSSC_Marks','8. Aggregate UG Marks(in Percentage)':'UG_Marks','9. Aggregate PG Marks(in Percentage)':'PG_Marks','10. Are you having active backlogs?':'Active_Backlogs','11. Has your college placement cell provided you the information about the placement activity?':'Placement_Notification','12. Was the college providing training to the students in order to answer aptitude, coding tests and interview?':'Training','13. Communication skills':'Communication_Skills','14. Analytical Reasoning':'Analytical_Reasoning','15. Logical Reasoning':'Logical_Reasoning','16. The company you were placed':'Company_Placed','17. Which location were you placed?':'Location','18. What position were you interviewed for?':'Position','19. CTC Received':'CTC'})\ndata.head()"
 },
 {
  "nbid": "NB_976101",
  "file": "NB_976101,data-1.ipynb",
  "row_id": 7,
  "target_code": "data=data.drop(['Timestamp'           ],axis=1)\ndata.head()",
  "generation": "data=data.rename(columns={'1. Name':'Name','2. Email ID':'Email_ID','3. Highest Education':'Highest_Education','4. Year of Passing':'Year_Of_Passing','5. Native Place':'Native_Place',\n                          '6. Aggregate SSC Marks (in Percentage)':'SSC_Marks','7. Aggregate HSSC Marks(in Percentage)':'HSSC_Marks',\n                          '8. Aggregate UG Marks(in Percentage)':'UG_Marks','9. Aggregate PG Marks(in Percentage)':'PG_Marks',\n                          '10. Are you having active backlogs?':'Active_Backlogs',\n                          '11. Has your college placement cell provided you the information about the placement activity?'\n                          :'Placement_Notification','12. Was the college providing training to the students in order to answer aptitude, coding tests and interview?'\n                          :'Training','13. Communication skills':'Communication_Skills','14. Analytical Reasoning':'Analytical_Reasoning','15. Logical Reasoning':'Logical_Reasoning','16. The company you were placed':'Company_Placed','17. Which location were you placed?'\n                          :'Location','18. What position were you interviewed for?':'Position','19. CTC Received':'CTC'})"
 },
 {
  "nbid": "NB_977382",
  "file": "NB_977382,df,y_test_b-1.ipynb",
  "row_id": 12,
  "target_code": "df.index = df ['Date'      ]\ndf.head()",
  "generation": "df.columns = ['0', 't', 't1', 't2', 'Date']\ndf = df.reset_index(drop=True)\ndf['Date'] = pd.date_range(start='2002-01-31', periods=len(df), freq='M')\ndf = df[['Date', '0', 't', 't1', 't2']]\ndf.columns = pd.MultiIndex.from_product([['Unnamed: 0_level_0', '0', 't', 't1', 't2'], ['Date', '', 'Unnamed: 1_level_1', 'Unnamed: 2_level_1', 'Unnamed: 3_level_1', 'Unnamed: 4_level_1', 'Unnamed: 5_level_1']])\ndf.head()"
 },
 {
  "nbid": "NB_983206",
  "file": "NB_983206,credit_train,y-1.ipynb",
  "row_id": 7,
  "target_code": "credit_train = credit_train.drop('ID'    , axis = 1)\ncredit_train.head(3)",
  "generation": "credit_train.drop(['ID'], axis=1, inplace=True)\ncredit_train.head(3)"
 },
 {
  "nbid": "NB_984301",
  "file": "NB_984301,data,data_new-1.ipynb",
  "row_id": 6,
  "target_code": "data = data.drop(columns=['Value Footnotes'                 ])\ndata.head(5)",
  "generation": "data.drop('Value Footnotes', axis=1, inplace=True)\ndata.head(5)"
 },
 {
  "nbid": "NB_987151",
  "file": "NB_987151,crime_final,x_scaled,crime_final_norm-1.ipynb",
  "row_id": 24,
  "target_code": "crime_final['\uac15\uac04\uac80\uac70\uc728'       ] = crime_final['\uac15\uac04 \uac80\uac70'       ]/crime_final['\uac15\uac04 \ubc1c\uc0dd'       ]*100\ncrime_final['\uac15\ub3c4\uac80\uac70\uc728'       ] = crime_final['\uac15\ub3c4 \uac80\uac70'       ]/crime_final['\uac15\ub3c4 \ubc1c\uc0dd'       ]*100\ncrime_final['\uc0b4\uc778\uac80\uac70\uc728'       ] = crime_final['\uc0b4\uc778 \uac80\uac70'       ]/crime_final['\uc0b4\uc778 \ubc1c\uc0dd'       ]*100\ncrime_final['\uc808\ub3c4\uac80\uac70\uc728'       ] = crime_final['\uc808\ub3c4 \uac80\uac70'       ]/crime_final['\uc808\ub3c4 \ubc1c\uc0dd'       ]*100\ncrime_final['\ud3ed\ub825\uac80\uac70\uc728'       ] = crime_final['\ud3ed\ub825 \uac80\uac70'       ]/crime_final['\ud3ed\ub825 \ubc1c\uc0dd'       ]*100\ncrime_final.head()",
  "generation": "crime_final['\uac15\uac04\uac80\uac70\uc728'] = crime_final['\uac15\uac04 \uac80\uac70'] / crime_final['\uac15\uac04 \ubc1c\uc0dd'] * 100\ncrime_final['\uac15\ub3c4\uac80\uac70\uc728'] = crime_final['\uac15\ub3c4 \uac80\uac70'] / crime_final['\uac15\ub3c4 \ubc1c\uc0dd'] * 100\ncrime_final['\uc0b4\uc778\uac80\uac70\uc728'] = crime_final['\uc0b4\uc778 \uac80\uac70'] / crime_final['\uc0b4\uc778 \ubc1c\uc0dd'] * 100\ncrime_final['\uc808\ub3c4\uac80\uac70\uc728'] = crime_final['\uc808\ub3c4 \uac80\uac70'] / crime_final['\uc808\ub3c4 \ubc1c\uc0dd'] * 100\ncrime_final['\ud3ed\ub825\uac80\uac70\uc728'] = crime_final['\ud3ed\ub825 \uac80\uac70'] / crime_final['\ud3ed\ub825 \ubc1c\uc0dd'] * 100\ncrime_final.head()"
 },
 {
  "nbid": "NB_998263",
  "file": "NB_998263,df-1.ipynb",
  "row_id": 8,
  "target_code": "df[\"Month\"       ] = pd.to_datetime(df[\"Month\"       ], infer_datetime_format = True)\ndf.head()",
  "generation": "df['Month'] = pd.to_datetime(df['Month']).dt.date\ndf.head()"
 },
 {
  "nbid": "NB_998263",
  "file": "NB_998263,df-1.ipynb",
  "row_id": 10,
  "target_code": "df = df.set_index(df[\"Month\"       ])\ndf.head()",
  "generation": "df = pd.read_csv('../data/international-airline-passengers.csv')\ndf = df.rename(columns={'Month': 'Month', 'Thousand Passengers': 'Thousand Passengers', 'Unnamed: 0': 'Unnamed: 0_level_0'})\ndf = df.set_index('Month')\ndf.columns = pd.MultiIndex.from_product([['Thousand Passengers'], df.columns])\ndf.head()"
 },
 {
  "nbid": "NB_998263",
  "file": "NB_998263,df-1.ipynb",
  "row_id": 11,
  "target_code": "df = df.drop(columns = [\"Month\"       ])\ndf.head()",
  "generation": "df.drop(['Month', 'Unnamed: 0_level_0', 'Unnamed: 1_level_1', 'Unnamed: 2_level_1'], axis=1, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_111149",
  "file": "NB_111149,fasting-1.ipynb",
  "row_id": 16,
  "target_code": "fasting['D'   ] = pd.DatetimeIndex(fasting.Date).day\nfasting['M'   ] = pd.DatetimeIndex(fasting.Date).month\nfasting.head()",
  "generation": "fasting['D'] = pd.DatetimeIndex(fasting['Date']).day\nfasting['M'] = pd.DatetimeIndex(fasting['Date']).month\nfasting.head()"
 },
 {
  "nbid": "NB_3533624",
  "file": "NB_3533624,df-1.ipynb",
  "row_id": 4,
  "target_code": "df['Altura'        ] = df['Height'        ] * 0.0254\ndf['Peso'      ] = df['Weight'        ] * .453592\ndf['IMC'     ] = df['Peso'      ] / (df['Altura'        ] * df['Altura'        ])\ndf.head()",
  "generation": "df['Altura'] = df['Height']/100\ndf['Peso'] = df['Weight']*0.453592\ndf['IMC'] = df['Peso']/(df['Altura']**2)\ndf.head()"
 },
 {
  "nbid": "NB_3533624",
  "file": "NB_3533624,df-1.ipynb",
  "row_id": 6,
  "target_code": "df['classe'        ] = df['IMC'     ].apply(status)\ndf.head()",
  "generation": "df['classe'] = pd.cut(df['IMC'], bins=[0, 18.5, 24.9, 29.9, 34.9, 39.9, 100], labels=['Magreza', 'Saud\u00e1vel', 'Sobrepeso', 'Obesidade G1', 'Obesidade G2', 'Obesidade G3'])\ndf.head()"
 },
 {
  "nbid": "NB_7787113",
  "file": "NB_7787113,data-1.ipynb",
  "row_id": 2,
  "target_code": "from sklearn.preprocessing import LabelEncoder\nlab=LabelEncoder()\ndata.iloc[:,1]=lab.fit_transform(data.iloc[:,1])\ndata.iloc[:,2]=lab.fit_transform(data.iloc[:,2])\ndata.iloc[:,3]=lab.fit_transform(data.iloc[:,3])\ndata.iloc[:,4]=lab.fit_transform(data.iloc[:,4])\ndata.iloc[:,5]=lab.fit_transform(data.iloc[:,5])\ndata.head()",
  "generation": "le = LabelEncoder()\ndata['outlook'    ] = le.fit_transform(data['outlook'    ])\ndata['temperature'] = le.fit_transform(data['temperature'])\ndata['humidity'   ] = le.fit_transform(data['humidity'   ])\ndata['windy'      ] = le.fit_transform(data['windy'      ])\ndata['play'       ] = le.fit_transform(data['play'       ])\ndata.head()"
 },
 {
  "nbid": "NB_846611",
  "file": "NB_846611,ts-1.ipynb",
  "row_id": 62,
  "target_code": "ts.index=ts.index.to_period()\nts.tail()",
  "generation": "The input and output dataframes are not provided in the code context. Therefore, I cannot write the target code to transform the input dataframe to the output dataframe. Please provide the input and output dataframes for me to write the target code."
 },
 {
  "nbid": "NB_850655",
  "file": "NB_850655,df-1.ipynb",
  "row_id": 5,
  "target_code": "df=df.drop('Unnamed: 0'            , axis=1)\ndf.head()",
  "generation": "The input dataframe is already in the correct format, so no transformation is needed."
 },
 {
  "nbid": "NB_858946",
  "file": "NB_858946,df-1.ipynb",
  "row_id": 10,
  "target_code": "df['title_occurence'                 ] =  df.groupby(['channel_title'               ])['channel_title'               ].transform('count'       )\ndf.head()",
  "generation": "us_videos['average_tag_length'] = (us_videos.tags.str.len() / us_videos.number_of_tags)\npublish.columns = ['publish_date']\ndf = pd.concat([us_videos, publish], axis=1)\ndf = df.drop(['thumbnail_link', 'comments_disabled', 'ratings_disabled', 'video_error_or_removed'],axis=1)\ndf['Unnamed: 11'] = df['tags'].apply(lambda x: len(x.split('|')))\ndf.head()"
 },
 {
  "nbid": "NB_919844",
  "file": "NB_919844,d-1.ipynb",
  "row_id": 12,
  "target_code": "d['datetime'          ] = d['date'      ] + ' '     + d['time'      ]\nd.head()",
  "generation": "d['datetime'] = pd.to_datetime(d['date'] + ' ' + d['time'], utc=True).dt.tz_convert('Asia/Shanghai')\nd.head()"
 },
 {
  "nbid": "NB_919844",
  "file": "NB_919844,d-1.ipynb",
  "row_id": 13,
  "target_code": "d = d.drop(['date'      , 'time'       ],1)\nd.head()",
  "generation": "d['length'] = pd.to_timedelta(d['length'])\nd = d.drop(['url', 'play', 'danmaku', 'review', 'favorites', 'coin'], axis=1)"
 },
 {
  "nbid": "NB_919844",
  "file": "NB_919844,d-1.ipynb",
  "row_id": 14,
  "target_code": "d['datetime'          ] = pd.to_datetime(d['datetime'          ])\nd.head()",
  "generation": "d['datetime'] = pd.to_datetime(d['datetime'], utc=True).dt.tz_convert('Asia/Shanghai').dt.strftime('%Y-%m-%d %H:%M:%S')\nd.head()"
 },
 {
  "nbid": "NB_922751",
  "file": "NB_922751,df-1.ipynb",
  "row_id": 3,
  "target_code": "df = df.drop(['lat_long'          ,\n              'highlight_fur_color'                                   ,\n              'combination_of_primary_and_highlight_color'                                                          ,\n              'color_notes'                           ,\n              'specific_location'                                 ,\n              'above_ground_sighter_measurement'                                                ,\n              'other_activities'                                ,\n              'other_interactions'                                  ,\n              'zip_codes'                         ,\n              'community_districts'                                   ,\n              'borough_boundaries'                                  ,\n              'city_council_districts'                                      ,\n              'police_precincts'                                ], axis=1)\ndf = (df * 1)\nprint('Shape: '         ,df.shape)\ndf.head()",
  "generation": "df.drop(['hectare', 'shift', 'date', 'hectare_squirrel_number', 'highlight_fur_color', 'combination_of_primary_and_highlight_color', 'color_notes', 'specific_location', 'other_activities', 'lat_long', 'zip_codes', 'community_districts', 'borough_boundaries', 'city_council_districts', 'police_precincts'], axis=1, inplace=True)\ndf['running'] = df['running'].astype(int)\ndf['chasing'] = df['chasing'].astype(int)\ndf['climbing'] = df['climbing'].astype(int)\ndf['eating'] = df['eating'].astype(int)\ndf['foraging'] = df['foraging'].astype(int)\ndf['kuks'] = df['kuks'].astype(int)\ndf['quaas'] = df['quaas'].astype(int)\ndf['moans'] = df['moans'].astype(int)\ndf['tail_flags'] = df['tail_flags'].astype(int)\ndf['tail_twitches'] = df['tail_twitches'].astype(int)\ndf['approaches'] = df['approaches'].astype(int)\ndf['indifferent'] = df['indifferent'].astype(int)\ndf['runs_from'] = df['runs_from'].astype(int)\ndf.rename(columns={'primary_fur_color': 'location'}, inplace=True)\ndf.replace({'location': {'Cinnamon': 'Above Ground', 'Gray': 'Ground Plane'}}, inplace=True)\ndf.replace({False: 0, True: 1}, inplace=True)"
 },
 {
  "nbid": "NB_3346572",
  "file": "NB_3346572,deaths-1.ipynb",
  "row_id": 18,
  "target_code": "deaths = deaths[deaths.columns.drop(list(deaths.filter(regex='flag'      )))]\ndeaths.head()",
  "generation": "deaths = deaths.drop(['All causes, by age (years), All Ages**, flag', 'All causes, by age (years), \u226565, flag', 'All causes, by age (years), 45\u201364, flag', 'All causes, by age (years), 25\u201344, flag', 'All causes, by age (years), 1\u201324, flag', 'All causes, by age (years), LT 1, flag', 'P&I\u2020 Total, flag', 'Location 1', 'Location 2'], axis=1)\ndeaths = deaths.rename(columns={'Reporting Area': 'region', 'MMWR YEAR': 'YEAR'})\ndeaths.head()"
 },
 {
  "nbid": "NB_3387444",
  "file": "NB_3387444,df-1.ipynb",
  "row_id": 5,
  "target_code": "z=df.Customer.value_counts()\ndf['Counts'        ]=df['Customer'          ].map(z.to_dict())\ndf.head()",
  "generation": "df['Counts'] = df.groupby('Customer')['Customer'].transform('count')\ndf.head()"
 },
 {
  "nbid": "NB_3412036",
  "file": "NB_3412036,paris_trafic_cleaned-1.ipynb",
  "row_id": 14,
  "target_code": "paris_trafic_cleaned['key'     ] = range(1, len(paris_trafic_cleaned) + 1)\nparis_trafic_cleaned.head(1)",
  "generation": "paris_trafic_cleaned['key'] = paris_trafic_cleaned.index + 1\nparis_trafic_cleaned.head()"
 },
 {
  "nbid": "NB_3451041",
  "file": "NB_3451041,df-1.ipynb",
  "row_id": 25,
  "target_code": "df['PatientId'           ] = df['PatientId'           ].astype(int)\ndf.head()",
  "generation": "df['PatientId'] = df['PatientId'].astype(int)\ndf.rename(columns={'Neighbourhood': 'Hospital_location', 'Hipertension': 'Hypertension', 'Handcap': 'Handicap'}, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_724916",
  "file": "NB_724916,df-1.ipynb",
  "row_id": 11,
  "target_code": "df['Years'       ] = df['tradeTime'           ].astype('str'     ).apply(lambda x: int(x.split('/'   )[2]))\ndf['Months'        ] = df['tradeTime'           ].astype('str'     ).apply(lambda x: int(x.split('/'   )[0]))\ndf.head(10)",
  "generation": "df['tradeTime'] = pd.to_datetime(df['tradeTime'])\ndf['Years'] = df['tradeTime'].dt.year.apply(lambda x: x-2000)\ndf['Months'] = df['tradeTime'].dt.month\ndf.head(10)"
 },
 {
  "nbid": "NB_728010",
  "file": "NB_728010,df-1.ipynb",
  "row_id": 42,
  "target_code": "df[\"score_sq\"          ] = df[\"score\"       ]**2\ndf.head()",
  "generation": "df['score_sq'] = df['score']**2\ndf.head()"
 },
 {
  "nbid": "NB_743755",
  "file": "NB_743755,data-1.ipynb",
  "row_id": 15,
  "target_code": "data = data.replace(-9, 0)\ndata.head()",
  "generation": "data = data.replace(-9, 0)\ndata.head()"
 },
 {
  "nbid": "NB_872695",
  "file": "NB_872695,dfcars-1.ipynb",
  "row_id": 11,
  "target_code": "dfcars=dfcars.rename(columns={\"Unnamed: 0\"            :\"name\"      })\ndfcars.head()",
  "generation": "dfcars = dfcars.rename(columns={'Unnamed: 0': 'name'})\ndfcars.head()"
 },
 {
  "nbid": "NB_880535",
  "file": "NB_880535,data-1.ipynb",
  "row_id": 3,
  "target_code": "values = {'Location'          : 'North America'                }\ndata = data.fillna(value=values)\ndata.head()",
  "generation": "data['Location'].fillna('North America', inplace=True)\ndata.head()"
 },
 {
  "nbid": "NB_895718",
  "file": "NB_895718,airline-1.ipynb",
  "row_id": 30,
  "target_code": "airline.index = pd.to_datetime(airline.index)\nairline.head()",
  "generation": "airline['Thousands of Passengers'] = airline['Thousands of Passengers'].ewm(span=12).mean()\nairline.head()"
 },
 {
  "nbid": "NB_895718",
  "file": "NB_895718,df-1.ipynb",
  "row_id": 43,
  "target_code": "df['Month'       ] = pd.to_datetime(df['Month'       ])\ndf.head()",
  "generation": "df.columns = ['Month', 'Milk in Pounds per Cow']\ndf['Month'] = pd.to_datetime(df['Month'])\ndf.set_index('Month', inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_748780",
  "file": "NB_748780,fluor_data-1.ipynb",
  "row_id": 13,
  "target_code": "fluor_data[\"num_mutations\"               ] = [len(str(x).split(':'   )) for x in fluor_data[\"aaMutations\"             ]]\nfluor_data.loc[0,\"num_mutations\"               ] = 0\nfluor_data.head()",
  "generation": "fluor_data['num_mutations'] = fluor_data['aaMutations'].str.count(':') + 1\nfluor_data.head()"
 },
 {
  "nbid": "NB_754392",
  "file": "NB_754392,dfFeederData-1.ipynb",
  "row_id": 9,
  "target_code": "dfFeederData = dfFeederData.reset_index()\ndfFeederData.head()",
  "generation": "dfFeederData = dfFeederData.reset_index()\ndfFeederData.head()"
 },
 {
  "nbid": "NB_757318",
  "file": "NB_757318,aus-1.ipynb",
  "row_id": 7,
  "target_code": "aus.Date = pd.to_datetime(aus.Date)\naus.head()",
  "generation": "aus['Date'] = pd.to_datetime(aus['Date'], format='%YM%m')\naus.head()"
 },
 {
  "nbid": "NB_783295",
  "file": "NB_783295,under5_deaths_df-1.ipynb",
  "row_id": 4,
  "target_code": "under5_deaths_df['Under 5 Mortality'                   ], under5_deaths_df['Mortality Range'                 ] = zip(*under5_deaths_df['Under 5 Mortality'                   ].str.split(' '   ).tolist())\nunder5_deaths_df.head()",
  "generation": "under5_deaths_df[['Under 5 Mortality', 'Mortality Range']] = under5_deaths_df['Under 5 Mortality'].str.split('[', expand=True)\nunder5_deaths_df['Mortality Range'] = '[' + under5_deaths_df['Mortality Range']\nunder5_deaths_df.head()"
 },
 {
  "nbid": "NB_784069",
  "file": "NB_784069,df-1.ipynb",
  "row_id": 6,
  "target_code": "df = df[(df['Layout'        ].notnull())]\ndf.head(10)",
  "generation": "df['Layout'] = df.loc[(df['Layout'].str.extract('^\\d(.*?)\\d.*?', expand=False) == '\u5ba4'), 'Layout']\ndf.head(10)"
 },
 {
  "nbid": "NB_784069",
  "file": "NB_784069,df-1.ipynb",
  "row_id": 7,
  "target_code": "df['Layout_room_num'                 ] = df['Layout'        ].str.extract('(^\\d).*'         , expand=False).astype('int64'       )\ndf['Layout_hall_num'                 ] = df['Layout'        ].str.extract('^\\d.*?(\\d).*'              , expand=False).astype('int64'       ) \ndf.head(10)",
  "generation": "df['Layout_room_num'] = df['Layout'].str.extract('(^\\d).*', expand=False).astype('int64')\ndf['Layout_hall_num'] = df['Layout'].str.extract('^\\d.*?(\\d).*', expand=False).astype('int64')\ndf.head(10)"
 },
 {
  "nbid": "NB_793565",
  "file": "NB_793565,data-1.ipynb",
  "row_id": 6,
  "target_code": "data['profit'        ] = data['total_revenue'               ] - data['avg_cost_per_click'                    ] * data['clicked'         ]\ndata.head()",
  "generation": "data['profit'] = data['total_revenue'] - data['avg_cost_per_click'] * data['clicked']\ndata.head()"
 },
 {
  "nbid": "NB_2963065",
  "file": "NB_2963065,df-1.ipynb",
  "row_id": 3,
  "target_code": "df.index = df['DateTime'          ]\ndf.head()",
  "generation": "df['Open'] = df['Open'].str.replace(',', '').astype(float)\ndf['30d_avg'] = df['Open'].rolling(window=30, center=False).mean()\ndf['Std'] = df['Open'].rolling(window=30, center=False).std()\ndf.head()"
 },
 {
  "nbid": "NB_3011622",
  "file": "NB_3011622,df-1.ipynb",
  "row_id": 2,
  "target_code": "df.columns = df.columns.str.strip().str.lower().str.replace(' '   , '_'    ).str.replace('('   , ''   ).str.replace(')'   , ''   )\ndf.head(5)",
  "generation": "df = df.drop('Unnamed: 0', axis=1)\ndf.columns = df.columns.str.strip().str.lower() \ndf.head(5)"
 },
 {
  "nbid": "NB_3025200",
  "file": "NB_3025200,star_wars-1.ipynb",
  "row_id": 8,
  "target_code": "star_wars = star_wars[pd.notnull(star_wars[\"RespondentID\"              ])]\nstar_wars.head()",
  "generation": "star_wars = star_wars[pd.notnull(star_wars['RespondentID'])]"
 },
 {
  "nbid": "NB_3025200",
  "file": "NB_3025200,star_wars-1.ipynb",
  "row_id": 11,
  "target_code": "true_false = {\n    \"Yes\"         : True,\n    \"No\"        : False\n}\nstar_wars[\"Have you seen any of the 6 films in the Star Wars franchise?\"                                                              ] = star_wars[\"Have you seen any of the 6 films in the Star Wars franchise?\"                                                              ].map(true_false)\nstar_wars[\"Do you consider yourself to be a fan of the Star Wars film franchise?\"                                                                       ] = star_wars[\"Do you consider yourself to be a fan of the Star Wars film franchise?\"                                                                       ].map(true_false)\nstar_wars.head()",
  "generation": "yes_no = {\"Yes\": True, \"No\": False}\nfor col in [\"Have you seen any of the 6 films in the Star Wars franchise?\", \"Do you consider yourself to be a fan of the Star Wars film franchise?\"]:\n    star_wars[col] = star_wars[col].map(yes_no)\nstar_wars.head()"
 },
 {
  "nbid": "NB_678345",
  "file": "NB_678345,brew_df-1.ipynb",
  "row_id": 4,
  "target_code": "brew_df = brew_df[(brew_df[\"Year Established\"                  ] >= 2010) & (brew_df[\"Year Established\"                  ] <= 2017)]\nbrew_df.head()",
  "generation": "brew_df = brew_df.drop_duplicates(subset=['Brewery Name'])\nbrew_df = brew_df[brew_df['Still in Business?'] == 'Y']\nbrew_df = brew_df[brew_df['Closed?'] == 'N']\nbrew_df = brew_df[brew_df['Year Established'] != 'unknown']\nbrew_df['Year Established'] = pd.to_numeric(brew_df['Year Established'])\nbrew_df = brew_df[brew_df['Year Established'] >= 2010]\nbrew_df = brew_df.sort_values(by=['Year Established'], ascending=True)\nbrew_df.head()"
 },
 {
  "nbid": "NB_685100",
  "file": "NB_685100,x-1.ipynb",
  "row_id": 29,
  "target_code": "x['Customer type'               ] = le.fit_transform(x['Customer type'               ])\nx.head()",
  "generation": "x['Customer type'] = le.fit_transform(x['Customer type'])\nx.head()"
 },
 {
  "nbid": "NB_720765",
  "file": "NB_720765,gm_sales-1.ipynb",
  "row_id": 40,
  "target_code": "gm_sales = gm_sales.melt(id_vars='Year'      , var_name = 'Month'        , value_name = 'Sales'        )\ngm_sales.head(8)",
  "generation": "gm_sales_melted = pd.melt(gm_sales, id_vars=['Year'], var_name='Month', value_name='Sales')\ngm_sales_melted.head(5)"
 },
 {
  "nbid": "NB_721343",
  "file": "NB_721343,bksstatchil-1.ipynb",
  "row_id": 70,
  "target_code": "bksstatchil['chilbkPerVol'              ] = bksstatchil['child_count'             ]/bksstatchil['vol'     ]\nbksstatchil.head(5)",
  "generation": "bksstatchil['chilbkPerVol'] = bksstatchil['child_count'] / bksstatchil['vol'] \nbksstatchil.head()"
 },
 {
  "nbid": "NB_3049289",
  "file": "NB_3049289,data-1.ipynb",
  "row_id": 6,
  "target_code": "data = data.drop([\"Year\"      , \"App.\"       , \"Unit\"       , \"Purpose\"          , \"Source\"         ], axis = 1)\ndata = data.rename(columns={\"Importer reported quantity\"                            : \"Import Quantity\"                  , \"Exporter reported quantity\"                             : \"Export Quantity\"                  })\ndata.head()",
  "generation": "data = data[['Taxon', 'Class', 'Order', 'Family', 'Genus', 'Importer', 'Exporter', 'Origin', 'Importer reported quantity', 'Exporter reported quantity', 'Term']]\ndata.rename(columns = {'Importer reported quantity': 'Import Quantity', 'Exporter reported quantity': 'Export Quantity'}, inplace = True)\ndata.head()"
 },
 {
  "nbid": "NB_3091191",
  "file": "NB_3091191,df-1.ipynb",
  "row_id": 4,
  "target_code": "labelled_diagnoses = [\n    'benign'            ,\n    'malignant'               ,\n]\nfor idx, label in enumerate(labelled_diagnoses):\n    df.diagnosis = df.diagnosis.replace(label, idx)\ndf.head()",
  "generation": "df['diagnosis'] = df['diagnosis'].map({'malignant': 1, 'benign': 0})\ndf.head()"
 },
 {
  "nbid": "NB_3112465",
  "file": "NB_3112465,df_full-1.ipynb",
  "row_id": 5,
  "target_code": "gpa = df_full[['person'        ,'high_school_GPA'                 , 'college_GPA'              ]] \ngpa = gpa.rename(columns={'person'        : 'person_other'               })  \ngpa = gpa.rename(columns={'high_school_GPA'                 : 'high_school_GPA_roommate'                           }) \ngpa = gpa.rename(columns={'college_GPA'             : 'college_GPA_roommate'                       }) \ndf_full = pd.merge(df_full,gpa[['person_other'              ,'high_school_GPA_roommate'                          ,'college_GPA_roommate'                      ]],on='person_other'              , how='left'      )\ndf_full.head()",
  "generation": "unique1 = unique.rename(columns={'1': 'person_other', '2': 'high_school_GPA_roommate'})\nunique2 = unique.rename(columns={'2': 'person_other', '1': 'high_school_GPA_roommate'})\n\ndata1 = data.merge(unique1, on='person_other')\ndata2 = data.merge(unique2, on='person_other')\n\ndata1 = data1.rename(columns={'high_school_GPA': 'high_school_GPA_person', 'college_GPA': 'college_GPA_person'})\ndata2 = data2.rename(columns={'high_school_GPA': 'high_school_GPA_person', 'college_GPA': 'college_GPA_person'})\n\ndata1 = data1.merge(data2[['person', 'high_school_GPA_roommate', 'college_GPA_person']], on='person')\ndata2 = data2.merge(data1[['person', 'high_school_GPA_roommate', 'college_GPA_person']], on='person')\n\ndf_full = pd.concat([data1, data2], axis=0, ignore_index=True) \ndf_full = df_full[['person', 'room', 'high_school_GPA_person', 'college_GPA_person', 'person_other', 'high_school_GPA_roommate', 'college_GPA_roommate']]"
 },
 {
  "nbid": "NB_635266",
  "file": "NB_635266,nycrime-1.ipynb",
  "row_id": 3,
  "target_code": "nycrime.columns = map(str.lower, nycrime.columns)\nnycrime.columns = nycrime.columns.str.split().str[0].str.strip()\nnycrime.fillna(0, inplace=True)\nnycrime.head()",
  "generation": "nycrime.columns = nycrime.columns.str.lower().str.split().str[0]\nnycrime.fillna(0, inplace=True)\nnycrime.head()"
 },
 {
  "nbid": "NB_647641",
  "file": "NB_647641,data-1.ipynb",
  "row_id": 2,
  "target_code": "data = data.sample(frac=1)\ndata.head()",
  "generation": "data = data.sample(frac=1).reset_index(drop=True)\ndata"
 },
 {
  "nbid": "NB_647641",
  "file": "NB_647641,data-1.ipynb",
  "row_id": 5,
  "target_code": "df = pd.DataFrame(data['trust_factor'              ])\nx = df[['trust_factor'              ]].values.astype(float)\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndf_normalized = pd.DataFrame(x_scaled)\ndf_normalized = df_normalized.round(1)\ndata[\"normalized_trust_values\"                         ] = df_normalized\ndata.head()",
  "generation": "min_max_scaler = preprocessing.MinMaxScaler()\ndata['normalized_trust_values'] = min_max_scaler.fit_transform(data[['trust_factor']])"
 },
 {
  "nbid": "NB_703632",
  "file": "NB_703632,df_noshow-1.ipynb",
  "row_id": 21,
  "target_code": "df_noshow['scheduled_day'               ]=df_noshow['scheduled_day'               ].apply(lambda x: x.strftime('%Y-%m-%d'          ))\ndf_noshow.head(2)",
  "generation": "df_noshow['scheduled_day'] = df_noshow['scheduled_day'].dt.date\ndf_noshow.head()"
 },
 {
  "nbid": "NB_3148885",
  "file": "NB_3148885,df-1.ipynb",
  "row_id": 7,
  "target_code": "IDs = df[[\"ANFANG\"        ,\"ID\"    , \"VORNAME\"          , \"NACHNAME\"           ]].copy()\ndf = df.drop([\"ID\"    , \"NUMIND\"         , \"BERUF\"        , \"VORNAME\"          , \"NACHNAME\"           ], axis = 1)\ntimeline= df[\"ANFANG\"        ].unique()\nprint(timeline)\ndf.head()",
  "generation": "df = df[['ANFANG', 'ALTER', 'GESCHLECHT', 'PARTEI_KURZ', 'AUSLAND', 'MANDATSART', 'WP1', 'WP2', 'WP3', 'WP4', 'WP5', 'WP6', 'WP7', 'WP8', 'WP9', 'WP10', 'WP11', 'WP12', 'WP13', 'WP14', 'WP15', 'WP16', 'WP17', 'WP18', 'WP19']]\ndf.head()"
 },
 {
  "nbid": "NB_3233430",
  "file": "NB_3233430,df-1.ipynb",
  "row_id": 6,
  "target_code": "df = df.drop(['Engine Fuel Type'                  , 'Market Category'                  , 'Vehicle Style'                , 'Popularity'             , 'Number of Doors'                  , 'Vehicle Size'               ], axis=1)\ndf.head(5)",
  "generation": "df = df.drop(['Engine Fuel Type', 'Number of Doors', 'Market Category', 'Vehicle Size', 'Vehicle Style', 'Popularity', 'highway MPG', 'city mpg'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_3233430",
  "file": "NB_3233430,df-1.ipynb",
  "row_id": 7,
  "target_code": "df = df.rename(columns={\"Engine HP\"           : \"HP\"     , \"Engine Cylinders\"                   : \"Cylinders\"            , \"Transmission Type\"                    : \"Transmission\"               , \"Driven_Wheels\"                : \"Drive Mode\"             ,\"highway MPG\"             : \"MPG-H\"        , \"city mpg\"           : \"MPG-C\"        , \"MSRP\"       : \"Price\"         })\ndf.head(5)",
  "generation": "df = df.rename(columns={\n    'Engine HP': 'HP',\n    'Engine Cylinders': 'Cylinders',\n    'Transmission Type': 'Transmission',\n    'Driven_Wheels': 'Drive Mode',\n    'highway MPG': 'MPG-H',\n    'city mpg': 'MPG-C',\n    'MSRP': 'Price'\n})\ndf.head(5)"
 },
 {
  "nbid": "NB_3233430",
  "file": "NB_3233430,df-1.ipynb",
  "row_id": 11,
  "target_code": "df = df.drop_duplicates()\ndf.head(5)",
  "generation": "df = df.drop_duplicates()\ndf.head(5)"
 },
 {
  "nbid": "NB_605896",
  "file": "NB_605896,data-1.ipynb",
  "row_id": 11,
  "target_code": "x_to_be_replaced=['Male'      ,'Female'        ]\nValue=[1,0]\ndata['Gender'        ] = data['Gender'        ].replace(to_replace =x_to_be_replaced, value = Value, regex = True) \ndata.head() ",
  "generation": "data['Gender'] = data['Gender'].replace(['Male','Female'],[1,0])\ndata.head()"
 },
 {
  "nbid": "NB_613794",
  "file": "NB_613794,X-1.ipynb",
  "row_id": 132,
  "target_code": "df_dummies =  pd.get_dummies(X[categorical_features], drop_first=True)\nX = pd.concat([X.drop(categorical_features, axis=1), df_dummies], axis=1)\nX.head(5)",
  "generation": "X = pd.get_dummies(X, columns=categorical_features, drop_first=True)\nX.head(5)"
 },
 {
  "nbid": "NB_661817",
  "file": "NB_661817,messages-1.ipynb",
  "row_id": 7,
  "target_code": "messages = messages.drop([\"Unnamed: 2\"            , \"Unnamed: 3\"             , \"Unnamed: 4\"             ], axis=1)\nmessages = messages.rename(columns={\"v1\"    :\"label\"       , \"v2\"     :\"text\"      })\nmessages.tail()",
  "generation": "messages = messages.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\nmessages = messages.rename(columns={'v1': 'label', 'v2': 'text'})\nmessages.head()"
 },
 {
  "nbid": "NB_661817",
  "file": "NB_661817,messages-1.ipynb",
  "row_id": 9,
  "target_code": "messages['label_num'           ] = messages.label.map({'ham'     :0, 'spam'       :1})\nmessages.head()",
  "generation": "messages['label_num'] = messages['label'].map({'ham':0, 'spam':1})"
 },
 {
  "nbid": "NB_77730",
  "file": "NB_77730,mk-1.ipynb",
  "row_id": 7,
  "target_code": "mk['datetime'          ] = pd.to_datetime(mk[\"datetime\"          ])\nmk[mk['datetime'          ].dt.hour >= 7]\nmk.drop('DATE'      , axis = 1,  inplace = True)\nmk.head(2)",
  "generation": "mk = mk.drop(columns=['DATE'])"
 },
 {
  "nbid": "NB_2108623",
  "file": "NB_2108623,df-1.ipynb",
  "row_id": 3,
  "target_code": "df['yearQuarter'             ] = df['year'      ].astype(str) + ', '      + df['quarter'         ]\ndf.head(3)",
  "generation": "df['yearQuarter'] = df['year'].astype(str) + ', ' + df['quarter']\ndf.head()"
 },
 {
  "nbid": "NB_2125935",
  "file": "NB_2125935,df-1.ipynb",
  "row_id": 5,
  "target_code": "le = LabelEncoder()\ndf.categoria = le.fit_transform(df.categoria)\ndf.head()",
  "generation": "le = LabelEncoder()\ndf['categoria'] = le.fit_transform(df['categoria'])\ndf.head()"
 },
 {
  "nbid": "NB_2144297",
  "file": "NB_2144297,df_test-1.ipynb",
  "row_id": 27,
  "target_code": "df_test = df_test.select_dtypes(include=['number'        ]).copy()\ndf_test.head()",
  "generation": "df_test = df_test[['id', 'amount_tsh', 'construction_year', 'gps_height', 'latitude', 'longitude', 'population', 'region_code', 'district_code', 'year']]\ndf_test.fillna(df_test.mean(), inplace=True)\ndf_test.set_index('id', inplace=True)\ndf_test.head()"
 },
 {
  "nbid": "NB_2146140",
  "file": "NB_2146140,train-1.ipynb",
  "row_id": 3,
  "target_code": "train = train.sort_values(by = 'Date'       )\ntrain.head()",
  "generation": "scaler = MinMaxScaler()\ntrain[['Open', 'High', 'Low', 'Close', 'Volume']] = scaler.fit_transform(train[['Open', 'High', 'Low', 'Close', 'Volume']])\ntrain.head(2)"
 },
 {
  "nbid": "NB_3245474",
  "file": "NB_3245474,df-1.ipynb",
  "row_id": 2,
  "target_code": "df['percent_female'                ] = 100 - df.percent_male\ndf.head()",
  "generation": "df['percent_female'] = 100 - df['percent_male']\ndf.head()"
 },
 {
  "nbid": "NB_3250389",
  "file": "NB_3250389,df-1.ipynb",
  "row_id": 3,
  "target_code": "df['Family'        ] = df['SibSp'       ]+df['Parch'       ]\ndf.head()",
  "generation": "df['Family'] = df['SibSp'] + df['Parch']\ndf.head()"
 },
 {
  "nbid": "NB_3276741",
  "file": "NB_3276741,df-1.ipynb",
  "row_id": 34,
  "target_code": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['DEST'      ] = le.fit_transform(df['DEST'      ])\ndf['ORIGIN'        ] = le.fit_transform(df['ORIGIN'        ])\ndf.head()",
  "generation": "le = LabelEncoder()\ndf['ORIGIN'] = le.fit_transform(df['ORIGIN'])\ndf['DEST'] = le.fit_transform(df['DEST'])\ndf.head()"
 },
 {
  "nbid": "NB_3281730",
  "file": "NB_3281730,final_merge-1.ipynb",
  "row_id": 2,
  "target_code": "from sklearn.preprocessing import LabelEncoder\nlabelencoder= LabelEncoder() \nfinal_merge['State'       ] = labelencoder.fit_transform(final_merge['State'       ]) \nfinal_merge.head(2)",
  "generation": "le = LabelEncoder()\nfinal_merge['State'] = le.fit_transform(final_merge['State'])\nfinal_merge.head()"
 },
 {
  "nbid": "NB_3286834",
  "file": "NB_3286834,df-1.ipynb",
  "row_id": 3,
  "target_code": "col = ['Categories'            , 'Q&A'      ]\ndf = df[col]\ndf.head()",
  "generation": "df = df[['Categories', 'Q&A']]"
 },
 {
  "nbid": "NB_617442",
  "file": "NB_617442,data-1.ipynb",
  "row_id": 72,
  "target_code": "data['Review Text'             ] = data['Title'       ] + ' '     + data['Review Text'             ]\ndata.drop('Title'       , inplace=True, axis=1)\ndata.rename(columns = {'Recommended IND'                 :'Recommend'           }, inplace = True)\ndata.head()",
  "generation": "data['Review Text'] = data['Title'] + ' ' + data['Review Text']\ndata.drop(['Title'], axis=1, inplace=True)\ndata.columns = ['Recommend', 'Review Text']\ndata.head()"
 },
 {
  "nbid": "NB_617839",
  "file": "NB_617839,covid-1.ipynb",
  "row_id": 5,
  "target_code": "covid.columns = covid.columns.str.replace('Admin2'        ,'County'        )\ncovid.columns = covid.columns.str.replace('Long_'       ,'Long'      )\ncovid.head()",
  "generation": "import seaborn as sns\ncovid_raw = pd.read_csv('maycovid.csv')\ndel_col_list = ['FIPS', 'Last_Update']\ncovid = covid_raw.drop(del_col_list, axis=1)\ncovid = covid.rename(columns={'Admin2': 'County', 'Long_': 'Long'})\ncovid.head()"
 },
 {
  "nbid": "NB_622592",
  "file": "NB_622592,temp_df-1.ipynb",
  "row_id": 37,
  "target_code": "temp_df = temp_df.drop(columns = [\"Entity\"        , \"Code\"       , \"Year\"       , \"Traditional biofuels (terrawatt-hours)\"                                         ])\ntemp_df.head()",
  "generation": "df = df[['Hydropower (terawatt-hours)', 'Solar (terawatt-hours)', 'Wind (terawatt-hours)', 'Other renewables (terawatt-hours)']]\ndf.head()"
 },
 {
  "nbid": "NB_624344",
  "file": "NB_624344,df-1.ipynb",
  "row_id": 14,
  "target_code": "df['title_length'              ] = df['title'       ].apply(len)\ndf.head()",
  "generation": "data = df[df['label'].map(len) == 4]\ndata.head()"
 },
 {
  "nbid": "NB_624344",
  "file": "NB_624344,df-1.ipynb",
  "row_id": 21,
  "target_code": "df[\"total\"       ] = df[\"title\"       ] + df[\"text\"      ]\ndf.head()",
  "generation": "df['title_text'] = df['title'] + ' ' + df['text']\ndf = df.drop(['title', 'text'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_624344",
  "file": "NB_624344,df_test-1.ipynb",
  "row_id": 290,
  "target_code": "df_test[\"total\"       ] = df_test[\"title\"       ] + df_test[\"text\"      ]\ndf_test.head()",
  "generation": "df_test['text' ] = df_test['title'] + df_test['text']\ndf_test.head()"
 },
 {
  "nbid": "NB_625624",
  "file": "NB_625624,mdf2-1.ipynb",
  "row_id": 24,
  "target_code": "print(df.columns)\nfeaturelist1=['VE_TOTAL'          , 'PEDS'       , 'PERSONS'          , 'COUNTY'         , 'MONTH'        , 'DAY_WEEK'           , 'SCH_BUS'          , 'FATALS'         , 'DRUNK_DR'           ]\nmdf2=df.groupby('STATE'       ).aggregate(np.mean)\nmdf2=mdf2[featurelist1]\nmdf2.head()",
  "generation": "featurelist1 = ['VE_TOTAL', 'PEDS', 'PERSONS', 'COUNTY', 'MONTH', 'DAY_WEEK', 'SCH_BUS', 'FATALS', 'DRUNK_DR']\nmdf2 = df.groupby('STATE').aggregate(np.mean)\nmdf2 = mdf2[featurelist1]\nmdf2.head()"
 },
 {
  "nbid": "NB_6960397",
  "file": "NB_6960397,df-1.ipynb",
  "row_id": 3,
  "target_code": "df['rate'      ] = round(df['qty_imaged'            ] / df['minutes_spent'               ], 3)\ndf.head(2)",
  "generation": "df['rate'] = df['qty_imaged'] / df['minutes_spent']\ndf.head(2)"
 },
 {
  "nbid": "NB_6960397",
  "file": "NB_6960397,df-1.ipynb",
  "row_id": 4,
  "target_code": "df['tot_min'         ] = df.groupby('name'      )['minutes_spent'               ].cumsum()\ndf.head(3)",
  "generation": "df['tot_min'] = df.groupby('name')['minutes_spent'].cumsum()\ndf['binned'] = pd.cut(df['tot_min'], bins=[0, 120, 180, 240, 300, 360, 420, 480, 540, 600, 660, 720, 780, 840, 900, 960, 1020, 1080, 1140, 1200, 1260, 1320, 1380, 1440])\ndf['tot_hours'] = round(df['tot_min'] / 60, 2)\ndf.head()"
 },
 {
  "nbid": "NB_7005660",
  "file": "NB_7005660,olympics_df-1.ipynb",
  "row_id": 46,
  "target_code": "inches = 39.3700787\npounds = 2.20462262\nolympics_df[\"height(in)\"            ] = olympics_df[\"height\"        ]*inches \nolympics_df[\"weight(lbs)\"             ] = olympics_df[\"weight\"        ]*pounds \nolympics_df.head()",
  "generation": "olympics_df['height(in)'] = olympics_df['height'] * 39.3700787\nolympics_df['weight(lbs)'] = olympics_df['weight'] * 2.20462262\nolympics_df"
 },
 {
  "nbid": "NB_2170394",
  "file": "NB_2170394,df-1.ipynb",
  "row_id": 22,
  "target_code": "df = df.replace({'voice mail plan'                 : d})\ndf.head()",
  "generation": "df['voice mail plan'] = df['voice mail plan'].astype('bool')\ndf.head()"
 },
 {
  "nbid": "NB_2170394",
  "file": "NB_2170394,df-1.ipynb",
  "row_id": 29,
  "target_code": "df['total charge'              ] = df['total day charge'                  ] + df['total eve charge'                  ] +                     df['total night charge'                    ] + df['total intl charge'                   ]\ndf.head()",
  "generation": "df['total calls'] = df['total day calls'] + df['total eve calls'] + df['total night calls']\ndf['total charge'] = df['total day charge'] + df['total eve charge'] + df['total night charge']\ndf.head()"
 },
 {
  "nbid": "NB_2181938",
  "file": "NB_2181938,df-1.ipynb",
  "row_id": 31,
  "target_code": "df = df.replace({'Voice mail plan'                 : d})\ndf.head()",
  "generation": "d = {'No' : False, 'Yes' : True}\ndf['International plan'] = df['International plan'].map(d)"
 },
 {
  "nbid": "NB_2181938",
  "file": "NB_2181938,df-1.ipynb",
  "row_id": 43,
  "target_code": "df['Total charge'              ] = df['Total day charge'                  ] + df['Total eve charge'                  ] +                     df['Total night charge'                    ] + df['Total intl charge'                   ]\ndf.head()",
  "generation": "df['Total calls'] = df['Total day calls'] + df['Total eve calls'] + df['Total night calls'] + df['Total intl calls']\ndf['Total charge'] = df['Total day charge'] + df['Total eve charge'] + df['Total night charge'] + df['Total intl charge']\ndf.head()"
 },
 {
  "nbid": "NB_2190258",
  "file": "NB_2190258,df-1.ipynb",
  "row_id": 2,
  "target_code": "df[\"Family\"        ]=df['SibSp'       ] +df[\"Parch\"       ]\ndf.head(2)",
  "generation": "df['Family'] = df['SibSp'] + df['Parch']\ndf.head()"
 },
 {
  "nbid": "NB_2190469",
  "file": "NB_2190469,data-1.ipynb",
  "row_id": 3,
  "target_code": "data= data.dropna(axis = 0, how ='any'     ) \ndata.head()",
  "generation": "The given input dataframe is not in a proper format. Please provide the input dataframe in a proper format."
 },
 {
  "nbid": "NB_2192987",
  "file": "NB_2192987,df-1.ipynb",
  "row_id": 1,
  "target_code": "df = df.iloc[:, 3:]\ndf.head()",
  "generation": "df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_2192987",
  "file": "NB_2192987,df-1.ipynb",
  "row_id": 2,
  "target_code": "from sklearn.preprocessing import LabelBinarizer\nlb = LabelBinarizer()\ndf['Gender'        ] = lb.fit_transform(df['Gender'        ])\ndf.head()",
  "generation": "lb = LabelBinarizer()\ndf['Gender'] = lb.fit_transform(df['Gender'])\ndf.head()"
 },
 {
  "nbid": "NB_2223989",
  "file": "NB_2223989,df_final-1.ipynb",
  "row_id": 14,
  "target_code": "df_final['Budget_Percent'                ] = df_final['Budget'        ] / df_final['Budget'        ].sum() * 500\ndf_final.head()",
  "generation": "df_grouped = df.groupby('Site', as_index=False)['Budget'].sum()\ndf_critical = pd.read_csv('data/map_budget_to_principal_list.csv')\ndf_with_critical = pd.merge(df_grouped, df_critical, how='right', \n                            left_on = 'Site', right_on = 'Budget_Category', \n                            left_index = False, right_index = False, sort=False)\ndf_final = df_with_critical.copy()\ndf_final['Budget_Percent'] = df_final['Budget'] / df_final['Budget'].sum() * 100\ndf_final.dropna(inplace=True)\ndf_final.head()"
 },
 {
  "nbid": "NB_2243583",
  "file": "NB_2243583,grades-1.ipynb",
  "row_id": 13,
  "target_code": "labels = ('F'   , 'D'    , 'C'    , 'B'    , 'A'    )\ngrades[\"Letter\"        ] = pd.cut(grades[\"Final score\"             ], [0, 60, 70, 80, 90, 100], labels=labels) \ngrades.head()",
  "generation": "# Define the letter grade function\ndef letter_grade(score):\n    if score >= 90:\n        return \"A\"\n    elif score >= 80:\n        return \"B\"\n    elif score >= 70:\n        return \"C\"\n    elif score >= 60:\n        return \"D\"\n    else:\n        return \"F\"\n\n# Add the letter grade column\ngrades[\"Letter\"] = grades[\"Final score\"].apply(letter_grade)\n\ngrades.head()"
 },
 {
  "nbid": "NB_2249795",
  "file": "NB_2249795,df-1.ipynb",
  "row_id": 17,
  "target_code": "df[\"user_id\"         ] = df.user.astype('category'          ).cat.codes.values\ndf[\"movie_id\"          ] = df.title.astype('category'          ).cat.codes.values\ndf = df[['title'       ,'user'      ,'rating'        ,'user_id'         ,'movie_id'          ]]\ndf = df.reset_index()\nprint(len(df))\ndf.tail()",
  "generation": "user_unique = df[\"user\"].unique()\nuser_to_index = {}\nfor i, user_id in enumerate(user_unique):\n    user_to_index[user_id] = i\nmovie_unique = df[\"title\"].unique()\nmovie_to_index = {}\nfor i, movie_id in enumerate(movie_unique):\n    movie_to_index[movie_id] = i\ndf[\"user_id\"] = df[\"user\"].apply(lambda x: user_to_index[x])\ndf[\"movie_id\"] = df[\"title\"].apply(lambda x: movie_to_index[x])\ndf = df[['title', 'user', 'rating', 'user_id', 'movie_id']]\ndf.tail()"
 },
 {
  "nbid": "NB_2249795",
  "file": "NB_2249795,df-1.ipynb",
  "row_id": 21,
  "target_code": "df = pd.merge(df, users_nb, on='user_id'         )\ndf.tail()",
  "generation": "df['nb_lines'] = df.groupby('user')['user'].transform('count')\ndf.tail()"
 },
 {
  "nbid": "NB_2261046",
  "file": "NB_2261046,data-1.ipynb",
  "row_id": 43,
  "target_code": "d = {'no'     : False, 'yes'       : True}\ndata['international plan'                    ] = data['international plan'                    ].map(d)\ndata.head()",
  "generation": "data['international plan'] = data['international plan'].map({'yes': True, 'no': False})\ndata.head(10)"
 },
 {
  "nbid": "NB_2261046",
  "file": "NB_2261046,data-1.ipynb",
  "row_id": 45,
  "target_code": "d = {'no'     : False, 'yes'       : True}\ndata = data.replace({'voice mail plan'                 : 'd'    })\ndata.head()",
  "generation": "d = {'no' : False, 'yes' : True}\ndata['international plan'] = data['international plan'].map(d)\ndata.replace({'voice mail plan': d}, inplace=True)\ndata"
 },
 {
  "nbid": "NB_556238",
  "file": "NB_556238,movie_genre-1.ipynb",
  "row_id": 61,
  "target_code": "movie_genre['ranking'         ] = pd.cut(movie_genre['popularity'            ], bin_cuts, labels=bin_names)\nmovie_genre.head()",
  "generation": "movie_genre['ranking'] = pd.cut(movie_genre['popularity'], bins=bin_cuts, labels=bin_names)\nmovie_genre.head()"
 },
 {
  "nbid": "NB_595833",
  "file": "NB_595833,data-1.ipynb",
  "row_id": 22,
  "target_code": "data.columns = ['year_quarter'              , 'unemployment_rate'                    ]\ndata['unemployment_rate'                   ] = data['unemployment_rate'                   ].map(lambda x: float(str(x).replace('%'   ,''  )))\ndata.dropna(inplace=True)\ndata.head()",
  "generation": "data['year_quarter'] = data['Quarter']\ndata['unemployment_rate'] = data['Seasonally adjusted quarterly U.S. unemployment rates from 1948 to 1993']\ndata = data.drop(columns=['Quarter', 'Seasonally adjusted quarterly U.S. unemployment rates from 1948 to 1993'])\ndata.head()"
 },
 {
  "nbid": "NB_595833",
  "file": "NB_595833,data-1.ipynb",
  "row_id": 26,
  "target_code": "data['date'      ] = pd.to_datetime(data.year_quarter).dt.to_period('Q'   )\ndata.set_index('date'      , inplace=True)\ndata.head()",
  "generation": "result = seasonal_decompose(data['unemployment_rate'], model='multiplicative', freq=4)\nresult.plot()"
 },
 {
  "nbid": "NB_6657213",
  "file": "NB_6657213,df-1.ipynb",
  "row_id": 3,
  "target_code": "df = df.join(pd.get_dummies(df['day'     ]))\ndf.head()",
  "generation": "df['weekday'] = np.where(df['day'] == 'weekday', 1, 0)\ndf['weekend'] = np.where(df['day'] == 'weekend', 1, 0)\ndf.drop('day', axis=1, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_6657213",
  "file": "NB_6657213,df-1.ipynb",
  "row_id": 4,
  "target_code": "df[['not_fraud'           , 'fraud'        ]] = pd.get_dummies(df['fraud'       ])\ndf.head()",
  "generation": "df[\"weekday\"] = df[\"weekday\"].astype(int)\ndf[\"weekend\"] = df[\"weekend\"].astype(int)\ndf[\"not_fraud\"] = 1 - df[\"fraud\"]\ndf.head()"
 },
 {
  "nbid": "NB_7146756",
  "file": "NB_7146756,SMS-1.ipynb",
  "row_id": 7,
  "target_code": "mapping = {'ham'     :0, 'spam'       :1}\nSMS['NLabel'        ] = list(map(lambda x: mapping[x], SMS.Label))\nSMS.head()",
  "generation": "SMS['NLabel'] = SMS.Label.map({'ham':0, 'spam':1})\nSMS.head()"
 },
 {
  "nbid": "NB_2273859",
  "file": "NB_2273859,df-1.ipynb",
  "row_id": 19,
  "target_code": "df['genres'        ] = df['genres'        ].str.split('|'   )\ndf.head(2)",
  "generation": "df.dropna(inplace=True)\ndf['genres'] = df['genres'].apply(lambda x: x.split('|'))\ndf = df[['popularity', 'original_title', 'cast', 'director', 'runtime', 'genres', 'production_companies', 'vote_count', 'vote_average', 'release_year', 'budget_adj', 'revenue_adj']]\ndf.head(2)"
 },
 {
  "nbid": "NB_2273859",
  "file": "NB_2273859,df_gen-1.ipynb",
  "row_id": 21,
  "target_code": "df_gen=df.merge(df_gen, left_index=True, right_index = True)\ndf_gen.head(3)",
  "generation": "merged_df = pd.concat([df, df_gen], axis=1).drop('genres', axis=1).melt(id_vars=['id', 'popularity', 'original_title', 'cast', 'director', 'runtime', 'production_companies', 'vote_count', 'vote_average', 'release_year', 'budget', 'revenue'], value_name='genre').drop('variable', axis=1).dropna()\nmerged_df = merged_df.sort_values(by=['id']).reset_index(drop=True)\nmerged_df.head(5)"
 },
 {
  "nbid": "NB_2273859",
  "file": "NB_2273859,df_gen-1.ipynb",
  "row_id": 23,
  "target_code": "df_gen=df_gen.melt(id_vars=['id'    ,'popularity'            ,'original_title'                ,'cast'      ,'director'          ,'runtime'         ,'genres'        ,'production_companies'                      ,'vote_count'            ,'vote_average'              ,'release_year'              ,'budget'        ,'revenue'         ],value_name=\"genre\"       )\ndf_gen.head(3)",
  "generation": "df_gen = pd.melt(df_gen, id_vars=['id', 'popularity', 'original_title', 'cast', 'director', 'runtime', 'genres', 'production_companies', 'vote_count', 'vote_average', 'release_year', 'budget', 'revenue'], value_vars=[0, 1, 2, 3, 4], var_name='variable', value_name='genre')\ndf_gen = df_gen.drop('variable', axis=1)"
 },
 {
  "nbid": "NB_2287834",
  "file": "NB_2287834,df-1.ipynb",
  "row_id": 3,
  "target_code": "df = df.drop(['I1'    ,'I2'    ,'I3'    ,'pIPO'      ,'pH'    ,'pL'    ,'p1day'       ,'Pmid'      ],axis=1)\ndf.head()",
  "generation": "df = df.drop(columns=['Y1', 'Y2'])"
 },
 {
  "nbid": "NB_2290673",
  "file": "NB_2290673,df-1.ipynb",
  "row_id": 10,
  "target_code": "df['diagnosis'           ] = df['diagnosis'           ].map({'M'   :1,'B'   :0})\ndf.head()",
  "generation": "df = df.drop('Unnamed: 32', axis=1)\ndf['diagnosis'] = df['diagnosis'].map({'M':1,'B':0})\ndf.head()"
 },
 {
  "nbid": "NB_2299375",
  "file": "NB_2299375,ri-1.ipynb",
  "row_id": 8,
  "target_code": "ri['stop_minutes'              ] = ri['stop_duration'               ].map({'0-15 Min'          : 8, '16-30 Min'            : 23, '30+ Min'          : 45})\nri.head()",
  "generation": "ri['stop_duration'   ] = ri['stop_duration'].map({'0-15 Min':8, '16-30 Min':23, '30+ Min':45})\nri['stop_minutes'    ] = ri['stop_duration']\nri.groupby('violation_raw')['stop_minutes'].mean().round(2)"
 },
 {
  "nbid": "NB_2313300",
  "file": "NB_2313300,df-1.ipynb",
  "row_id": 2,
  "target_code": "df['Ndate'       ] = pd.to_datetime(df['Date'      ], format=' %d %m %Y'           )\ndf['month'       ] = df['Ndate'       ].dt.month\ndf.head()",
  "generation": "df['Ndate'] = pd.to_datetime(df['Date'], format='%d %m %Y')\ndf['month'] = df['Ndate'].dt.month\ndf.head()"
 },
 {
  "nbid": "NB_2318257",
  "file": "NB_2318257,df_germany-1.ipynb",
  "row_id": 7,
  "target_code": "df_germany = df_germany[df_germany.Country == \"Germany\"          ]\ndf_germany.tail(10)",
  "generation": "df_germany['Country'] = 'Germany'\ndf_germany['AverageTemperature'] = df_germany['AverageTemperature'].fillna(5.468)\ndf_germany.head(10)"
 },
 {
  "nbid": "NB_2331549",
  "file": "NB_2331549,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = pd.concat([df, dfy], axis=1)\ndf.head()",
  "generation": "df = pd.concat([df, dfy], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_2333508",
  "file": "NB_2333508,data-1.ipynb",
  "row_id": 8,
  "target_code": "data[\"newconservation\"                 ] = np.where(data[\"conservation\"              ]==\"1A\"    , \"A\"    ,np.where(data[\"conservation\"              ]==\"2A\"    , \"B\"    , \"C\"    ))\ndata = data.drop(\"conservation\"              , axis = 1)\ndata.head()",
  "generation": "data['newconservation'] = np.where(data['conservation'] == 'A', 'A', np.where(data['conservation'] == 'B', 'B', 'C'))\ndata = data[['totalprice', 'area', 'zone', 'category', 'age', 'floor', 'rooms', 'out', 'toilets', 'garage', 'elevator', 'streetcategory', 'heating', 'tras', 'newconservation']]\ndata.head()"
 },
 {
  "nbid": "NB_2336502",
  "file": "NB_2336502,df00-1.ipynb",
  "row_id": 3,
  "target_code": "df00['Ndate'       ] = pd.to_datetime(df00['Date'      ], format='%d %m %Y'          )\ndf00['month'       ] = df00['Ndate'       ].dt.month\ndf00.head()",
  "generation": "df00['Ndate'] = pd.to_datetime(df00['Date'], format='%d %m %Y')\ndf00['month'] = df00['Ndate'].dt.month\ndf00.head()"
 },
 {
  "nbid": "NB_2338161",
  "file": "NB_2338161,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.iloc[:,1:]\ndf.head(2)",
  "generation": "df = df.drop('Unnamed:_0', axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_2342978",
  "file": "NB_2342978,df-1.ipynb",
  "row_id": 15,
  "target_code": "del_col = [ 'id'     , 'imdb_id'          , 'popularity'             , 'budget_adj'             , 'revenue_adj'              , 'homepage'           , 'keywords'           , 'overview'           , 'production_companies'                       ]\ndf = df.drop(del_col, 1)\ndf.head()",
  "generation": "columns_to_keep = ['budget', 'revenue', 'original_title', 'cast', 'director', 'tagline', 'runtime', 'genres', 'release_date', 'vote_count', 'vote_average', 'release_year']\ndf = df[columns_to_keep]\ndf.drop(df[(df['budget'] == 0) | (df['revenue'] == 0)].index, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_2344515",
  "file": "NB_2344515,df-1.ipynb",
  "row_id": 5,
  "target_code": "df['status'        ]=np.where(df['surv_status'             ]==1,'survived'          ,'died'      )\ndf.head()",
  "generation": "df['status'] = np.where(df['surv_status'] == 1, 'survived', 'not survived')\ndf"
 },
 {
  "nbid": "NB_2345098",
  "file": "NB_2345098,trends-1.ipynb",
  "row_id": 7,
  "target_code": "trends['khloe'       ] = trends['khloe'       ].str.replace(\"<\"   , \"\"   )\ntrends['khloe'       ] = pd.to_numeric(trends['khloe'       ])\ntrends['kourtney'          ] = trends['kourtney'          ].str.replace(\"<\"   , \"\"   )\ntrends['kourtney'          ] = pd.to_numeric(trends['kourtney'          ])\ntrends['kendall'         ] = trends['kendall'         ].str.replace(\"<\"   , \"\"   )\ntrends['kendall'         ] = pd.to_numeric(trends['kendall'         ])\ntrends.info()\ntrends.head()",
  "generation": "# Remove \"<\" sign and convert dtype to integer\ntrends[trends.columns[1:]] = trends[trends.columns[1:]].replace('<1', 0).astype(int)\n\n# Rename columns\ntrends = trends.rename(columns={'Kim Kardashian: (Worldwide)': 'kim',\n                                'Khlo\u00e9 Kardashian: (Worldwide)': 'khloe',\n                                'Kourtney Kardashian: (Worldwide)': 'kourtney',\n                                'Kendall Jenner: (Worldwide)': 'kendall',\n                                'Kylie Jenner: (Worldwide)': 'kylie'})\n\n# Set index name\ntrends.index.name = 'month'\n\ntrends.head()"
 },
 {
  "nbid": "NB_2345400",
  "file": "NB_2345400,clustering-1.ipynb",
  "row_id": 3,
  "target_code": "clustering = pd.merge(clustering, main_df, on='name'      )\nclustering.head()",
  "generation": "clustering = pd.merge(clustering, main_df, on='name', how='left')\nclustering.head()"
 },
 {
  "nbid": "NB_2373345",
  "file": "NB_2373345,protein-1.ipynb",
  "row_id": 10,
  "target_code": "protein = protein.drop(['Unit (all except Population)'                              ], axis = 1)\nprotein.head()",
  "generation": "protein = protein.drop(['Unit (all except Population)'], axis=1)\nprotein.head()"
 },
 {
  "nbid": "NB_2376250",
  "file": "NB_2376250,exam_df-1.ipynb",
  "row_id": 13,
  "target_code": "exam_df = pd.get_dummies(exam_df, columns=['gender'        ,'lunch'       ,'test preparation course'                         ])\nexam_df.head()",
  "generation": "exam_df = pd.get_dummies(exam_df, columns=['gender', 'lunch', 'test preparation course'])\nprint(exam_df.shape)\nexam_df.head()"
 },
 {
  "nbid": "NB_2376961",
  "file": "NB_2376961,df-1.ipynb",
  "row_id": 10,
  "target_code": "state_dummies = pd.get_dummies(df['state'       ])\ndf = pd.concat([df.drop(['state'       ], axis = 1), state_dummies], axis = 1)\ndf.head()",
  "generation": "df = pd.get_dummies(df, columns=['state'])\ndf.head()"
 },
 {
  "nbid": "NB_510964",
  "file": "NB_510964,newdf-1.ipynb",
  "row_id": 34,
  "target_code": "newdf=newdf[['Cluster'         ,'Type'      , 'Price_x'          , 'YearBuilt'            ]]\nnewdf.head()",
  "generation": "table2 = pd.pivot_table(newdf, index=['Cluster', 'Type'],values=['Price_x', 'YearBuilt'],aggfunc={'Price_x': 'mean', 'YearBuilt': 'mean'})\ntable2.reset_index(inplace=True)\ntable2"
 },
 {
  "nbid": "NB_521827",
  "file": "NB_521827,cars_data-1.ipynb",
  "row_id": 4,
  "target_code": "cars_data=cars_data.join(cars_data['CarName'         ].str.split(' '   ,1,expand=True).rename(columns={0:'Company'         ,1:'CarModel'          }))\ncars_data.head() ",
  "generation": "cars_data['Company'] = cars_data['CarName'].apply(lambda x : x.split(' ')[0])\ncars_data['CarModel'] = cars_data['CarName'].apply(lambda x : ' '.join(x.split(' ')[1:]))\ncars_data.head()"
 },
 {
  "nbid": "NB_526233",
  "file": "NB_526233,dfEstCamposCorrigido-1.ipynb",
  "row_id": 21,
  "target_code": "dfEstCamposCorrigido['DataVerificacao'                 ] = pd.DataFrame(data=pd.date_range('1/1/1961'          , periods=(len(dfEstCamposCorrigido.index)-10), freq='D'   ))\ndfEstCamposCorrigido['DataConvertida'                ] = pd.to_datetime(dfEstCamposCorrigido['Data'      ], format='%d/%m/%Y'          )\ndfEstCamposCorrigido.head()",
  "generation": "dfEstCamposCorrigido['DataVerificacao'] = pd.to_datetime(dfEstCamposCorrigido['Data'], format='%d/%m/%Y')\ndfEstCamposCorrigido['DataConvertida'] = pd.to_datetime(dfEstCamposCorrigido['Data'], format='%d/%m/%Y')\ndfEstCamposCorrigido = dfEstCamposCorrigido[['Estacao', 'Data', 'Hora', 'TempMaxima', 'Insolacao', 'Evaporacao Piche', 'Temp Comp Media', 'Umidade Relativa Media', 'Velocidade do Vento Media', 'TempMinima', 'Precipitacao', 'DataVerificacao', 'DataConvertida']]\ndfEstCamposCorrigido.head()"
 },
 {
  "nbid": "NB_526966",
  "file": "NB_526966,hep-1.ipynb",
  "row_id": 7,
  "target_code": "hep['hurdles'         ] = 110. / hep['hurdles'         ]\nhep['run200m'         ] = 200. / hep['run200m'         ]\nhep['run800m'         ] = 800. / hep['run800m'         ]\nhep.head()",
  "generation": "hep['run200m'] = 100 / hep['run200m']\nhep['run800m'] = 100 / hep['run800m']\nhep.head()"
 },
 {
  "nbid": "NB_6478006",
  "file": "NB_6478006,df_in-1.ipynb",
  "row_id": 3,
  "target_code": "from sklearn.preprocessing import LabelEncoder \nSex_le = LabelEncoder()\ndf_in['Sex_le'        ] = Sex_le.fit_transform(df_in.Sex)\ndf_in = df_in.drop('Sex'     , axis='columns'         )\ndf_in.head()",
  "generation": "le = LabelEncoder()\ndf_in['Sex_le'] = le.fit_transform(df_in['Sex'])\ndf_in['Pclass'] = df_in['Pclass'].astype(float)\ndf_in['Age'] = df_in['Age'].astype(float)\ndf_in['Parch'] = df_in['Parch'].astype(float)\ndf_in['Fare'] = df_in['Fare'].astype(float)\ndf_out = df_in[['Pclass', 'Age', 'Parch', 'Fare', 'Sex_le']]\ndf_out.head()"
 },
 {
  "nbid": "NB_2396054",
  "file": "NB_2396054,df-1.ipynb",
  "row_id": 9,
  "target_code": "df=df.reset_index()\ndf.head()",
  "generation": "df = pd.read_csv('Step3_output.csv')\ndf = df.drop(['Name','state'], axis=1)\ndf = pd.get_dummies(df, columns=['Region'])\nscaler = preprocessing.StandardScaler()\nX = scaler.fit_transform(df.drop('AdultWeekend', axis=1))\ny = df.AdultWeekend.values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nprint('MAE:', mean_absolute_error(y_test, y_pred))"
 },
 {
  "nbid": "NB_2396340",
  "file": "NB_2396340,county_pivot-1.ipynb",
  "row_id": 3,
  "target_code": "county_pivot.columns = county_pivot.iloc[0]\ncounty_pivot.head()",
  "generation": "county_pivot.columns = county_pivot.iloc[0]\ncounty_pivot = county_pivot[1:]\ncounty_pivot.head()"
 },
 {
  "nbid": "NB_2396340",
  "file": "NB_2396340,county_pivot-1.ipynb",
  "row_id": 4,
  "target_code": "county_pivot = county_pivot.rename(columns={\"RegionName\"            :\"Period\"        })\ncounty_pivot.head()",
  "generation": "county_pivot = county_pivot.rename(columns={'index': 'Period'})\ncounty_pivot.head()"
 },
 {
  "nbid": "NB_2396340",
  "file": "NB_2396340,county_pivot-1.ipynb",
  "row_id": 5,
  "target_code": "county_pivot = county_pivot.drop(0)\ncounty_pivot = county_pivot.drop(1)\ncounty_pivot.head()",
  "generation": "county_pivot = county_pivot.drop(0).reset_index(drop=True)"
 },
 {
  "nbid": "NB_2396340",
  "file": "NB_2396340,county_pivot-1.ipynb",
  "row_id": 6,
  "target_code": "county_pivot = county_pivot.set_index(\"Period\"        )\ncounty_pivot.head()",
  "generation": "county_pivot = metro_df.transpose().reset_index()\ncounty_pivot.columns = county_pivot.iloc[0]\ncounty_pivot = county_pivot.rename(columns={\"RegionName\":\"Period\"})\ncounty_pivot = county_pivot.drop(0)\ncounty_pivot = county_pivot.drop(1)\ncounty_pivot = county_pivot.set_index('Period')\ncounty_pivot = county_pivot.apply(pd.to_numeric)\ncounty_pivot.head()"
 },
 {
  "nbid": "NB_2396340",
  "file": "NB_2396340,county_pivot-1.ipynb",
  "row_id": 8,
  "target_code": "county_pivot['Fulton County'               ] = pd.to_numeric(county_pivot['Fulton County'               ])\ncounty_pivot['Douglas County'                ] = pd.to_numeric(county_pivot['Douglas County'                ])\ncounty_pivot['Cobb County'             ] = pd.to_numeric(county_pivot['Cobb County'             ])\ncounty_pivot['Cherokee County'                 ] = pd.to_numeric(county_pivot['Cherokee County'                 ])\ncounty_pivot['Forsyth County'                ] = pd.to_numeric(county_pivot['Forsyth County'                ])\ncounty_pivot['Gwinnett County'                 ] = pd.to_numeric(county_pivot['Gwinnett County'                 ])\ncounty_pivot['Dekalb County'               ] = pd.to_numeric(county_pivot['Dekalb County'               ])\ncounty_pivot['Clayton County'                ] = pd.to_numeric(county_pivot['Clayton County'                ])\ncounty_pivot['Fayette County'                ] = pd.to_numeric(county_pivot['Fayette County'                ])\ncounty_pivot['Coweta County'               ] = pd.to_numeric(county_pivot['Coweta County'               ])\ncounty_pivot.head()",
  "generation": "No target code is needed as the input and output dataframes are already the same."
 },
 {
  "nbid": "NB_2410646",
  "file": "NB_2410646,coverage-1.ipynb",
  "row_id": 5,
  "target_code": "coverage['lines'       ] = coverage.LINE_MISSED + coverage.LINE_COVERED\ncoverage.head()",
  "generation": "coverage['lines'] = coverage['LINE_MISSED'] + coverage['LINE_COVERED']\ncoverage.head(5)"
 },
 {
  "nbid": "NB_2410646",
  "file": "NB_2410646,coverage-1.ipynb",
  "row_id": 6,
  "target_code": "coverage['covered'         ] = coverage.LINE_COVERED / coverage.lines\ncoverage.head()",
  "generation": "coverage['covered'] = coverage.LINE_COVERED / (coverage.LINE_MISSED + coverage.LINE_COVERED)\ncoverage.head()"
 },
 {
  "nbid": "NB_2419619",
  "file": "NB_2419619,df-1.ipynb",
  "row_id": 18,
  "target_code": "df = df[[\"days_since_previous_inspection\"                                , \"num_critical\"               , \"num_non_critical\"                   , \"num_critical_previous\"                        , \"num_non_critical_previous\"                            ]]\ndf.head()",
  "generation": "df = pd.read_csv('inspections.csv')\ndf = df[['days_since_last_inspection', 'num_critical', 'num_non_critical', 'num_critical_previous', 'num_non_critical_previous']]\ndf.head()"
 },
 {
  "nbid": "NB_2450568",
  "file": "NB_2450568,data-1.ipynb",
  "row_id": 9,
  "target_code": "data['CTR'     ] = data.Clicks/data.Impressions \ndata.head(10)",
  "generation": "data['CTR'] = data['Clicks'] / data['Impressions']\ndata.head()"
 },
 {
  "nbid": "NB_2470493",
  "file": "NB_2470493,df2-1.ipynb",
  "row_id": 8,
  "target_code": "col = {\"Sex \"      : \"Sex\"      ,\"Fatal (Y/N)\"             : \"Fatal\"        ,\"Species \"          : \"Species\"          }\ndf2 = df2.rename(columns = col)\ndf2.head()",
  "generation": "df2.columns = ['Year', 'Type', 'Country', 'Area', 'Sex', 'Age', 'Injury', 'Fatal', 'Species']\ndf2.head()"
 },
 {
  "nbid": "NB_2480221",
  "file": "NB_2480221,users-1.ipynb",
  "row_id": 2,
  "target_code": "users[\"sum\"     ] = users[\"variable1\"           ] + users[\"variable2\"           ]\nusers.head()",
  "generation": "users['sum'] = users['variable1'] + users['variable2']\nusers.head()"
 },
 {
  "nbid": "NB_486117",
  "file": "NB_486117,data-1.ipynb",
  "row_id": 5,
  "target_code": "data = data.drop(['Surname'         ,'RowNumber'           ,'CustomerId'            ], axis=1)\ndata.head()",
  "generation": "columns=['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited']\ndf=df[columns]\ndf.head()"
 },
 {
  "nbid": "NB_489193",
  "file": "NB_489193,df-1.ipynb",
  "row_id": 33,
  "target_code": "df = df.drop(columns=['Minute'        ])\ndf.head()",
  "generation": ""
 },
 {
  "nbid": "NB_489193",
  "file": "NB_489193,df-1.ipynb",
  "row_id": 37,
  "target_code": "df = df.drop(columns = ['rainfall_lr'             , 'snowfall_lr'              ])\ndf.head()",
  "generation": ""
 },
 {
  "nbid": "NB_495112",
  "file": "NB_495112,df-1.ipynb",
  "row_id": 3,
  "target_code": "from sklearn.preprocessing import LabelEncoder\na_school=LabelEncoder()\na_address=LabelEncoder()\na_famsize=LabelEncoder()\na_sex=LabelEncoder()\na_Pstatus=LabelEncoder()\na_Mjob=LabelEncoder()\na_Fjob=LabelEncoder()\na_reason=LabelEncoder()\na_guardian=LabelEncoder()\na_scoolsup=LabelEncoder()\na_famsup=LabelEncoder()\na_paid=LabelEncoder()\na_activities=LabelEncoder()\na_nursery=LabelEncoder()\na_higher=LabelEncoder()\na_internet=LabelEncoder()\na_romantic=LabelEncoder()\ndf[\"school\"        ]=a_school.fit_transform(df[\"school\"        ])\ndf[\"sex\"     ]=a_sex.fit_transform(df[\"sex\"     ])\ndf[\"address\"         ]=a_address.fit_transform(df[\"address\"         ])\ndf[\"famsize\"         ]=a_famsize.fit_transform(df[\"famsize\"         ])\ndf[\"Pstatus\"         ]=a_Pstatus.fit_transform(df[\"Pstatus\"         ])\ndf[\"Mjob\"      ]=a_Mjob.fit_transform(df[\"Mjob\"      ])\ndf[\"Fjob\"      ]=a_Fjob.fit_transform(df[\"Fjob\"      ])\ndf[\"reason\"        ]=a_reason.fit_transform(df[\"reason\"        ])\ndf[\"guardian\"          ]=a_guardian.fit_transform(df[\"guardian\"          ])\ndf[\"schoolsup\"           ]=a_scoolsup.fit_transform(df[\"schoolsup\"           ])\ndf[\"famsup\"        ]=a_famsup.fit_transform(df[\"famsup\"        ])\ndf[\"paid\"      ]=a_paid.fit_transform(df[\"paid\"      ])\ndf[\"activities\"            ]=a_activities.fit_transform(df[\"activities\"            ])\ndf[\"nursery\"         ]=a_nursery.fit_transform(df[\"nursery\"         ])\ndf[\"higher\"        ]=a_higher.fit_transform(df[\"higher\"        ])\ndf[\"internet\"          ]=a_internet.fit_transform(df[\"internet\"          ])\ndf[\"romantic\"          ]=a_romantic.fit_transform(df[\"romantic\"          ])\ndf.head()",
  "generation": "le = LabelEncoder()\ndf = df.apply(le.fit_transform)\ndf.head()"
 },
 {
  "nbid": "NB_6285187",
  "file": "NB_6285187,df-1.ipynb",
  "row_id": 6,
  "target_code": "for exp in range(0, 8):\n    name = \"FRAP_exp_%i\"              %exp\n    df[name] = df[name].values / (1000 * 40 * 40)\ndf.head()",
  "generation": "df.iloc[:,1:] = df.iloc[:,1:] / (40**2 * 1000)\ndf.head()"
 },
 {
  "nbid": "NB_6285187",
  "file": "NB_6285187,df-1.ipynb",
  "row_id": 8,
  "target_code": "df[\"time_(s)\"          ] = df[\"time_(s)\"          ].values - 3.76\ndf.head()",
  "generation": "df['time_(s)'] = df['time_(s)'] - 3.76\ndf.head()"
 },
 {
  "nbid": "NB_6510898",
  "file": "NB_6510898,X-1.ipynb",
  "row_id": 12,
  "target_code": "from sklearn.preprocessing import LabelEncoder\nlabelEncoder_X = LabelEncoder()\nX.Sex=labelEncoder_X.fit_transform(X.Sex)\nprint ('Number of null values in Embarked:'                                    , sum(X.Embarked.isnull()))\nrow_index = X.Embarked.isnull()\nX.loc[row_index,'Embarked'          ]='S'    \nEmbarked  = pd.get_dummies(  X.Embarked , prefix='Embarked'            )\nX = X.drop(['Embarked'          ], axis=1)\nX= pd.concat([X, Embarked], axis=1)  \nX = X.drop(['Embarked_S'            ], axis=1)\nX.head()",
  "generation": "le = LabelEncoder()\nX['Sex'] = le.fit_transform(X['Sex'])\nX['Embarked'].fillna('S', inplace=True)\nX = pd.concat([X, pd.get_dummies(X['Embarked'], prefix='Embarked')], axis=1)\nX.drop(['Embarked', 'Embarked_S'], axis=1, inplace=True)\nX.head()"
 },
 {
  "nbid": "NB_6545021",
  "file": "NB_6545021,test-1.ipynb",
  "row_id": 9,
  "target_code": "test['prediction1'             ] = 0.23959427*test.father_height+0.25013358*test.mother_height+0.10030806*test.boy_dummy+0.8274299645517075\ntest['prediction2'             ] = model.predict(test.loc[:,['father_height'               ,'mother_height'               ,'boy_dummy'           ]])\ntest.head(10)",
  "generation": "test[\"prediction1\"] = model.predict(test[[\"father_height\", \"mother_height\", \"boy_dummy\"]])\ntest[\"prediction2\"] = test[[\"father_height\", \"mother_height\", \"boy_dummy\"]].apply(lambda x: model.predict([x])[0], axis=1)\ntest.head()"
 },
 {
  "nbid": "NB_6568129",
  "file": "NB_6568129,data-1.ipynb",
  "row_id": 5,
  "target_code": "data=data[['label'       , 'text'       ]]\ndata.head()",
  "generation": "label = data['label']\nemail_text = data['text']"
 },
 {
  "nbid": "NB_6568129",
  "file": "NB_6568129,data-1.ipynb",
  "row_id": 8,
  "target_code": "data['email_body'            ]=email_body\ndata=data[['label'       , 'email_body'             ]]\ndata.head()",
  "generation": "vectorizer = TfidfVectorizer(stop_words='english')\ndata['email_body'] = data['text']\ndata = data.drop(['text'], axis=1)\ndata.head()"
 },
 {
  "nbid": "NB_6569692",
  "file": "NB_6569692,tips-1.ipynb",
  "row_id": 46,
  "target_code": "gender = {'Male'      : 0, 'Female'         : 1}\ntips['isFemale'          ] = tips['sex'     ].map(gender)\ntips.head()",
  "generation": "tips['isFemale'] = tips['sex'].map({'Male':0, 'Female':1})\ntips.head()"
 },
 {
  "nbid": "NB_2503505",
  "file": "NB_2503505,data-1.ipynb",
  "row_id": 2,
  "target_code": "data=pd.get_dummies(data)\ndata.head()",
  "generation": "data['airport_YES'] = pd.get_dummies(data['airport'], drop_first=True)\ndata['waterbody'] = data['waterbody'].map({'Lake':1, 'River':2, 'Lake and River':3, 'None':0})\nwaterbody_dummies = pd.get_dummies(data['waterbody'], prefix='waterbody')\ndata = pd.concat([data, waterbody_dummies], axis=1)\ndata.drop(['waterbody'], axis=1, inplace=True)\ndata.head()"
 },
 {
  "nbid": "NB_2511128",
  "file": "NB_2511128,df_forest_area-1.ipynb",
  "row_id": 22,
  "target_code": "df_forest_area= df_forest_area.rename(columns= lambda x: x+'_FOR'      )\ndf_forest_area.head(1)",
  "generation": "df_forest_area.columns = [col_name + '_FOR' for col_name in df_forest_area.columns]\ndf_forest_area.head(1)"
 },
 {
  "nbid": "NB_2511128",
  "file": "NB_2511128,df_income-1.ipynb",
  "row_id": 19,
  "target_code": "df_income = df_income.rename(columns = lambda x: x+'_INC'      )\ndf_income.head(1)",
  "generation": "df_income.columns = ['1996_INC', '2001_INC', '2006_INC', '2011_INC', '2016_INC']\ndf_income.head(1)"
 },
 {
  "nbid": "NB_2511128",
  "file": "NB_2511128,df_population_growth-1.ipynb",
  "row_id": 21,
  "target_code": "df_population_growth = df_population_growth.rename(columns = lambda x: x+'_POP'      )\ndf_population_growth.head(1)",
  "generation": "df_population_growth.columns = ['1996_POP','2001_POP','2006_POP','2011_POP','2016_POP']\ndf_population_growth.head(1)"
 },
 {
  "nbid": "NB_2511128",
  "file": "NB_2511128,df_urban_population-1.ipynb",
  "row_id": 20,
  "target_code": "df_urban_population = df_urban_population.rename(columns = lambda x: x+'_URB'      )\ndf_urban_population.head(1)",
  "generation": "df_urban_population.columns = [col + '_URB' for col in df_urban_population.columns]\ndf_urban_population.head()"
 },
 {
  "nbid": "NB_2511658",
  "file": "NB_2511658,dataFrame-1.ipynb",
  "row_id": 27,
  "target_code": "dataFrame = dataFrame.replace({'Voice mail plan'                 : d})\ndataFrame.head()",
  "generation": "d = {'No' : False, 'Yes' : True}\ndataFrame['International plan'] = dataFrame['International plan'].map(d)\ndataFrame.head()"
 },
 {
  "nbid": "NB_2511658",
  "file": "NB_2511658,dataFrame-1.ipynb",
  "row_id": 34,
  "target_code": "dataFrame['Total charge'              ] = dataFrame['Total day charge'                  ] + dataFrame['Total eve charge'                  ] +                            dataFrame['Total night charge'                    ] + dataFrame['Total intl charge'                   ]\ndataFrame.head()",
  "generation": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (8, 6)\ndataFrame = pd.read_csv('../../data/telecom_churn.csv')\ndataFrame.head()\ndataFrame.Churn = dataFrame.Churn.astype('int64')\nd = {'No' : False, 'Yes' : True}\ndataFrame['International plan'] = dataFrame['International plan'].map(d)\ndataFrame.head()\ndataFrame = dataFrame.replace({'Voice mail plan': d})\ndataFrame['Total calls'] = dataFrame['Total day calls'] + dataFrame['Total eve calls'] + dataFrame['Total night calls']\ndataFrame['Total charge'] = dataFrame['Total day charge'] + dataFrame['Total eve charge'] + dataFrame['Total night charge']\ndataFrame.head()"
 },
 {
  "nbid": "NB_2512552",
  "file": "NB_2512552,covid_continents-1.ipynb",
  "row_id": 3,
  "target_code": "covid_continents = covid_continents[['Entity'        , 'Date'       , 'Total_confirmed_cases'                        ]]\ncovid_continents = covid_continents.rename(columns = {'Entity'        : 'Continent'            })\ncovid_continents.set_index('Continent'           , inplace=True, drop=True)\ncovid_continents[\"Log_total_confirmed_cases\"                           ] = np.log(covid_continents.Total_confirmed_cases)\ncovid_continents[\"Date\"      ] = pd.to_datetime(covid_continents[\"Date\"      ])\nstart_date = pd.Timestamp(\"2019-12-31\"            )\ncovid_continents[\"Daysafter\"           ] = (covid_continents[\"Date\"      ] - start_date).dt.days\ncovid_continents.head()",
  "generation": "covid_continents = covid_continents[[\"Entity\", \"Date\", \"Total_confirmed_cases\"]]\ncovid_continents = covid_continents.rename(columns={\"Entity\": \"Continent\"})\ncovid_continents = covid_continents.set_index(\"Continent\")\ncovid_continents[\"Log_total_confirmed_cases\"] = np.log(covid_continents[\"Total_confirmed_cases\"])\ncovid_continents[\"Daysafter\"] = (pd.to_datetime(covid_continents[\"Date\"]) - pd.to_datetime(\"2019-12-31\")).dt.days\ncovid_continents = covid_continents[[\"Date\", \"Total_confirmed_cases\", \"Log_total_confirmed_cases\", \"Daysafter\"]]\ncovid_continents.head()"
 },
 {
  "nbid": "NB_2518056",
  "file": "NB_2518056,survey-1.ipynb",
  "row_id": 3,
  "target_code": "cut_score = {'Fair'      : 1, 'Good'       : 2, 'Very Good'            : 3, 'Premium'          : 4, 'Ideal'        : 5}\ncolor_score = {'J'   : 1, 'I'    : 2, 'H'    : 3, 'G'    : 4, 'F'    : 5, 'E'    : 6, 'D'    : 7}\nclarity_score = {'I1'    : 1, 'SI2'      : 2, 'SI1'      : 3, 'VS2'      : 4, 'VS1'      : 5, 'VVS2'       : 6, 'VVS1'       : 7, 'IF'     : 8}\nsurvey['cut_score'           ] = survey['cut'     ].replace(cut_score)\nsurvey['color_score'             ] = survey['color'       ].replace(color_score)\nsurvey['clarity_score'               ] = survey['clarity'         ].replace(clarity_score)\nsurvey.head()",
  "generation": "cut_dict = {'Fair':1, 'Good':2, 'Very Good':3, 'Premium':4, 'Ideal':5}\ncolor_dict = {'D':7, 'E':6, 'F':5, 'G':4, 'H':3, 'I':2, 'J':1}\nclarity_dict = {'I1':1, 'SI1':2, 'SI2':3, 'VS1':4, 'VS2':5, 'VVS1':6, 'VVS2':7, 'IF':8}\n\nsurvey['cut_score'] = survey['cut'].map(cut_dict)\nsurvey['color_score'] = survey['color'].map(color_dict)\nsurvey['clarity_score'] = survey['clarity'].map(clarity_dict)\n\nsurvey.drop(['cut', 'color', 'clarity'], inplace=True, axis=1)\n\nsurvey.head()"
 },
 {
  "nbid": "NB_2526668",
  "file": "NB_2526668,df-1.ipynb",
  "row_id": 18,
  "target_code": "df['profit'        ]=df['revenue'         ]-df['budget'        ]\ndf.head()",
  "generation": "df = df[['budget', 'popularity', 'revenue', 'original_title', 'cast', 'director', 'runtime', 'genres', 'vote_count', 'vote_average', 'release_year', 'budget_adj', 'revenue_adj']]\ndf.head()"
 },
 {
  "nbid": "NB_2540001",
  "file": "NB_2540001,df-1.ipynb",
  "row_id": 2,
  "target_code": "df = df.drop(df.columns[0], axis = 'columns'          )\ndf.head()",
  "generation": "TARGET_CODE:\n```df = df.T\ndf.head()```"
 },
 {
  "nbid": "NB_2546438",
  "file": "NB_2546438,df-1.ipynb",
  "row_id": 6,
  "target_code": "print('Jobs:\\n'         , df['job'     ].unique())\ndf['job'     ] =df['job'     ].fillna(\"unknown\"         )\nprint('Jobs:\\n'         , df['job'     ].unique())\nprint('Marital:\\n'            , df['marital'         ].unique())\ndf['marital'         ] =df['marital'         ].fillna(\"unknown\"         )\nprint('Marital:\\n'            , df['marital'         ].unique())\nprint('Education:\\n'              , df['education'           ].unique())\ndf['education'           ] =df['education'           ].fillna(\"unknown\"         )\nprint('Education:\\n'              , df['education'           ].unique())\nprint('Default:\\n'            , df['default'         ].unique())\nprint('Housing:\\n'            , df['housing'         ].unique())\nprint('Loan:\\n'         , df['loan'      ].unique())\nprint('Contact:\\n'            , df['contact'         ].unique())\ndf['contact'         ] =df['contact'         ].fillna(\"unknown\"         )\nprint('Contact:\\n'            , df['contact'         ].unique())\nprint('Poutcome:\\n'             , df['poutcome'          ].unique())\nprint('Credit History:\\n'                   , df['credit.history'                ].unique())\ndf['credit.history'                ] = df['credit.history'                ].str.replace('verygood'          ,'very good'           )\ndf['credit.history'                ] = df['credit.history'                ].str.replace('Poor'      ,'poor'      )\ndf['credit.history'                ] =df['credit.history'                ].fillna(\"unknown\"         )\nprint('Credit History:\\n'                   , df['credit.history'                ].unique())\nimp = SimpleImputer ()\ndf['age'     ]=imp.fit_transform(df[['age'     ]])\ndf['credit.score'              ]=imp.fit_transform(df[['credit.score'              ]])\ndf['work.exp'          ]=imp.fit_transform(df[['work.exp'          ]])\ndf.loc[df[\"month\"       ] == \"jan\"      , \"month_int\"            ] = 1\ndf.loc[df[\"month\"       ] == \"feb\"      , \"month_int\"            ] = 2\ndf.loc[df[\"month\"       ] == \"mar\"      , \"month_int\"            ] = 3\ndf.loc[df[\"month\"       ] == \"apr\"      , \"month_int\"            ] = 4\ndf.loc[df[\"month\"       ] == \"may\"      , \"month_int\"            ] = 5\ndf.loc[df[\"month\"       ] == \"jun\"      , \"month_int\"            ] = 6\ndf.loc[df[\"month\"       ] == \"jul\"      , \"month_int\"            ] = 7\ndf.loc[df[\"month\"       ] == \"aug\"      , \"month_int\"            ] = 8\ndf.loc[df[\"month\"       ] == \"sep\"      , \"month_int\"            ] = 9\ndf.loc[df[\"month\"       ] == \"oct\"      , \"month_int\"            ] = 10\ndf.loc[df[\"month\"       ] == \"nov\"      , \"month_int\"            ] = 11\ndf.loc[df[\"month\"       ] == \"dec\"      , \"month_int\"            ] = 12\ndf = df.drop(['month'       ],axis=1)\ndf = df.drop(['contact'         ],axis=1)\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore'        )\ndf = pd.get_dummies(df, prefix_sep='_'   , drop_first=True)\ndf.head()",
  "generation": "#filling the missing values with mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\ndf[['Client Id', 'age', 'dependents', 'work.exp', 'balance', 'credit.score', 'day', 'duration', 'campaign', 'pdays', 'previous']] = imputer.fit_transform(df[['Client Id', 'age', 'dependents', 'work.exp', 'balance', 'credit.score', 'day', 'duration', 'campaign', 'pdays', 'previous']])\n#one hot encoding\nenc = OneHotEncoder(handle_unknown='ignore')\nenc_df = pd.DataFrame(enc.fit_transform(df[['job', 'marital', 'education', 'default', 'housing', 'loan', 'credit.history', 'poutcome']]).toarray())\ndf = df.join(enc_df)\ndf = df.drop(['job', 'marital', 'education', 'default', 'housing', 'loan', 'credit.history', 'poutcome'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_2553858",
  "file": "NB_2553858,stock_df-1.ipynb",
  "row_id": 17,
  "target_code": "stock_df['year'      ] = pd.DatetimeIndex(stock_df['date'      ]).year\nstock_df.tail()",
  "generation": "stock_df['year'] = stock_df['date'].dt.year\nstock_df.head()"
 },
 {
  "nbid": "NB_2561242",
  "file": "NB_2561242,df-1.ipynb",
  "row_id": 7,
  "target_code": "df['Churn'       ] = df['Churn'       ].astype('int64'       )\ndf.head()",
  "generation": "df['Churn'] = df['Churn'].astype('int64')\ndf.head()"
 },
 {
  "nbid": "NB_2561242",
  "file": "NB_2561242,df-1.ipynb",
  "row_id": 40,
  "target_code": "d = {'No'     : False, 'Yes'       : True} \ndf['International plan'                    ] = df['International plan'                    ].map(d)\ndf.head()",
  "generation": "df['International plan'] = df['International plan'].map({'Yes': True, 'No': False})"
 },
 {
  "nbid": "NB_2565275",
  "file": "NB_2565275,df-1.ipynb",
  "row_id": 4,
  "target_code": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df['win'     ].unique())\ndf['win'     ] = le.transform(df['win'     ])\ndf.head()",
  "generation": "le = LabelEncoder()\ndf['win'] = le.fit_transform(df['win'].astype(str))\ndf.head()"
 },
 {
  "nbid": "NB_2571451",
  "file": "NB_2571451,df-1.ipynb",
  "row_id": 12,
  "target_code": "df.columns = [x.lower() for x in df.columns]\ndf.head(0)",
  "generation": "df.rename(columns=lambda x: x.lower().replace('-', '_'), inplace=True)\ndf.rename(columns={'no-show': 'no_show'}, inplace=True)\ndf['scheduledday'  ] = pd.to_datetime(df['scheduledday'  ])\ndf['appointmentday'] = pd.to_datetime(df['appointmentday'])\ndf.drop('patientid', axis=1, inplace=True)\nprint(df.head())"
 },
 {
  "nbid": "NB_2571451",
  "file": "NB_2571451,df-1.ipynb",
  "row_id": 15,
  "target_code": "df['attended'          ] = df['attended'          ].replace(to_replace = \"Yes\"      , value = 0)\ndf.head(10)",
  "generation": "df = df.rename(columns={'patientid': 'patient_id', 'appointmentid': 'appointment_id', 'hipertension': 'hypertension', 'handcap': 'handicap', 'no-show': 'attended'})\ndf = df.drop(['diabetes', 'alcoholism'], axis=1)\ndf['scheduled_day'] = pd.to_datetime(df['scheduled_day']).dt.date\ndf['appointment_day'] = pd.to_datetime(df['appointment_day']).dt.date\ndf['attended'] = df['attended'].replace({'Yes': '1', 'No': '0'})\ndf = df.rename(columns={'attended': 'attended'})\ndf.head()"
 },
 {
  "nbid": "NB_2571451",
  "file": "NB_2571451,df-1.ipynb",
  "row_id": 16,
  "target_code": "df['attended'          ] = df['attended'          ].replace(to_replace = \"No\"     , value = 1)\ndf.head(10)",
  "generation": "df['attended'] = df['attended'].replace(to_replace = \"No\", value = 1)\ndf.head()"
 },
 {
  "nbid": "NB_2571451",
  "file": "NB_2571451,df-1.ipynb",
  "row_id": 19,
  "target_code": "df['appointment_day'                 ] = df['appointment_day'                 ].str[:-10]\ndf.head()",
  "generation": "df['appointment_day'] = df['appointment_day'].str[:10]\ndf.head()"
 },
 {
  "nbid": "NB_2587194",
  "file": "NB_2587194,df-1.ipynb",
  "row_id": 19,
  "target_code": "df['title'       ] = df['name'      ].str.extract('(,\\s[\\w\\s]+\\.)'                , expand=False).apply(lambda x: x.split(', '    )[1])\ndf.head()",
  "generation": "df['title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf['age_group'] = pd.cut(df['Age'], bins=[0, 12, 18, 65, 200], labels=['child', 'teenager', 'adult', 'elderly'])\ntitle_age_map = {title: df['Age'][(df['title'] == title)].median() for title in df['title'].unique()}\ndf['age'] = df.apply(lambda row: title_age_map[row['title']] if np.isnan(row['Age']) else row['Age'], axis=1)\ndf.drop(['Name', 'Age'], axis=1, inplace=True)\ndf = pd.get_dummies(df, columns=['title', 'age_group', 'Sex', 'Embarked'], drop_first=True)\ndf.head()"
 },
 {
  "nbid": "NB_2587194",
  "file": "NB_2587194,df-1.ipynb",
  "row_id": 23,
  "target_code": "df['age'     ] = df['age'     ].fillna(df['title'       ].map(title_age_map))\ndf.head()",
  "generation": "age_median_map = df.groupby('title')['age'].median().to_dict()\ndf['age'] = df.apply(lambda row: age_median_map[row['title']] if np.isnan(row['age']) else row['age'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_38616",
  "file": "NB_38616,df_train-1.ipynb",
  "row_id": 55,
  "target_code": "df_train= df_train.rename(columns={'Date'      : 'ds'     , 'Total'        : 'y'    })\ndf_train.tail()",
  "generation": "There is no target code provided for this task. Please provide the target code."
 },
 {
  "nbid": "NB_423011",
  "file": "NB_423011,data-1.ipynb",
  "row_id": 12,
  "target_code": "data = data[data.TICKER != 'GEF'      ]\ndata = data[data.TYPE != 'Intraday'           ]\ndrop_cols = ['SPLIT_RATIO'             , 'EX_DIVIDEND'              , 'ADJ_FACTOR'             , 'ADJ_VOLUME'             , 'ADJ_CLOSE'            , 'ADJ_LOW'          , \n             'ADJ_HIGH'                       , 'ADJ_OPEN'           , 'VOLUME'         , 'FREQUENCY'            , 'TYPE'       , 'FIGI'       ]\ndata.drop(drop_cols, axis=1, inplace=True)\ndata.head()",
  "generation": "data = data[data.TYPE == 'EOD']\ndata = data[data.TICKER != 'GEF']\ndata = data.drop('FIGI', axis=1)\ndata = data.drop('TYPE', axis=1)\ndata = data.drop('FREQUENCY', axis=1)\ndata = data.drop('VOLUME', axis=1)\ndata = data.drop('ADJ_FACTOR', axis=1)\ndata = data.drop('EX_DIVIDEND', axis=1)\ndata = data.drop('SPLIT_RATIO', axis=1)\ndata.head()"
 },
 {
  "nbid": "NB_6337019",
  "file": "NB_6337019,avengers-1.ipynb",
  "row_id": 33,
  "target_code": "avengers = avengers.drop('Name'      , axis=1)\navengers.head(3)",
  "generation": "avengers.drop('Name', axis=1, inplace=True)\navengers.head()"
 },
 {
  "nbid": "NB_6386006",
  "file": "NB_6386006,df-1.ipynb",
  "row_id": 4,
  "target_code": "df['is_italian'            ] = (df['cuisine'         ] == 'italian'          ).astype(int)\ndf.head()",
  "generation": "vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(df['ingredient_list'])\ndf['is_italian'] = df['cuisine'].apply(lambda x: 1 if x == 'italian' else 0)\ndf"
 },
 {
  "nbid": "NB_2594302",
  "file": "NB_2594302,df-1.ipynb",
  "row_id": 2,
  "target_code": "df = df.drop([\"Unnamed: 2\"            , \"Unnamed: 3\"             , \"Unnamed: 4\"             ], axis=1)\ndf.head()",
  "generation": "df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_2630583",
  "file": "NB_2630583,Pokemon-1.ipynb",
  "row_id": 4,
  "target_code": "Pokemon.columns = Pokemon.columns.str.upper().str.replace('_'   , ''   ) \nPokemon = Pokemon.set_index('NAME'      ) \nPokemon.head()\nPokemon=Pokemon.drop(['#'   ],axis=1) \nPokemon.head()",
  "generation": "Pokemon.columns = map(str.upper, Pokemon.columns)\nPokemon.set_index('NAME', inplace=True)\nPokemon.drop(['#'], axis=1, inplace=True)\nPokemon.head()"
 },
 {
  "nbid": "NB_2630583",
  "file": "NB_2630583,Pokemon-1.ipynb",
  "row_id": 6,
  "target_code": "Pokemon.index = Pokemon.index.str.replace(\".*(?=Mega)\"            , \"\"   )\nPokemon.index = Pokemon.index.str.replace(\".*(?=Alolan)\"              , \"\"   )\nPokemon.index = Pokemon.index.str.replace(\".*(?=Primal)\"              , \"\"   )\nPokemon.head()",
  "generation": "Pokemon.columns = Pokemon.columns.str.upper().str.replace('_', '') #change into upper case\nPokemon = Pokemon.set_index('NAME') #change and set the index to the name attribute\nPokemon=Pokemon.drop(['#'],axis=1) #Removes Pokemon Number\nPokemon.head()"
 },
 {
  "nbid": "NB_2645276",
  "file": "NB_2645276,design_mat-1.ipynb",
  "row_id": 11,
  "target_code": "design_mat = design_mat.drop(['target'        ,'var2'      ], axis=1)\ndesign_mat.head()",
  "generation": "y_train = df['target']\ndesign_mat = df[['var1', 'var2']].copy()\ndesign_mat['var2_squared'] = df['var2']**2\ndesign_mat['log_var1'] = df['var1'].apply(np.log)\ndesign_mat = design_mat[['var1', 'var2_squared', 'log_var1']]\ndesign_mat.head()"
 },
 {
  "nbid": "NB_2661305",
  "file": "NB_2661305,data-1.ipynb",
  "row_id": 3,
  "target_code": "data[\"passenger_survived\"                    ] = (data[\"passenger_survived\"                    ] == \"Y\"    ).astype(int)\ndata.head()",
  "generation": "data['passenger_survived'] = np.where(data['passenger_survived'] == 'Y', 1, 0)\ndata.head()"
 },
 {
  "nbid": "NB_2672022",
  "file": "NB_2672022,data_well-1.ipynb",
  "row_id": 9,
  "target_code": "data_well= data_well.loc[(data_well[\"BORE_OIL_VOL\"              ] != 0) & (data_well[\"BORE_GAS_VOL\"              ] != 0) & (data_well[\"BORE_WAT_VOL\"              ] != 0)]\ndata_well.head()",
  "generation": "data_well['DATEPRD'] = pd.to_datetime(data_well['DATEPRD'])\ndata_well['YEAR'] = data_well['DATEPRD'].dt.year\ndata_well.head()"
 },
 {
  "nbid": "NB_2672333",
  "file": "NB_2672333,movies-1.ipynb",
  "row_id": 25,
  "target_code": "movies['gross'       ] = movies['gross'       ].div(1000000)\nmovies['budget'        ] = movies['budget'        ].div(1000000)\nmovies.head()",
  "generation": "movies['budget'] = movies['budget']/1000000\nmovies['gross'] = movies['gross']/1000000\nmovies.head()"
 },
 {
  "nbid": "NB_2672333",
  "file": "NB_2672333,movies-1.ipynb",
  "row_id": 27,
  "target_code": "movies['profit'        ] = movies['gross'       ] - movies['budget'        ]\nmovies.head()",
  "generation": "movies['profit'] = movies['gross'] - movies['budget']\nmovies.head()"
 },
 {
  "nbid": "NB_2684247",
  "file": "NB_2684247,df-1.ipynb",
  "row_id": 2,
  "target_code": "col_to_drop = [\"Surname\"         , \"RowNumber\"            , \"CustomerId\"             ]\ndf = df.drop(columns = col_to_drop, axis=1)\ndf.head()",
  "generation": "df = df.drop(columns=['RowNumber', 'CustomerId', 'Surname', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary'])\ndf.head()"
 },
 {
  "nbid": "NB_2684387",
  "file": "NB_2684387,movies_df-1.ipynb",
  "row_id": 50,
  "target_code": "movies_df = movies_df.reset_index()\nmovies_df.head(2)",
  "generation": "movies_df = movies_df.reset_index()\nmovies_df = movies_df[['Title', 'Rank', 'Genre', 'Description', 'Director', 'Actors', 'Year', 'Runtime (Minutes)', 'Rating', 'Votes', 'Revenue (Millions)', 'Metascore']]\nmovies_df.head()"
 },
 {
  "nbid": "NB_2691003",
  "file": "NB_2691003,data-1.ipynb",
  "row_id": 7,
  "target_code": "data.dropna(inplace=True)\nchange_labels = lambda x: 1 if x==0 else 0\ndata['label'       ] = data['label'       ].apply(change_labels)\ndata.head()",
  "generation": "vectorizer = CountVectorizer(lowercase=True, preprocessor=remove_non_alphabets, tokenizer=tokenize, stop_words='english', min_df=2)\ndata = vectorizer.fit_transform(data['text'])\ndata = pd.DataFrame(data.toarray())"
 },
 {
  "nbid": "NB_447802",
  "file": "NB_447802,test_final-1.ipynb",
  "row_id": 91,
  "target_code": "test_final=pd.pivot_table(test_final,index='team_id'         ,values=list(test_final.iloc[:,3:].columns),aggfunc='sum'     )\ntest_final=test_final.reset_index()\ntest_final.head()",
  "generation": "test_final = pd.merge(test_team_recent, test_players, on='player_name')\nscaler.fit(test_final)\ntest_final_scaled = scaler.transform(test_final)\ntest_final_scaled_df = pd.DataFrame(test_final_scaled, columns=test_final.columns)\ntest_final_scaled_df['team_id'] = test_team_recent['team_id']\ntest_final_scaled_df['player_name'] = test_team_recent['player_name']\ntest_final_scaled_df = test_final_scaled_df.groupby('team_id').mean().reset_index()\ntest_final_scaled_df.drop(['date'], axis=1, inplace=True)\ntest_final_scaled_df.head()"
 },
 {
  "nbid": "NB_457301",
  "file": "NB_457301,dummy_survey-1.ipynb",
  "row_id": 15,
  "target_code": "from sklearn import preprocessing\nq2 = dummy_survey[['Question 2'            ]]\nmin_max_scaler = preprocessing.MinMaxScaler()\nq2 = min_max_scaler.fit_transform(q2)\ndummy_survey['Question 2'            ] = q2\nq12 = dummy_survey[['Question 12'             ]]\nmin_max_scaler = preprocessing.MinMaxScaler()\nq12 = min_max_scaler.fit_transform(q12)\ndummy_survey['Question 12'             ] = q12\ndummy_survey.head()",
  "generation": "scaler = preprocessing.MinMaxScaler()\ndummy_survey[['age', 'ranking']] = scaler.fit_transform(dummy_survey[['age', 'ranking']])\ndummy_survey.head()"
 },
 {
  "nbid": "NB_6187881",
  "file": "NB_6187881,df-1.ipynb",
  "row_id": 4,
  "target_code": "df = df[['v1'    , 'v2'     ]]\ndf.head()",
  "generation": "df = df[['v1', 'v2']]\ndf.head()"
 },
 {
  "nbid": "NB_2698189",
  "file": "NB_2698189,data-1.ipynb",
  "row_id": 21,
  "target_code": "data['company'         ] = data['company'         ].apply(lambda x: x.lower())\ndata['director'          ] = data['director'          ].apply(lambda x: x.lower())\ndata['genre'       ] = data['genre'       ].apply(lambda x: x.lower())\ndata['star'      ] = data['star'      ].apply(lambda x: x.lower())\ndata['rating'        ] = data['rating'        ].apply(lambda x: x.lower())\nmovies = pd.Series(data['name'      ].str.lower())\ndata.head()",
  "generation": "data['director'] = data['director'].apply(lambda x: x.replace(' ',''))\ndata['genre'] = data['genre'].apply(lambda x: x.replace(' ',''))\ndata['star'] = data['star'].apply(lambda x: x.replace(' ',''))\ndata['rating'] = data['rating'].apply(lambda x: x.replace(' ',''))\n\ndata['company'] = data['company'].apply(lambda x: x.lower())\ndata['director'] = data['director'].apply(lambda x: x.lower())\ndata['genre'] = data['genre'].apply(lambda x: x.lower())\ndata['star'] = data['star'].apply(lambda x: x.lower())\ndata['rating'] = data['rating'].apply(lambda x: x.lower())"
 },
 {
  "nbid": "NB_2713528",
  "file": "NB_2713528,data-1.ipynb",
  "row_id": 5,
  "target_code": "data['y'   ] = (data[\"y\"   ]==\"yes\"     ).astype(int)\ndata.head()",
  "generation": "data['y'] = np.where(data['y'] == 'yes', 1, 0)\ndata.head()"
 },
 {
  "nbid": "NB_2721230",
  "file": "NB_2721230,df-1.ipynb",
  "row_id": 13,
  "target_code": "from scipy.stats import zscore\ndf = df.apply(zscore)\ndf.head()",
  "generation": "df = car.drop('class' , axis =1)\ndf_std = df.apply(zscore)\ndf_std.head()"
 },
 {
  "nbid": "NB_2735809",
  "file": "NB_2735809,load_data_clean-1.ipynb",
  "row_id": 13,
  "target_code": "load_data_clean.term = load_data_clean.term.apply(extract_number)\nload_data_clean.head()",
  "generation": "load_data_clean['term'] = load_data_clean['term'].apply(lambda x: int(x.split()[0]))\nload_data_clean.head()"
 },
 {
  "nbid": "NB_2736969",
  "file": "NB_2736969,my_data-1.ipynb",
  "row_id": 8,
  "target_code": "delete=[ 'id'     , 'imdb_id'          , 'popularity'             , 'budget_adj'             , 'revenue_adj'              , 'homepage'           , 'keywords'           , 'overview'           , 'production_companies'                       , 'vote_count'             , 'vote_average'               ]\nmy_data = my_data.drop(delete,1)\nmy_data.head()",
  "generation": "my_data.drop(['id', 'imdb_id', 'popularity', 'budget_adj', 'revenue_adj', 'homepage', 'keywords', 'overview', 'production_companies', 'vote_count', 'vote_average'], axis=1, inplace=True)\nmy_data.drop_duplicates(inplace=True)\nmy_data.drop(my_data[(my_data.budget == 0) | (my_data.revenue == 0)].index, inplace=True)\nmy_data['release_date'] = pd.to_datetime(my_data['release_date'])\nmy_data['budget'] = my_data['budget'].apply(lambda x: '${:,.2f}'.format(x))\nmy_data['revenue'] = my_data['revenue'].apply(lambda x: '${:,.2f}'.format(x))\nmy_data['runtime'] = my_data['runtime'].replace(0, np.nan)\nmy_data = my_data[['budget', 'revenue', 'original_title', 'cast', 'director', 'tagline', 'runtime', 'genres', 'release_date', 'release_year']]\nmy_data.head()"
 },
 {
  "nbid": "NB_2743493",
  "file": "NB_2743493,df-1.ipynb",
  "row_id": 9,
  "target_code": "df['NVT'     ] = df_nvt['NVT'     ]\ndf['NVT'     ] = df['NVT'     ].fillna(method='ffill'       )\ndf.to_csv('input.csv'           )\ndf.head()",
  "generation": "scaler = MinMaxScaler()\ndf_nvt['NVT'] = scaler.fit_transform(df_nvt[['NVT']])\ndf = pd.merge(df, df_nvt[['NVT']], left_index=True, right_index=True)\ndf.head()"
 },
 {
  "nbid": "NB_2754591",
  "file": "NB_2754591,movies-1.ipynb",
  "row_id": 28,
  "target_code": "movies = movies.sort_values(['profit'        ])\nmovies.head()",
  "generation": "movies = movies.sort_values(by=['profit'], ascending=False)\ntop10 = movies.head(10)\ntop10"
 },
 {
  "nbid": "NB_2757347",
  "file": "NB_2757347,df-1.ipynb",
  "row_id": 29,
  "target_code": "df = df.replace({\"Voice mail plan\"                 : dic})\ndf.head()",
  "generation": "dic = {\"No\" : False, \"Yes\" : True}\ndf[\"International plan\"] = df[\"International plan\"].map(dic)\ndf.head()"
 },
 {
  "nbid": "NB_2758790",
  "file": "NB_2758790,data-1.ipynb",
  "row_id": 7,
  "target_code": "data.IncomeClass= data['IncomeClass'             ].map( {' <=50K'         : 0 , ' >50K'         : 1} )\ndata.head()",
  "generation": "data['IncomeClass'] = data['IncomeClass'].apply(lambda x: 0 if x == ' <=50K' else 1)\ndata.head()"
 },
 {
  "nbid": "NB_2765143",
  "file": "NB_2765143,reviews_df-1.ipynb",
  "row_id": 2,
  "target_code": "reviews_df[\"is_bad_review\"               ] = reviews_df[\"rating\"        ].apply(lambda x: 1 if x < 3 else 0)\nreviews_df = reviews_df[[\"review\"        , \"is_bad_review\"                ]]\nreviews_df['review'        ]= reviews_df['review'        ].apply(str)\nreviews_df.head(5)",
  "generation": "reviews_df['sentiment'] = reviews_df['review'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(x)['compound'])\nreviews_df = reviews_df.loc[reviews_df['sentiment'] > 0].reset_index(drop=True)\nreviews_df = reviews_df[['review', 'sentiment']]\nreviews_df.head(2)"
 },
 {
  "nbid": "NB_2765399",
  "file": "NB_2765399,df-1.ipynb",
  "row_id": 26,
  "target_code": "df['TimeSecs'          ] = df['Time'      ] / 1000\ndf.head()",
  "generation": "df['TimeSecs'] = df['Time'] / 1000\ndf.head()"
 },
 {
  "nbid": "NB_2765399",
  "file": "NB_2765399,df-1.ipynb",
  "row_id": 28,
  "target_code": "df['IsCorrect'           ] = df['Correct'         ].astype(bool)\ndf.head()",
  "generation": "df['Correct'] = df['Correct'].astype(bool)\ndf['IsCorrect'] = df['Correct']\ndf.head()"
 },
 {
  "nbid": "NB_2790098",
  "file": "NB_2790098,pdf-1.ipynb",
  "row_id": 2,
  "target_code": "print (\"Shape of dataset before cleaning: \"                                    , pdf.size)\npdf[[ 'sales'        , 'resale'         , 'type'       , 'price'        , 'engine_s'           ,\n       'horsepow'                 , 'wheelbas'           , 'width'        , 'length'         , 'curb_wgt'           , 'fuel_cap'           ,\n       'mpg'            , 'lnsales'          ]] = pdf[['sales'       , 'resale'         , 'type'       , 'price'        , 'engine_s'           ,\n       'horsepow'                 , 'wheelbas'           , 'width'        , 'length'         , 'curb_wgt'           , 'fuel_cap'           ,\n       'mpg'            , 'lnsales'          ]].apply(pd.to_numeric, errors='coerce'        )\npdf = pdf.dropna()\npdf = pdf.reset_index(drop=True)\nprint (\"Shape of dataset after cleaning: \"                                   , pdf.size)\npdf.head(5)",
  "generation": "pdf.dropna(inplace=True)\npdf.head()"
 },
 {
  "nbid": "NB_2790769",
  "file": "NB_2790769,df-1.ipynb",
  "row_id": 1,
  "target_code": "df = df.drop('Loan_ID'         , axis = 1)\ndf.head(10)",
  "generation": "df = df.drop(\"Loan_ID\", axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_344319",
  "file": "NB_344319,df-1.ipynb",
  "row_id": 8,
  "target_code": "df.Month = pd.to_datetime(df.Month)\ndf = df.set_index(\"Month\"       )\ndf.head()",
  "generation": "df = df.set_index('Month')\nresult = seasonal_decompose(df, model='multiplicative')\nresult.trend['Monthly beer production']"
 },
 {
  "nbid": "NB_360859",
  "file": "NB_360859,RPCE_M-1.ipynb",
  "row_id": 21,
  "target_code": "RPCE_M = RPCE_M.interpolate(method='linear'        )\nRPCE_M['RPCE_M_PCT_CHG'                ]= RPCE_M['RPCE_M'        ].pct_change()\nRPCE_M.head(3)\nRPCE_M.tail(3)\nRPCE_M['RPCE_Q_PCT_CHG'                ]= RPCE_M['RPCE_M'        ].pct_change(periods = 2)\nRPCE_M['RPCE_A_PCT_CHG'                ]= RPCE_M['RPCE_M'        ].pct_change(periods = 11)\nRPCE_M['RPCE_2A_PCT_CHG'                 ]= RPCE_M['RPCE_M'        ].pct_change(periods = 23)\nRPCE_M['RPCE_3A_PCT_CHG'                 ]= RPCE_M['RPCE_M'        ].pct_change(periods = 35)\nRPCE_M = RPCE_M.fillna(method='bfill'       )\nRPCE_M.describe()",
  "generation": "RPCE_M['RPCE_M_PCT_CHG'] = RPCE_M['RPCE_M'].pct_change()\nRPCE_M.head(1)"
 },
 {
  "nbid": "NB_360859",
  "file": "NB_360859,STOCK_MKT_M-1.ipynb",
  "row_id": 71,
  "target_code": "STOCK_MKT_M = STOCK_MKT_M.interpolate(method='linear'        )\nSTOCK_MKT_M.tail(3)",
  "generation": "STOCK_MKT_M = STOCK_MKT_M.fillna(method='ffill')\nSTOCK_MKT_M = STOCK_MKT_M.fillna(method='bfill')\nSTOCK_MKT_M = STOCK_MKT_M.interpolate()\nSTOCK_MKT_M.head(3)"
 },
 {
  "nbid": "NB_362814",
  "file": "NB_362814,df1-1.ipynb",
  "row_id": 7,
  "target_code": "df1 = df1.reindex(index=df1.index[::-1])\ndf1.head()",
  "generation": "df1['Date'] = pd.to_datetime(df1['Date'])\ndf1 = df1.sort_values('Date')\ndf1.head()"
 },
 {
  "nbid": "NB_467073",
  "file": "NB_467073,merged_df-1.ipynb",
  "row_id": 48,
  "target_code": "import numpy as np\ndistances = kmeansmodel.fit_transform(X)\nmerged_df['Distance'          ] = np.min(distances, axis=1)\nmerged_df.head()",
  "generation": "kmeansmodel = KMeans(n_clusters=3, random_state=13)\nX = StandardScaler().fit_transform(nutr_df)\nnutr_df['Cluster'] = kmeansmodel.labels_\nmerged_df = nutr_df.join(desc_df)\nmerged_df['Distance'] = np.sqrt(((X - kmeansmodel.cluster_centers_[nutr_df['Cluster']]) ** 2).sum(axis=1))\nmerged_df = merged_df.sort_values(['Cluster', 'Distance'])\nmerged_df.head()"
 },
 {
  "nbid": "NB_467073",
  "file": "NB_467073,nutr_df-1.ipynb",
  "row_id": 14,
  "target_code": "nutr_df.drop(['Folate_DFE_(\u00c2\u00b5g)'                  , 'Vit_A_RAE'            , 'Vit_D_IU'           ], \n        inplace=True, axis=1)\nnutr_df = nutr_df.dropna()\nnutr_df.head()",
  "generation": "df = df.drop(['Folate_DFE_(\u00b5g)', 'Vit_A_RAE', 'Vit_D_IU'], axis=1)\ndf = df.dropna()\ndf.head()"
 },
 {
  "nbid": "NB_5911682",
  "file": "NB_5911682,dfcars-1.ipynb",
  "row_id": 22,
  "target_code": "trans_map = dict(M=0, A=1)\ndfcars['am'    ] = dfcars['am'    ].apply(lambda x: trans_map[x])\ndfcars.head()",
  "generation": "dfcars['am'] = dfcars['am'].replace({'A': 1, 'M': 0})\ndfcars.head()"
 },
 {
  "nbid": "NB_5920399",
  "file": "NB_5920399,df-1.ipynb",
  "row_id": 4,
  "target_code": "df['Clean_text'            ] = np.vectorize(remove_pattern)(df.loc[:,['Text'      ]], \"@[\\w]*\"         )\ndf.head()",
  "generation": "df['Clean_text'] = df['Text']\ndf.head()"
 },
 {
  "nbid": "NB_6094608",
  "file": "NB_6094608,spam_data-1.ipynb",
  "row_id": 2,
  "target_code": "spam_data['target'        ] = np.where(spam_data['target'        ]=='spam'      ,1,0)\nspam_data.head(10)",
  "generation": "spam_data['target'] = np.where(spam_data['target']=='spam', 1, 0)\nspam_data"
 },
 {
  "nbid": "NB_2796738",
  "file": "NB_2796738,train1-1.ipynb",
  "row_id": 10,
  "target_code": "train1 = train1.fillna(train1.mean())\ntrain1.insert(0,\"RiskFlag\"          , df['RiskFlag'          ])\ntrain1.head()",
  "generation": "train1[train1.isnull()] = train1.mean()\ntrain.iloc[:,1:] = train1\ntrain.head()"
 },
 {
  "nbid": "NB_2814012",
  "file": "NB_2814012,df-1.ipynb",
  "row_id": 3,
  "target_code": "df[\"time_cst\"          ] = pd.to_datetime(df[\"time_cst\"          ])\ndf.head(5)",
  "generation": "df[\"time_cst\"] = pd.to_datetime(df[\"time_cst\"])\ndf.head()"
 },
 {
  "nbid": "NB_2816089",
  "file": "NB_2816089,trop_atmos-1.ipynb",
  "row_id": 14,
  "target_code": "trop_atmos['YYYYMMDD'          ] = pd.to_datetime(trop_atmos['YYYYMMDD'          ].astype(str), format = '%Y%m%d'         )\ntrop_atmos.head()",
  "generation": "trop_atmos['Timestamp'] = pd.to_datetime(trop_atmos['YYYYMMDD'].astype(str) + trop_atmos['HHMMSS'].astype(str), format='%Y%m%d%H%M%S')\ntrop_atmos.head()"
 },
 {
  "nbid": "NB_2816089",
  "file": "NB_2816089,trop_atmos-1.ipynb",
  "row_id": 15,
  "target_code": "trop_atmos['Month'       ]=trop_atmos['YYYYMMDD'          ].dt.month\ntrop_atmos.head()",
  "generation": "trop_atmos['YYYYMMDD'] = pd.to_datetime(trop_atmos['YYYYMMDD'].astype(str), format = '%Y%m%d')\ntrop_atmos['Month'] = trop_atmos['YYYYMMDD'].dt.month\ntrop_atmos.head()"
 },
 {
  "nbid": "NB_2817364",
  "file": "NB_2817364,data-1.ipynb",
  "row_id": 5,
  "target_code": "data['Reasons'         ] = data['title'       ].apply(lambda title: title.split(':'   )[0])\ndata.head()",
  "generation": "data['Reasons'] = data['title'].apply(lambda x: x.split(':')[0])\ndata"
 },
 {
  "nbid": "NB_2853881",
  "file": "NB_2853881,data-1.ipynb",
  "row_id": 6,
  "target_code": "weights = [1, 1, 1]\nchosen_atts = [\"Average Count\"               , \"Faculty\"          , \"GRE\"      ]\ndata[\"Score\"       ] = sum([weights[idx]*data[atti] for idx, atti in enumerate(chosen_atts)])\ndata.head(5)",
  "generation": "data['Score'] = data['Faculty']*w1 + data['Average Count']*w2 + data['GRE']*w3\ndata.head()"
 },
 {
  "nbid": "NB_2859445",
  "file": "NB_2859445,data-1.ipynb",
  "row_id": 16,
  "target_code": "data['release_date'              ] = pd.to_datetime(data['release_date'              ])\ndata.head()",
  "generation": "data['release_date'] = pd.to_datetime(data['release_date'])"
 },
 {
  "nbid": "NB_2859445",
  "file": "NB_2859445,data-1.ipynb",
  "row_id": 20,
  "target_code": "del_col=[ 'id'     , 'imdb_id'          , 'budget_adj'             , 'revenue_adj'              ,'homepage'          , 'overview'           ]\ndata= data.drop(del_col,1)\ndata.head()",
  "generation": "data = data[['budget', 'revenue', 'original_title', 'cast', 'director', 'tagline', 'keywords', 'runtime', 'genres', 'production_companies', 'release_date', 'vote_count', 'vote_average', 'release_year']]\ndata.head()"
 },
 {
  "nbid": "NB_2860063",
  "file": "NB_2860063,data-1.ipynb",
  "row_id": 6,
  "target_code": "del_col=[ 'id'     , 'imdb_id'          , 'homepage'           , 'keywords'           , 'overview'           , 'production_companies'                       , 'vote_count'             ]\ndata = data.drop(del_col,1)\ndata.head()",
  "generation": "data.drop(['id', 'imdb_id', 'homepage', 'keywords', 'overview', 'production_companies', 'vote_count'], axis=1, inplace=True)\ndata.head(2)"
 },
 {
  "nbid": "NB_2868405",
  "file": "NB_2868405,pokemon-1.ipynb",
  "row_id": 3,
  "target_code": "pokemon = pokemon.rename(columns = {'#'   :\"Pokemon Number\"                })\npokemon.head(5)",
  "generation": "pokemon.rename(columns={'#':'Pokemon Number'}, inplace=True)\npokemon.head()"
 },
 {
  "nbid": "NB_2892574",
  "file": "NB_2892574,df-1.ipynb",
  "row_id": 8,
  "target_code": "df = df.rename(columns=lambda x: x.lower().replace(\"id\"    , \"_id\"      ).replace(\"day\"     , \"_day\"       ).replace(\"-\"   , \"_\"    ))\ndf = df.rename(columns={'handcap'         : 'handicap'           , 'hipertension'               : 'hypertension'               })\ndf.head(1)",
  "generation": "df.columns = ['patient_id', 'appointment_id', 'gender', 'scheduled_day', 'appointment_day', 'age', 'neighbourhood', 'scholarship', 'hypertension', 'diabetes', 'alcoholism', 'handicap', 'sms_received', 'no_show']\ndf.head(3)"
 },
 {
  "nbid": "NB_2893740",
  "file": "NB_2893740,category1_TS-1.ipynb",
  "row_id": 39,
  "target_code": "category1_TS['GDP'     ] = polEquation(category1_TS['Day'     ])\ncategory1_TS['Date'      ] = pd.to_datetime(category1_TS['Date'      ], format = '%Y.%m.%d'           )\ncategory1_TS.head()",
  "generation": "category1_TS['Date'] = pd.to_datetime(category1_TS['Date'])\nmerged_df = pd.merge(category1_TS, Russia_GDP_interval, on='Date', how='left')\nmerged_df = merged_df.rename(columns = {'GDP':'GDP_interval'})\nmerged_df"
 },
 {
  "nbid": "NB_2893740",
  "file": "NB_2893740,raw_RubinDolar-1.ipynb",
  "row_id": 12,
  "target_code": "raw_RubinDolar['Value'       ] = raw_RubinDolar['Value'       ].str.replace(\",\"   ,\".\"   )\nraw_RubinDolar['Value'       ] = pd.to_numeric(raw_RubinDolar['Value'       ] )\nraw_RubinDolar.head()",
  "generation": "raw_RubinDolar['Value'] = raw_RubinDolar['Value'].str.replace(',', '.').astype(float)\nraw_RubinDolar.head()"
 },
 {
  "nbid": "NB_2893740",
  "file": "NB_2893740,raw_unemployment-1.ipynb",
  "row_id": 54,
  "target_code": "raw_unemployment['Date'      ] = pd.to_datetime(raw_unemployment['Date'      ], format = '%Y.%m.%d'           )\nraw_unemployment= raw_unemployment.sort_values(['Date'      ], ascending=True)\nraw_unemployment.head()",
  "generation": "raw_unemployment['Date'] = pd.to_datetime(raw_unemployment['Date'])\nraw_unemployment.set_index('Date', inplace=True)\nraw_unemployment"
 },
 {
  "nbid": "NB_2893740",
  "file": "NB_2893740,raw_unemployment-1.ipynb",
  "row_id": 56,
  "target_code": "raw_unemployment = raw_unemployment[258:289]\nraw_unemployment = raw_unemployment.reset_index()\ndel raw_unemployment['index'       ]\nraw_unemployment.head()",
  "generation": "raw_unemployment['Date'] = pd.to_datetime(raw_unemployment['Date'], format = '%Y.%m.%d')\nraw_unemployment = raw_unemployment.set_index('Date').resample('D').ffill().reset_index()\nraw_unemployment.head()"
 },
 {
  "nbid": "NB_371699",
  "file": "NB_371699,cats-1.ipynb",
  "row_id": 18,
  "target_code": "attrition_code = LabelEncoder()\ncats[\"Attrition\"           ] = attrition_code.fit_transform(cats[\"Attrition\"           ])\ncats.head() ",
  "generation": "le = LabelEncoder()\nhr['Attrition'] = le.fit_transform(hr['Attrition'])\nhr.head()"
 },
 {
  "nbid": "NB_377385",
  "file": "NB_377385,df-1.ipynb",
  "row_id": 4,
  "target_code": "df = df.rename(columns={'@attribute fixedacid numeric'                              : 'fixedacid'            , \n                        '@attribute volacid numeric'                                                    : 'volacid'          ,\n                        '@attribute citricacid numeric'                                                       : 'citricacid'             ,\n                        '@attribute residualsugar numeric'                                                          : 'residualsugar'                ,\n                        '@attribute chlorides numeric'                                                      : 'chlorides'            ,\n                        '@attribute freesulfur numeric'                                                       : 'freesulfur'             ,\n                        '@attribute totalsulfur numeric'                                                        : 'totalsulfur'              ,\n                        '@attribute density numeric'                                                     : 'density'          ,\n                        '@attribute pH numeric'                                                : 'pH'     ,\n                        '@attribute sulphates numeric'                                                       : 'sulphates'            ,\n                        '@attribute alcohol numeric'                                                     : 'alcohol'          ,\n                        '@attribute quality numeric (0-10)'                                                            : 'quality'          \n                        })\ndf.head(5)",
  "generation": "df.columns = ['fixedacid', 'volacid', 'citricacid', 'residualsugar', 'chlorides', 'freesulfur', 'totalsulfur', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\ndf.head()"
 },
 {
  "nbid": "NB_380713",
  "file": "NB_380713,data-1.ipynb",
  "row_id": 4,
  "target_code": "data['review_comment_length'                       ] = data['review_comment_message'                        ].str.len()\ndata.head()",
  "generation": "data['review_comment_length'] = data['review_comment_message'].str.len()"
 },
 {
  "nbid": "NB_383856",
  "file": "NB_383856,df1-1.ipynb",
  "row_id": 7,
  "target_code": "df1 = df1.drop(['School'        ], axis=1)\ndf1 = df1.set_index('Name'      )\ndf1.head()",
  "generation": ""
 },
 {
  "nbid": "NB_383856",
  "file": "NB_383856,df1-1.ipynb",
  "row_id": 8,
  "target_code": "cols_at_end = ['NBATRB'        ,'NBAAST'        ,'NBASTL'        ,'NBABLK'        ,'NBAPTS'        ]\ndf1 = df1[[c for c in df1 if c not in cols_at_end] + [c for c in cols_at_end if c in df1]]\ndf1.head()",
  "generation": "scaler = StandardScaler()\ndf1.iloc[:, 0:23] = scaler.fit_transform(df1.iloc[:, 0:23])\ndf1.head()"
 },
 {
  "nbid": "NB_383856",
  "file": "NB_383856,df1-1.ipynb",
  "row_id": 10,
  "target_code": "from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nsc2 = StandardScaler()\nfor col in cols:\n    df1[[col]] = sc2.fit_transform(df1[[col]])\nsc3 = StandardScaler()\ndf1[['NBABLK'        ]] = sc3.fit_transform(df1[['NBABLK'        ]])\nsc4 = StandardScaler()\ndf1[['NBASTL'        ]] = sc4.fit_transform(df1[['NBASTL'        ]])\nsc5 = StandardScaler()\ndf1[['NBAAST'        ]] = sc5.fit_transform(df1[['NBAAST'        ]])\nsc6 = StandardScaler()\ndf1[['NBATRB'        ]] = sc6.fit_transform(df1[['NBATRB'        ]])\ndf1.head()",
  "generation": "sc = StandardScaler()\ndf1[cols_at_end] = sc.fit_transform(df1[cols_at_end])\ndf1.head()"
 },
 {
  "nbid": "NB_385422",
  "file": "NB_385422,df_crime_sorted_D-1.ipynb",
  "row_id": 26,
  "target_code": "df_crime_sorted_D['Total Crime'             ] = (df_crime_sorted_D['NIB_SCTYTOT'             ] + df_crime_sorted_D['NIB_DRUGVIOL'              ] + \n                                     df_crime_sorted_D['JRN_DRUGVIOL'              ])\ndf_crime_sorted_D['Crime %'         ] = ((df_crime_sorted_D['Total Crime'             ] / df_crime_sorted_D['Total Population'                  ])*100).round(2)\ndf_crime_sorted_D.head()",
  "generation": "df_crime_sorted_D['Total Crime'] = df_crime_sorted_D['NIB_SCTYTOT'] + df_crime_sorted_D['NIB_DRUGVIOL'] + df_crime_sorted_D['JRN_DRUGVIOL']\ndf_crime_sorted_D['Crime %'    ] = ((df_crime_sorted_D['Total Crime'] / df_crime_sorted_D['Total Population']) * 100).round(2)\ndf_crime_sorted_D.head()"
 },
 {
  "nbid": "NB_385422",
  "file": "NB_385422,df_crime_sorted_P-1.ipynb",
  "row_id": 34,
  "target_code": "df_crime_sorted_P['Total Crime'             ] = (df_crime_sorted_P['NIB_PROPTOT'             ] + df_crime_sorted_P['NIB_ARSON'           ] + \n                                     df_crime_sorted_P['NIB_BRIBERY'             ] + df_crime_sorted_P['NIB_BURGLARY'              ] + \n                                     df_crime_sorted_P['JRN_ARSON'           ] +  df_crime_sorted_P['JRN_BURGLARY'              ])\ndf_crime_sorted_P['Crime %'         ] = ((df_crime_sorted_P['Total Crime'             ] / df_crime_sorted_P['Total Population'                  ])*100).round(2)\ndf_crime_sorted_P.head()",
  "generation": "df_crime_sorted_P['Total Crime'] = df_crime_sorted_P[['NIB_PROPTOT', 'NIB_ARSON', 'NIB_BRIBERY', 'NIB_BURGLARY', 'JRN_ARSON', 'JRN_BURGLARY']].sum(axis=1)\ndf_crime_sorted_P['Crime %'] = ((df_crime_sorted_P['Total Crime'] / df_crime_sorted_P['Total Population']) * 100).round(2)\ndf_crime_sorted_P.head()"
 },
 {
  "nbid": "NB_385422",
  "file": "NB_385422,df_crime_sorted_R-1.ipynb",
  "row_id": 30,
  "target_code": "df_crime_sorted_R['Total Crime'             ] = (df_crime_sorted_R['NIB_EXTORTION'               ] + df_crime_sorted_R['NIB_ROBBERY'             ] + \n                                     df_crime_sorted_R['NIB_THEFT'           ] + df_crime_sorted_R['JRN_EXTORTION'               ] + \n                                     df_crime_sorted_R['JRN_ROBBERY'             ] +  df_crime_sorted_R['JRN_THEFT'           ])\ndf_crime_sorted_R['Crime %'         ] = ((df_crime_sorted_R['Total Crime'             ] / df_crime_sorted_R['Total Population'                  ])*100).round(2)\ndf_crime_sorted_R.head()",
  "generation": "df_crime_sorted_R['Total Crime'] = df_crime_sorted_R[['NIB_EXTORTION', 'NIB_ROBBERY', 'NIB_THEFT', 'JRN_EXTORTION', 'JRN_ROBBERY', 'JRN_THEFT']].sum(axis=1)\ndf_crime_sorted_R['Crime %'] = ((df_crime_sorted_R['Total Crime'] / df_crime_sorted_R['Total Population']) * 100).round(2)\ndf_crime_sorted_R"
 },
 {
  "nbid": "NB_385422",
  "file": "NB_385422,df_crime_sorted_S-1.ipynb",
  "row_id": 38,
  "target_code": "df_crime_sorted_S['Total Crime'             ] = (df_crime_sorted_S['NIB_FSEX'          ] + df_crime_sorted_S['NIB_NFSEX'           ] + \n                                     df_crime_sorted_S['NIB_HTRFFCKNG'               ] + df_crime_sorted_S['JRN_FSEX'          ] + \n                                     df_crime_sorted_S['JRN_NFSEX'           ])\ndf_crime_sorted_S['Crime %'         ] = ((df_crime_sorted_S['Total Crime'             ] / df_crime_sorted_S['Total Population'                  ])*100).round(2)\ndf_crime_sorted_S.head()",
  "generation": "df_crime_sorted_S['Total Crime'] = df_crime_sorted_S[['NIB_FSEX', 'NIB_NFSEX', 'NIB_HTRFFCKNG', 'JRN_FSEX', 'JRN_NFSEX']].sum(axis=1)\ndf_crime_sorted_S['Crime %'] = ((df_crime_sorted_S['Total Crime'] / df_crime_sorted_S['Total Population']) * 100).round(2)\ndf_crime_sorted_S.head()"
 },
 {
  "nbid": "NB_385422",
  "file": "NB_385422,df_crime_sorted_Total-1.ipynb",
  "row_id": 14,
  "target_code": "df_crime_sorted_Total['Total Crime'             ] = df_crime_sorted_Total['JRN_TOTAL'           ] + df_crime_sorted_Total['NIB_TOTAL'           ]\ndf_crime_sorted_Total.head()",
  "generation": "df_crime_sorted_Total['Total Crime'] = df_crime_sorted_Total['JRN_TOTAL'] + df_crime_sorted_Total['NIB_TOTAL']\ndf_crime_sorted_Total.head()"
 },
 {
  "nbid": "NB_385422",
  "file": "NB_385422,df_crime_sorted_Total-1.ipynb",
  "row_id": 15,
  "target_code": "df_crime_sorted_Total['Crime %'         ] = ((df_crime_sorted_Total['Total Crime'             ] / df_crime_sorted_Total['Total Population'                  ])*100).round(2)\ndf_crime_sorted_Total.head()",
  "generation": "df_crime_sorted_Total['Crime %'] = (df_crime_sorted_Total['Total Crime'] / df_crime_sorted_Total['Total Population']) * 100\ndf_crime_sorted_Total"
 },
 {
  "nbid": "NB_385422",
  "file": "NB_385422,df_crime_sorted_VM-1.ipynb",
  "row_id": 22,
  "target_code": "df_crime_sorted_VM['Total Crime'             ] = (df_crime_sorted_VM['NIB_ASSAULT'             ] + df_crime_sorted_VM['NIB_VIOLNCO'             ] + \n                                     df_crime_sorted_VM['NIB_KIDNAP'            ] + df_crime_sorted_VM['NIB_PROPTOT'             ] +\n                                     df_crime_sorted_VM['NIB_MURDER'            ] + df_crime_sorted_VM['NIB_MNSLGHTR'              ] +\n                                     df_crime_sorted_VM['JRN_ASSAULT'             ] + df_crime_sorted_VM['JRN_VIOLNCO'             ] +\n                                     df_crime_sorted_VM['JRN_KIDNAP'            ] + df_crime_sorted_VM['JRN_MURDER'            ] +\n                                     df_crime_sorted_VM['JRN_MNSLGHTR'              ])\ndf_crime_sorted_VM['Crime %'         ] = ((df_crime_sorted_VM['Total Crime'             ] / df_crime_sorted_VM['Total Population'                  ])*100).round(2)\ndf_crime_sorted_VM.head()",
  "generation": "df_crime_sorted_VM['Total Crime'] = df_crime_sorted_VM[['NIB_ASSAULT', 'NIB_VIOLNCO', 'NIB_KIDNAP', 'NIB_PROPTOT', \n                                                        'NIB_MURDER', 'NIB_MNSLGHTR', 'JRN_ASSAULT', 'JRN_VIOLNCO', \n                                                        'JRN_KIDNAP', 'JRN_MURDER', 'JRN_MNSLGHTR']].sum(axis=1)\ndf_crime_sorted_VM['Crime %'] = ((df_crime_sorted_VM['Total Crime'] / df_crime_sorted_VM['Total Population']) * 100).round(2)\ndf_crime_sorted_VM.head()"
 },
 {
  "nbid": "NB_392026",
  "file": "NB_392026,new_df-1.ipynb",
  "row_id": 26,
  "target_code": "new_df = new_df.drop(columns = \"fnlwgt\"         )\nnew_df.head()",
  "generation": "new_df.drop('fnlwgt', axis=1, inplace=True)\nnew_df.head()"
 },
 {
  "nbid": "NB_5816277",
  "file": "NB_5816277,data-1.ipynb",
  "row_id": 1,
  "target_code": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndata[\"Gender\"        ] = le.fit_transform(data[\"Gender\"        ])\ndata.head()",
  "generation": "le = LabelEncoder()\ndata['Gender'] = le.fit_transform(data['Gender'])\ndata.head()"
 },
 {
  "nbid": "NB_5844976",
  "file": "NB_5844976,df-1.ipynb",
  "row_id": 3,
  "target_code": "df = df.drop(['adsh'      ], axis=1)\ndf.head()",
  "generation": "df = df.astype(int)\ndf.head()"
 },
 {
  "nbid": "NB_5996326",
  "file": "NB_5996326,class_data-1.ipynb",
  "row_id": 25,
  "target_code": "class_data['vowel'       ] = [IPAdict[x] for x in class_data['vowel'       ]]\nclass_data.head()",
  "generation": "class_data['vowel'] = class_data['vowel'].map(IPAdict)\nclass_data.head()"
 },
 {
  "nbid": "NB_17942",
  "file": "NB_17942,df-1.ipynb",
  "row_id": 3,
  "target_code": "df.age = round(df.age / 365)\ndf.age = df.age.astype(int)\ndf.head()",
  "generation": "df['age'] = df['age'] // 365\ndf.head()"
 },
 {
  "nbid": "NB_1826134",
  "file": "NB_1826134,d-1.ipynb",
  "row_id": 7,
  "target_code": "unique_locs = d.location.value_counts().index.tolist()\nloc_map = [x for x in range(len(unique_locs))]\nloc_set = dict(zip(unique_locs, loc_map))\nd['numeric_location'                  ] = d['location'          ].map(lambda x: loc_set[x] if x in loc_set.keys() else -1)\nd.head()",
  "generation": "d['numeric_location'] = pd.factorize(d['location'])[0]\nd.head()"
 },
 {
  "nbid": "NB_1828853",
  "file": "NB_1828853,insurance_df-1.ipynb",
  "row_id": 29,
  "target_code": "insurance_df['Age'     ] = 2016 - insurance_df['Brithday Year'               ]\ninsurance_df.head()",
  "generation": "insurance_df['Age'] = insurance_df['First Policy\u00b4s Year'] - insurance_df['Brithday Year']\ninsurance_df = insurance_df.drop(['First Policy\u00b4s Year'], axis=1)\ninsurance_df.head()"
 },
 {
  "nbid": "NB_1852053",
  "file": "NB_1852053,data-1.ipynb",
  "row_id": 6,
  "target_code": "data.index = pd.to_datetime(data.index)\nprint(data.index)\ndata.head(2)",
  "generation": "data.index = pd.to_datetime(data.index, format='%Y%m%d %H:%M:%S.%f')\ndata.head(2)"
 },
 {
  "nbid": "NB_1852150",
  "file": "NB_1852150,df-1.ipynb",
  "row_id": 32,
  "target_code": "df['isCorrect'           ] = df['Correct'         ].astype(bool)\ndf.head()",
  "generation": "df['isCorrect'] = df['Correct'].astype(bool)\ndf.head()"
 },
 {
  "nbid": "NB_1860785",
  "file": "NB_1860785,movies_df_narrowed-1.ipynb",
  "row_id": 31,
  "target_code": "movies_df_narrowed['month'       ] = movies_df_narrowed['release_date'              ].apply(lambda x: x.month)\nmovies_df_narrowed['year'      ] = movies_df_narrowed['release_date'              ].apply(lambda x: x.year)\nmovies_df_narrowed.head()",
  "generation": "movies_df_narrowed['release_date'] = pd.to_datetime(movies_df_narrowed['release_date'])\nmovies_df_narrowed['month'] = movies_df_narrowed['release_date'].dt.month\nmovies_df_narrowed['year'] = movies_df_narrowed['release_date'].dt.year\nmovies_df_narrowed.head()"
 },
 {
  "nbid": "NB_19364",
  "file": "NB_19364,data-1.ipynb",
  "row_id": 3,
  "target_code": "data[\"Popularity\"            ].astype(str).astype(int)\ndata['Popularity_bucket'                   ]=data.Popularity.map(lambda x: x/10).apply(np.floor).astype(int)\ndata.head(10)",
  "generation": "data['Popularity_bucket'] = pd.cut(data['Popularity'], bins=[0, 20, 40, 60, 80, 100], labels=[1, 3, 5, 7, 9])\ndata.head()"
 },
 {
  "nbid": "NB_20131",
  "file": "NB_20131,population-1.ipynb",
  "row_id": 29,
  "target_code": "population[\"2011\"      ] = population[\"2011\"      ].astype(float)\npopulation[\"2012\"      ] = population[\"2012\"      ].astype(float)\npopulation[\"2013\"      ] = population[\"2013\"      ].astype(float)\npopulation[\"2014\"      ] = population[\"2014\"      ].astype(float)\npopulation[\"2015\"      ] = population[\"2015\"      ].astype(float)\npopulation[\"2016\"      ] = population[\"2016\"      ].astype(float)\npopulation[\"2017\"      ] = population[\"2017\"      ].astype(float)\npopulation.head()",
  "generation": "population.iloc[:,0:7] = population.iloc[:,0:7].astype(float)\npopulation.head()"
 },
 {
  "nbid": "NB_2915148",
  "file": "NB_2915148,df-1.ipynb",
  "row_id": 9,
  "target_code": "df['release_date'              ] = pd.to_datetime(df['release_date'              ], format='%m/%d/%y'          )\ndf.head(2)",
  "generation": "df['release_date'] = pd.to_datetime(df['release_date'])\ndf.head(2)"
 },
 {
  "nbid": "NB_2915148",
  "file": "NB_2915148,df-1.ipynb",
  "row_id": 13,
  "target_code": "df['profit'        ]=df['revenue'         ]-df['budget'        ]\ndf['profit_adj'            ]=df['revenue_adj'             ]-df['budget_adj'            ]\ndf.head(3)",
  "generation": "df['profit'] = df['revenue'] - df['budget']\ndf['year'] = pd.DatetimeIndex(df['release_date']).year\ndf.groupby('year')['profit'].idxmax().apply(lambda x: df.loc[x, ['budget', 'revenue', 'original_title', 'director', 'runtime', 'genres', 'release_date', 'vote_count', 'vote_average', 'year', 'profit']]).reset_index(drop=True)"
 },
 {
  "nbid": "NB_303725",
  "file": "NB_303725,df-1.ipynb",
  "row_id": 6,
  "target_code": "df=(df.melt(id_vars=[\"Indicator\"           ,\"Registry\"          ,\"Sex\"     ,\"Cancer\"        ,\"Age\"     ]\n         ,var_name=\"Year\"      ,value_name=\"Cases\"       ) \n    .astype({'Year'      : int})\n)\ndf.head()",
  "generation": "df = pd.melt(df, id_vars=['Indicator', 'Registry', 'Sex', 'Cancer', 'Age'], var_name='Year', value_name='Cases')\ndf = df.dropna(subset=['Cases'])\ndf.head()"
 },
 {
  "nbid": "NB_310583",
  "file": "NB_310583,df-1.ipynb",
  "row_id": 9,
  "target_code": "df['isCustomerSuspended'                     ] = df['isCustomerSuspended'                     ].map({'Yes'     : 1, 'No'     : 0})\ndf['isChurned'           ] = df['isChurned'           ].map({'Yes'     : 1, 'No'     : 0})\ndf['Gender'        ] = df['Gender'        ].map({'Male'      : 1, 'Female'         : 0})\ndf = df.rename(columns={\"MonthlyBilledAmount - Rs\"                          : \"MonthlyBilledAmount\"                      ,\n                        \"UnpaidBalance - Rs\"                                            : \"UnpaidBalance\"                \n                                          })\ndf.head()",
  "generation": "gender_dict = {'Male': 1, 'Female': 0}\ndf['Gender'] = df['Gender'].map(gender_dict)\ndf.head(5)"
 },
 {
  "nbid": "NB_311838",
  "file": "NB_311838,df-1.ipynb",
  "row_id": 59,
  "target_code": "df[\"cty_kml\"         ] = df[\"cty\"     ] * 0.425143707\ndf[\"cty_kml\"         ] = df[\"cty_kml\"         ].round(2)\ndf.head(5)",
  "generation": "The target code cannot be written based on the given context as the output dataframe is completely different from the input dataframe and there is no clear instruction on how to transform the input dataframe to the output dataframe. Please provide more information or clarify the task."
 },
 {
  "nbid": "NB_311838",
  "file": "NB_311838,df-1.ipynb",
  "row_id": 60,
  "target_code": "df[\"transmission\"              ] = df.trans.str[0]\ndf.head(5)",
  "generation": "df['transmission'] = df['trans'].apply(lambda x: x[0])\ndf.head()"
 },
 {
  "nbid": "NB_316793",
  "file": "NB_316793,dataX-1.ipynb",
  "row_id": 21,
  "target_code": "from sklearn import preprocessing as pp\nfeaturesToScale = dataX.columns\nsX = pp.StandardScaler(copy=True, with_mean=True, with_std=True)\ndataX.loc[:,featuresToScale] = sX.fit_transform(dataX[featuresToScale])\ndataX.head()",
  "generation": "scaler = pp.StandardScaler()\ndataX = scaler.fit_transform(dataX)\ndf = pd.concat([pd.DataFrame(dataX), df['Fraud']], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_316793",
  "file": "NB_316793,testDataX-1.ipynb",
  "row_id": 23,
  "target_code": "featuresToScale = testDataX.columns\ntestDataX.loc[:,featuresToScale] = sX.fit_transform(testDataX[featuresToScale])\ntestDataX.head()",
  "generation": "testDataX = sX.fit_transform(testDataX)"
 },
 {
  "nbid": "NB_5354288",
  "file": "NB_5354288,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.join(dummy_gender)\ndf = df.join(dummy_city_tier)\ndf.head()",
  "generation": "df = pd.read_csv('../datasets/ecom-expense/Ecom Expense.csv')\ndummy_gender = pd.get_dummies(df['Gender'], prefix='Gender', prefix_sep='_')\ndummy_city_tier = pd.get_dummies(df['City Tier'], prefix='City', prefix_sep='_')\ndf = pd.concat([df, dummy_gender, dummy_city_tier], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_5869704",
  "file": "NB_5869704,df-1.ipynb",
  "row_id": 5,
  "target_code": "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\nlabelencoder = LabelEncoder()\ndf['Genre'       ] = labelencoder.fit_transform(df['Genre'       ])\ndf.head()",
  "generation": "labelencoder = LabelEncoder()\ndf['Name'] = labelencoder.fit_transform(df['Name'])\ndf['Genre'] = labelencoder.fit_transform(df['Genre'])\ndf = df.drop(['Unique_ID', 'Country', 'Song_Name', 'Timestamp', 'Comments', 'Followers'], axis=1)\ndf = df.dropna()\ndf = df.astype({'Likes': 'float', 'Popularity': 'float'})\ndf.head()"
 },
 {
  "nbid": "NB_5882100",
  "file": "NB_5882100,dataset-1.ipynb",
  "row_id": 8,
  "target_code": "dataset = pd.concat([dataset, dataset1], axis=1)\ndataset.head()",
  "generation": "peptides = dataset[\"Peptides\"].str.split(\"\", expand=True)\npeptides = peptides.iloc[:, 1:-1]\ndataset1 = pd.concat([dataset, peptides], axis=1)\ndataset1 = dataset1.drop(columns=[\"Peptides\"])\ndataset1.head()"
 },
 {
  "nbid": "NB_5882100",
  "file": "NB_5882100,dataset-1.ipynb",
  "row_id": 9,
  "target_code": "dataset = dataset.drop(columns=\"Peptides\"          , axis=0)\ndataset.head()",
  "generation": "dataset = dataset.drop(columns=['Peptides'])\ndataset.head()"
 },
 {
  "nbid": "NB_5882100",
  "file": "NB_5882100,dataset-1.ipynb",
  "row_id": 10,
  "target_code": "dataset = dataset[['A'   ,'B'   ,'C'   ,'D'   ,'E'   ,'F'   ,'G'   ,'H'   ,'Result'        ]]\ndataset.head()",
  "generation": "dataset = dataset[['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Result']]\ndataset.head()"
 },
 {
  "nbid": "NB_1899877",
  "file": "NB_1899877,df-1.ipynb",
  "row_id": 4,
  "target_code": "df['Alliance'          ] = df['Party Abbreviation'                    ]\ndf['Alliance'          ] = df['Alliance'          ].replace(to_replace=['INC'     , 'NCP'      , 'RJD'      , 'DMK'      , 'IUML'       , 'JMM'      , 'JD(s)'        , 'KC(M)'        , 'RLD'      , \n                                                    'RSP'                                                         , 'CMP(J)'         , 'KC(J)'        , 'PPI'      , 'MD'     ], value='UPA'     )\ndf['Alliance'          ] = df['Alliance'          ].replace(to_replace=['BJP'     , 'SS'     , 'LJP'      , 'SAD'      , 'RLSP'       , 'AD'     , 'PMK'      , 'NPP'      , 'AINRC'        , 'NPF'      , \n                                                    'RPI(A)'                                                            , 'BPF'      , 'JD(U)'        , 'SDF'      , 'NDPP'       , 'MNF'      , 'RIDALOS'          , 'KMDK'       , \n                                                    'IJK'                                                         , 'PNK'      , 'JSP'      , 'GJM'      , 'MGP'      , 'GFP'      , 'GVP'      , 'AJSU'       , 'IPFT'       , 'MPP'      , \n                                                    'KPP'                                                         , 'JKPC'       , 'KC(T)'        , 'BDJS'       , 'AGP'      , 'JSS'      , 'PPA'      , 'UDP'      , 'HSPDP'        , \n                                                    'PSP'                                                         , 'JRS'      , 'KVC'      , 'PNP'      , 'SBSP'       , 'KC(N)'        , 'PDF'      , 'MDPF'       ], \n                                        value='NDA'     )\ndf['Alliance'          ] = df['Alliance'          ].replace(to_replace=['YSRCP'       , 'AAAP'       , 'IND'      , 'AIUDF'        , 'BLSP'       , 'JKPDP'        , 'JD(S)'        , 'INLD'       , \n                                                    'CPI'                                                         , 'AIMIM'        , 'KEC(M)'         ,'SWP'     , 'NPEP'       , 'JKN'      , 'AIFB'       , 'MUL'      , 'AUDF'       , \n                                                    'BOPF'                                                          , 'BVA'      , 'HJCBL'        , 'JVM'      , 'MDMK'       ], value='Others'        )\ndf.head()",
  "generation": ""
 },
 {
  "nbid": "NB_1904766",
  "file": "NB_1904766,mpesa-1.ipynb",
  "row_id": 8,
  "target_code": "mpesa['Created_at'            ] =  pd.to_datetime(mpesa['Created_at'            ])\nmpesa.index = mpesa['Created_at'            ]\nmpesa = mpesa.drop(columns=['Created_at'            ,'Location'          ])\nmpesa = mpesa.dropna() \nmpesa.head()",
  "generation": "mpesa = mpesa.set_index(pd.to_datetime(mpesa['Created_at']))\nmpesa = mpesa.drop(columns=['Created_at'])\nmpesa.head()"
 },
 {
  "nbid": "NB_1905945",
  "file": "NB_1905945,oshkosh-1.ipynb",
  "row_id": 28,
  "target_code": "oshkosh['osk_n'       ] = oshkosh[\"osk\"     ] / oshkosh.loc[dt.datetime(2018, 1, 2), \"osk\"      ]\noshkosh['sp500_n'         ] = oshkosh[\"sp500\"       ] / oshkosh.loc[dt.datetime(2018, 1, 2), \"sp500\"        ]\noshkosh.head()",
  "generation": "oshkosh['osk_n'] = oshkosh['osk'] / oshkosh.loc['2018-01-02','osk']\noshkosh['sp500_n'] = oshkosh['sp500'] / oshkosh.loc['2018-01-02','sp500']\noshkosh.head()"
 },
 {
  "nbid": "NB_1907095",
  "file": "NB_1907095,tdmb_data-1.ipynb",
  "row_id": 6,
  "target_code": "del_col=[ 'id'     , 'imdb_id'          , 'popularity'             , 'budget_adj'             , 'revenue_adj'              , 'homepage'           , 'keywords'           , 'overview'           , 'production_companies'                       , 'vote_count'             , 'vote_average'               ]\ntdmb_data=tdmb_data.drop(del_col,1)\ntdmb_data.head(3)",
  "generation": "tdmb_data.drop(['id', 'imdb_id', 'popularity', 'budget_adj', 'revenue_adj', 'homepage', 'keywords', 'overview', 'production_companies', 'vote_count', 'vote_average'], axis=1, inplace=True)\ntdmb_data.drop_duplicates(inplace=True)\ntdmb_data.dropna(subset=['budget', 'revenue'], inplace=True)\ntdmb_data['budget'] = tdmb_data['budget'].apply(lambda x: '${:,.2f}'.format(x))\ntdmb_data['revenue'] = tdmb_data['revenue'].apply(lambda x: '${:,.2f}'.format(x))\ntdmb_data['release_date'] = pd.to_datetime(tdmb_data['release_date'])\ntdmb_data['runtime'] = tdmb_data['runtime'].replace(0, np.nan)\ntdmb_data.dropna(subset=['runtime'], inplace=True)\ntdmb_data['year'] = pd.DatetimeIndex(tdmb_data['release_date']).year\ntdmb_data = tdmb_data[['budget', 'revenue', 'original_title', 'cast', 'director', 'tagline', 'runtime', 'genres', 'release_date', 'year']]\ntdmb_data.head(2)"
 },
 {
  "nbid": "NB_1909043",
  "file": "NB_1909043,churn-1.ipynb",
  "row_id": 8,
  "target_code": "churn['churn'       ] = np.where(churn['churn'       ] == 'True.'        , True, churn['churn'       ])\nchurn['churn'       ] = np.where(churn['churn'       ] == 'False.'         , False, churn['churn'       ])\nchurn.head()",
  "generation": "There is no transformation required as the input dataframe and output dataframe are the same. Therefore, the target code is simply the code provided in the NL context:\n\n```import pandas as pd\nimport numpy as np\nfrom scipy import stats\nchurn = pd.read_csv('churn_analysis.csv')\nchurn.head()```"
 },
 {
  "nbid": "NB_1910910",
  "file": "NB_1910910,movies_df-1.ipynb",
  "row_id": 125,
  "target_code": "movies_df[\"rating_category\"                 ] = movies_df[\"rating\"        ].apply(lambda x: 'good'        if x >= 6.0 else 'bad'      )\nmovies_df.head(2)",
  "generation": "movies_df.columns = [col.lower() for col in movies_df.columns]"
 },
 {
  "nbid": "NB_1916990",
  "file": "NB_1916990,crime_patterns-1.ipynb",
  "row_id": 22,
  "target_code": "crime_patterns = crime_patterns.set_index('START_DATE'            )\ncrime_patterns.tail(12)",
  "generation": "crime_patterns = crimes[['ucr-rank']]\ncrime_patterns.head()"
 },
 {
  "nbid": "NB_1923935",
  "file": "NB_1923935,HrData-1.ipynb",
  "row_id": 27,
  "target_code": "HrData_salary = pd.get_dummies(HrData['salary'        ])\nHrData = HrData.join(HrData_salary)\nHrData.drop([\"salary\"        ], axis = 1, inplace=True)\nHrData.head()",
  "generation": "HrData1 = HrData1.rename(columns={'sales': 'dept'})\nrfc = RandomForestClassifier(n_estimators=100)\nHrData_dept = pd.get_dummies(HrData1['dept'])\nHrData = HrData1.join(HrData_dept)\nHrData.drop([\"dept\"], axis = 1, inplace=True)\nHrData = HrData[['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'Work_accident', 'left', 'promotion_last_5years', 'IT', 'RandD', 'accounting', 'hr', 'management', 'marketing', 'product_mng', 'sales', 'support', 'technical', 'high', 'low', 'medium']]\nHrData.head()"
 },
 {
  "nbid": "NB_1934471",
  "file": "NB_1934471,patients-1.ipynb",
  "row_id": 18,
  "target_code": "patients=patients.set_index(patients['id'    ].values) \npatients.head()",
  "generation": "patients.set_index('id', inplace=True)\npatients.head()"
 },
 {
  "nbid": "NB_1934471",
  "file": "NB_1934471,patients-1.ipynb",
  "row_id": 19,
  "target_code": "patients=patients.drop('id'    ,axis=1) \npatients.head()",
  "generation": "patients = patients.drop(['id'], axis=1)\npatients.head()"
 },
 {
  "nbid": "NB_1939580",
  "file": "NB_1939580,datas-1.ipynb",
  "row_id": 3,
  "target_code": "datas['mpg_level'           ] = datas.apply(lambda row : 'low'       if (row['mpg'     ]<20) else ( 'medium'          if (row['mpg'     ]>=20 and row['mpg'     ]<=30) else 'high'       ), axis=1)\ndatas.head(20)",
  "generation": "def mpg_level(x):\n    if x < 20:\n        return 'low'\n    elif x >= 20 and x < 30:\n        return 'medium'\n    else:\n        return 'high'\n\ndatas['mpg_level'] = datas['mpg'].apply(mpg_level)\ndatas.head()"
 },
 {
  "nbid": "NB_1944186",
  "file": "NB_1944186,df_data-1.ipynb",
  "row_id": 4,
  "target_code": "cat_means = df_data.groupby(\"Category\"          ).mean().reset_index().rename(index=str, columns={\"% of Spending\"               : \"Cat Mean % of Spending\"                         })\ncat_means['Category Rank'               ]=cat_means['Cat Mean % of Spending'                        ].rank(ascending=0,method='dense'       ).astype(int)\ncat_means['row'     ]=((cat_means['Category Rank'               ].astype(int)-1)/3 + 1).astype(int)\ncat_means['col'     ]=(cat_means['Category Rank'               ]%3).replace(0, 3).astype(int)\ndf_data = df_data.merge(cat_means, on='Category'          )\ndf_data.head()",
  "generation": "df_data['Category Rank'] = df_data.groupby('Category')['% of Spending'].rank(ascending=False, method='dense').astype(int)\ndf_data['Cat Mean % of Spending'] = df_data.groupby('Category')['% of Spending'].transform('mean')\ndf_data['row'] = df_data['Category Rank']\ndf_data['col'] = df_data['Generation'].apply(lambda x: ['Millenials', 'Generation X', 'Baby Boomers', 'Traditionalists'].index(x) + 1)\ndf_data = df_data.sort_values(['col', 'row'])\ndf_data.reset_index(drop=True, inplace=True)\ndf_data.head()"
 },
 {
  "nbid": "NB_1944186",
  "file": "NB_1944186,df_data-1.ipynb",
  "row_id": 5,
  "target_code": "df_data['Generation'            ] = pd.Categorical(df_data['Generation'            ],    [\"Millenials\"            , \"Generation X\"               , \"Baby Boomers\"               , \"Traditionalists\"                  ])\ndf_data = df_data.sort_values('Generation'            )\ndf_data.head()",
  "generation": "df_data['Generation'      ]=pd.Categorical(df_data['Generation'      ], categories=['Traditionalists', 'Baby Boomers', 'Generation X', 'Millenials'], ordered=True)\ndf_data.sort_values(['Generation', 'Category Rank'], ascending=[True, True], inplace=True)\ndf_data.reset_index(drop=True, inplace=True)\ndf_data.head()"
 },
 {
  "nbid": "NB_1945013",
  "file": "NB_1945013,prices-1.ipynb",
  "row_id": 28,
  "target_code": "prices['osk_n'       ] = prices['osk'     ] / prices.loc['2018-01-02,'             ,'osk'     ]\nprices['sp500_n'         ] = prices['sp500'       ] / prices.loc['2018-01-02,'             ,'sp500'       ]\nprices.head()",
  "generation": "prices['osk_n'] = prices['osk'] / prices.loc['2018-01-02','osk']\nprices['sp500_n'] = prices['sp500'] / prices.loc['2018-01-02','sp500']\nprices.head()"
 },
 {
  "nbid": "NB_1947793",
  "file": "NB_1947793,df-1.ipynb",
  "row_id": 7,
  "target_code": "for col in df.columns:\n    df[col] = df[col].replace(\"?\"   , np.NaN)\ndf = df.apply(lambda x:x.fillna(x.value_counts().index[0]))\ndf.head()",
  "generation": "df = df.replace('?', np.nan)"
 },
 {
  "nbid": "NB_1950149",
  "file": "NB_1950149,data-1.ipynb",
  "row_id": 5,
  "target_code": "data['text'      ] = [i.lower() for i in data['text'      ]]\ndata.head()",
  "generation": "data['text'] = data['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndata.head()"
 },
 {
  "nbid": "NB_1957371",
  "file": "NB_1957371,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.drop([0], axis=0)\ndf.head(3)",
  "generation": "There is no transformation happening in the code provided. The input and output dataframes are the same."
 },
 {
  "nbid": "NB_1958519",
  "file": "NB_1958519,df-1.ipynb",
  "row_id": 13,
  "target_code": "df['subtask_a'           ] = (df['subtask_a'           ] == 'OFF'      ).astype(int)\ndf.head()",
  "generation": "df_train['subtask_a'] = df_train['subtask_a'].apply(lambda x: 1 if x == 'OFF' else 0)\ndf_train.head()"
 },
 {
  "nbid": "NB_1967610",
  "file": "NB_1967610,X-1.ipynb",
  "row_id": 3,
  "target_code": "y= X['Category'          ].ravel()\nX = X.select_dtypes(include=[object])\nX.head(3)",
  "generation": "categorical_columns = ['name', 'creative_type', 'source', 'production_method', 'genre', 'language', 'board_rating_reason', 'movie_board_rating_display_name', 'movie_release_pattern_display_name']\nX = X[categorical_columns]\nX.head()"
 },
 {
  "nbid": "NB_1975430",
  "file": "NB_1975430,df-1.ipynb",
  "row_id": 14,
  "target_code": "diabetes_map = {True : 1, False : 0}\ndf['diabetes'          ] = df['diabetes'          ].map(diabetes_map)\ndf.head(5)",
  "generation": "df.drop('skin', axis=1, inplace=True)\ndiabetes_map = {True:1, False:0}\ndf['diabetes'] = df['diabetes'].map(diabetes_map)\ndf.head()"
 },
 {
  "nbid": "NB_1975430",
  "file": "NB_1975430,exam_data-1.ipynb",
  "row_id": 23,
  "target_code": "from sklearn import preprocessing\nexam_data[['math score'            ]] = exam_data[['math score'            ]].astype(float)\nexam_data[['reading score'               ]] = exam_data[['reading score'               ]].astype(float)\nexam_data[['writing score'               ]] = exam_data[['writing score'               ]].astype(float)\nexam_data[['math score'            ]] = preprocessing.scale(exam_data[['math score'            ]])\nexam_data[['reading score'               ]] = preprocessing.scale(exam_data[['reading score'               ]])\nexam_data[['writing score'               ]] = preprocessing.scale(exam_data[['writing score'               ]])\nexam_data.head(5)",
  "generation": "scaler = preprocessing.StandardScaler()\nexam_data[['math score', 'reading score', 'writing score']] = scaler.fit_transform(exam_data[['math score', 'reading score', 'writing score']])\nexam_data"
 },
 {
  "nbid": "NB_330324",
  "file": "NB_330324,df-1.ipynb",
  "row_id": 24,
  "target_code": "print (len (df['A'   ]))\nprint (df['Age'     ].dtype)\ndf['Float_Age'           ] = pd.to_numeric(df['Age'     ], errors='coerce'        ,downcast='float'       )\nprint (df['Float_Age'           ].dtype)\nprint (\"Avg Age\"          ,df['Float_Age'           ].mean())\nprint (\"Oldest Animal\"               ,df['Float_Age'           ].max() )\nprint (\"Youngest Animal\"                 ,df['Float_Age'           ].min() )\ndf.head()",
  "generation": "df['Float_Age'] = df['Age'].astype(float)\ndf.head()"
 },
 {
  "nbid": "NB_338274",
  "file": "NB_338274,df_with_dummy-1.ipynb",
  "row_id": 75,
  "target_code": "df_with_dummy = df_with_dummy.join(dummies)\ndf_with_dummy.drop(columns=['LabelAppeal'             ], inplace=True)\ndf_with_dummy.head()",
  "generation": "import pandas as pd\n# get dummy variables based on AcidIndex_new\ndummies = pd.get_dummies(df_with_dummy['AcidIndex_new'])\n\n# merge the dummy variables with the original data (excluding index column)\ndf_with_dummy = df_with_dummy.join(dummies)\n\n# drop columns AcidIndex and STARS\ndf_with_dummy.drop(columns=['AcidIndex', 'AcidIndex_new', 'LabelAppeal'], inplace=True)\n\n# merge the dummy variables with the original data (excluding index column)\ndummies = pd.get_dummies(copy['LabelAppeal'])\ndf_with_dummy = df_with_dummy.join(dummies)\n\n# drop column LabelAppeal\ndf_with_dummy.drop(columns=['LabelAppeal'], inplace=True)\n\n# sanity check\ndf_with_dummy.head()"
 },
 {
  "nbid": "NB_338274",
  "file": "NB_338274,df_with_dummy-1.ipynb",
  "row_id": 83,
  "target_code": "df_with_dummy = df_with_dummy.join(df_absolute[columns])\ndf_with_dummy.head()",
  "generation": "df_with_dummy = df_absolute[columns]\ndf_with_dummy['AcidIndex_new'] = copy['AcidIndex_new']\ndummies = pd.get_dummies(df_with_dummy['AcidIndex_new'])\ndf_with_dummy = df_with_dummy.join(dummies)\ndf_with_dummy.drop(columns=['AcidIndex_new'], inplace=True)\ndf_with_dummy.drop(columns=['LabelAppeal'], inplace=True)\ndf_with_dummy = df_with_dummy.join(copy['LabelAppeal'])\ndf_with_dummy = df_with_dummy.join(copy['TARGET'])\ndf_with_dummy.head()"
 },
 {
  "nbid": "NB_5519912",
  "file": "NB_5519912,movies_df-1.ipynb",
  "row_id": 93,
  "target_code": "movies_df[\"rating_category\"                 ] = movies_df[\"rating\"        ].apply(lambda x: 'good'        if x >= 8.0 else 'bad'      )\nmovies_df.head(2)",
  "generation": "def rating_function(x):\n    if x >= 8.0:\n        return \"good\"\n    else:\n        return \"bad\"\n\nmovies_df[\"rating_category\"] = movies_df[\"rating\"].apply(rating_function)\nmovies_df.head(2)"
 },
 {
  "nbid": "NB_5525227",
  "file": "NB_5525227,auto-1.ipynb",
  "row_id": 9,
  "target_code": "auto = pd.concat([auto, pd.get_dummies(auto['cylinders'           ])], axis = 1)\nauto.head()",
  "generation": "auto = pd.get_dummies(auto, columns=['cylinders'], prefix=['cyl'])\nauto.head()"
 },
 {
  "nbid": "NB_5533785",
  "file": "NB_5533785,data-1.ipynb",
  "row_id": 7,
  "target_code": "data['y'   ] = data['y'   ].map({'yes'     :1, 'no'     :0})\ndata.drop(['Unnamed: 0'            ],axis=1,inplace=True)\ndata.head()",
  "generation": "data['y'] = data['y'].map({'yes': 1, 'no': 0})\ndata.head()"
 },
 {
  "nbid": "NB_5534958",
  "file": "NB_5534958,data-1.ipynb",
  "row_id": 6,
  "target_code": "label_encoder = LabelEncoder()\ndata.income = label_encoder.fit_transform(data.income)\ndata.head()",
  "generation": "label_encoder = LabelEncoder()\ndata['income'] = label_encoder.fit_transform(data['income'])\ndata.head()"
 },
 {
  "nbid": "NB_5539252",
  "file": "NB_5539252,df-1.ipynb",
  "row_id": 3,
  "target_code": "bins= [0,10,20,55,100]\nlabels = ['Kids'      ,'Teen'      ,'Adult'       ,'Old Age'         ]\nAge = pd.cut(df['age'     ], bins=bins, labels=labels, right=False)\ndf[\"Age\"     ]=Age\ndf=df.drop(['age'     ],axis=1)\nbins= [96,98.6,102,110]\nlabels = ['normal'        ,'fever'       ,'high fever'            ]\nTemp = pd.cut(df['body temperature'                  ], bins=bins, labels=labels, right=False)\ndf[\"Temp\"      ]=Temp\ndf=df.drop(['body temperature'                  ],axis=1)\ndf.head()",
  "generation": "le = LabelEncoder()\ndf['Age'] = pd.cut(df['age'], bins=[0, 12, 20, 60, 100], labels=['Child', 'Teen', 'Adult', 'Old Age'])\ndf['Temp'] = pd.cut(df['body temperature'], bins=[0, 99, 100, 103], labels=['normal', 'fever', 'high fever'])\ndf['gender'] = le.fit_transform(df['gender'])\ndf['Dry Cough'] = le.fit_transform(df['Dry Cough'])\ndf['sour throat'] = le.fit_transform(df['sour throat'])\ndf['weakness'] = le.fit_transform(df['weakness'])\ndf['breathing problem'] = le.fit_transform(df['breathing problem'])\ndf['drowsiness'] = le.fit_transform(df['drowsiness'])\ndf['pain in chest'] = le.fit_transform(df['pain in chest'])\ndf['travel history to infected countries'] = le.fit_transform(df['travel history to infected countries'])\ndf['diabetes'] = le.fit_transform(df['diabetes'])\ndf['heart disease'] = le.fit_transform(df['heart disease'])\ndf['lung disease'] = le.fit_transform(df['lung disease'])\ndf['stroke or reduced immunity'] = le.fit_transform(df['stroke or reduced immunity'])\ndf['symptoms progressed'] = le.fit_transform(df['symptoms progressed'])\ndf['high blood pressue'] = le.fit_transform(df['high blood pressue'])\ndf['kidney disease'] = le.fit_transform(df['kidney disease'])\ndf['change in appetide'] = le.fit_transform(df['change in appetide'])\ndf['Loss of sense of smell'] = le.fit_transform(df['Loss of sense of smell'])\ndf['Corona result'] = le.fit_transform(df['Corona result'])\ndf = df[['Sno', 'gender', 'Dry Cough', 'sour throat', 'weakness', 'breathing problem', 'drowsiness', 'pain in chest', 'travel history to infected countries', 'diabetes', 'heart disease', 'lung disease', 'stroke or reduced immunity', 'symptoms progressed', 'high blood pressue', 'kidney disease', 'change in appetide', 'Loss of sense of smell', 'Corona result', 'Age', 'Temp']]\ndf.head()"
 },
 {
  "nbid": "NB_1980584",
  "file": "NB_1980584,df-1.ipynb",
  "row_id": 6,
  "target_code": "df['year'      ] = pd.DatetimeIndex(df['date'      ]).year\ndf.head()",
  "generation": "df['year'] = pd.DatetimeIndex(df['date']).year\ndf"
 },
 {
  "nbid": "NB_1982564",
  "file": "NB_1982564,data-1.ipynb",
  "row_id": 3,
  "target_code": "cols_to_drop = ['Acronym'         , 'Status'         , 'Study Results'                ,\n        'Outcome Measures'                          ,'Sponsor/Collaborators'                       , 'Gender'         , 'Age'      , 'Enrollment'             ,\n       'Funded Bys'                   , 'Study Type'             , 'Study Designs'                , 'Other IDs'            , 'Start Date'             ,\n       'Primary Completion Date'                                , 'Completion Date'                  , 'First Posted'               ,\n       'Results First Posted'                             , 'Last Update Posted'                     ,\n       'Study Documents'                        ]\nfor col in cols_to_drop:\n    data = data.drop([col],axis=1)\ndata.head()",
  "generation": "data = data[['NCT Number', 'Title', 'Condition', 'Interventions', 'Phases', 'Locations', 'URL']]\ndata = data.dropna(subset=['Condition', 'Interventions', 'Phases', 'Locations'])\ndata = data.reset_index(drop=True)\ndata = data.iloc[0:1,:]\ndata"
 },
 {
  "nbid": "NB_1983775",
  "file": "NB_1983775,bitcoin-1.ipynb",
  "row_id": 4,
  "target_code": "bitcoin['prevClose'           ] = list([bitcoin['close'       ].values[0]] + list(bitcoin['close'       ].values))[:-1]\nbitcoin['percentChange'               ] = (bitcoin['close'       ] - bitcoin['prevClose'           ]) / bitcoin['prevClose'           ]\nbitcoin.head(10)",
  "generation": "Since there is no input or output dataframe provided, I cannot write the target code. Please provide the necessary data context for me to proceed."
 },
 {
  "nbid": "NB_1996010",
  "file": "NB_1996010,movies_df-1.ipynb",
  "row_id": 108,
  "target_code": "movies_df['categoria'           ] = movies_df['rating'        ].apply(lambda x: 'Buena'         if x >= 8.0 else 'Mala'       )\nmovies_df.head(3)",
  "generation": "There is no target code provided in this context. Please provide the target code so I can help you transform the input dataframe to the output dataframe."
 },
 {
  "nbid": "NB_2000453",
  "file": "NB_2000453,data-1.ipynb",
  "row_id": 7,
  "target_code": "data=data.sort_values(\"Popularity\"            , ascending=False)\ndata.head(3)",
  "generation": "data['Track' ] = data['Track.Name' ].copy()\ndata['Artist'] = data['Artist.Name'].copy()\ndata = data.drop(['Track.Name', 'Artist.Name'], axis=1)\ndata = data[['Track', 'Artist', 'Genre', 'Beats.Per.Minute', 'Energy', 'Danceability', 'Loudness..dB..', 'Liveness', 'Valence.', 'Length.', 'Acousticness..', 'Speechiness.', 'Popularity']]\ndata.head()"
 },
 {
  "nbid": "NB_2000542",
  "file": "NB_2000542,df-1.ipynb",
  "row_id": 22,
  "target_code": "df['Reasons'         ]=df['title'       ].apply(lambda t: t.split(':'   )[0])\ndf.head(5)",
  "generation": "df['Reasons'] = df['title'].apply(lambda x: x.split(':')[0])\ndf.head()"
 },
 {
  "nbid": "NB_2003461",
  "file": "NB_2003461,df-1.ipynb",
  "row_id": 5,
  "target_code": "df['target'        ] = np.where(df['target'        ]=='spam'      ,1,0)\ndf.head(3)",
  "generation": "df['target'] = np.where(df['target']=='spam', 1, 0)\ndf.head(3)"
 },
 {
  "nbid": "NB_2013727",
  "file": "NB_2013727,scaledPred-1.ipynb",
  "row_id": 99,
  "target_code": "scaledPred = scaledPred.reset_index(drop = True)\nscaledPred.head()",
  "generation": "There is no transformation happening in the provided code. The input dataframe and output dataframe are the same."
 },
 {
  "nbid": "NB_2015567",
  "file": "NB_2015567,data-1.ipynb",
  "row_id": 10,
  "target_code": "data = data.drop(columns=['No'    ])\ndata.head()",
  "generation": "data = data.drop(columns=['No'])\ndata.head()"
 },
 {
  "nbid": "NB_2022690",
  "file": "NB_2022690,new_data-1.ipynb",
  "row_id": 31,
  "target_code": "new_data = new_data.join(data_device_count, on ='device'        )\nnew_data.head()",
  "generation": "data_device_count = pd.DataFrame(data.groupby('device')['device'].count())\nnew_data = data.groupby('device').max()\nnew_data['device_count'] = data_device_count['device']\nnew_data.head()"
 },
 {
  "nbid": "NB_2022690",
  "file": "NB_2022690,new_data-1.ipynb",
  "row_id": 33,
  "target_code": "new_data.date = list(map(lambda x:x[5:7], new_data.date ))\nnew_data.head()",
  "generation": "new_data['date'] = new_data['date'].apply(lambda x: x.split('-')[1])\nnew_data.head()"
 },
 {
  "nbid": "NB_2022760",
  "file": "NB_2022760,gss-1.ipynb",
  "row_id": 8,
  "target_code": "gss = gss.iloc[0:-2,:]\ngss.tail(1)",
  "generation": "gss = gss.iloc[:-2, :]\ngss = gss.replace('Not applicable', np.nan)\ngss = gss.dropna()\ngss = gss.apply(lambda x: pd.factorize(x)[0])\npca = PCA(n_components=2)\npca.fit(gss)\ngss_pca = pca.transform(gss)\ngss_pca = pd.DataFrame(gss_pca, columns=['PC1', 'PC2'])\ngss_pca"
 },
 {
  "nbid": "NB_2026590",
  "file": "NB_2026590,df-1.ipynb",
  "row_id": 6,
  "target_code": "new_columns = ['scheduledday'              , 'appointmentday'                 ]\nfor c in new_columns:\n    df[c]=df[c].apply(lambda x: x[:10])\ndf.head()",
  "generation": "df['scheduledday'] = df['scheduledday'].apply(lambda x: x[:10])\ndf['appointmentday'] = df['appointmentday'].apply(lambda x: x[:10])\ndf.head()"
 },
 {
  "nbid": "NB_2035052",
  "file": "NB_2035052,supply-1.ipynb",
  "row_id": 57,
  "target_code": "supply = supply.fillna(0)\nsupply = supply.rename(columns={'Periods'         :'year'      , 'TotalSupply_1'                :'gas_used_in_the_Netherlands'                             , 'IndigenousProduction_2'                         :'indigenous_production'                       })\nsupply['year'      ] = supply['year'      ].astype(str)\nsupply['year'      ] = [x[:-4] for x in supply['year'      ]]\nsupply['year'      ] = supply['year'      ].astype(int)\nsupply = supply.query('year >= 1959'              )\nsupply['natural_gas_imports'                     ] = supply['ImportsOfGaseousNaturalGas_4'                              ] + supply['ImportsOfLiquefiedNaturalGasLng_5'                                   ]\nsupply['natural_gas_exports'                     ] = supply['ExportsOfGaseousNaturalGas_6'                              ] + supply['ExportsOfLiquefiedNaturalGasLng_7'                                   ]\nsupply.head()",
  "generation": "supply['year'] = supply['Periods'].apply(lambda x: int(x[:4]))\nsupply['gas_used_in_the_Netherlands'] = supply['TotalSupply_1'].apply(lambda x: int(x))\nsupply['indigenous_production'] = supply['IndigenousProduction_2'].apply(lambda x: int(x) if x != '.' else 0)\nsupply['natural_gas_imports'] = supply['ImportsOfGaseousNaturalGas_4'] + supply['ImportsOfLiquefiedNaturalGasLng_5']\nsupply['natural_gas_exports'] = supply['ExportsOfGaseousNaturalGas_6'] + supply['ExportsOfLiquefiedNaturalGasLng_7']\nsupply = supply[['ID', 'year', 'gas_used_in_the_Netherlands', 'indigenous_production', 'ProductionFromOtherSources_3', 'ImportsOfGaseousNaturalGas_4', 'ImportsOfLiquefiedNaturalGasLng_5', 'ExportsOfGaseousNaturalGas_6', 'ExportsOfLiquefiedNaturalGasLng_7', 'StockChange_8', 'natural_gas_imports', 'natural_gas_exports']]\nsupply.head()"
 },
 {
  "nbid": "NB_2045965",
  "file": "NB_2045965,df-1.ipynb",
  "row_id": 3,
  "target_code": "df = df[df.DCOILBRENTEU != \".\"    ]\ndf['Date'      ] = pd.to_datetime(df.DATE)\ndf['Price'       ] = pd.to_numeric(df.DCOILBRENTEU)\ndf.drop(['DATE'      ,'DCOILBRENTEU'              ],axis=1,inplace=True)\ndf.set_index('Date'      )\ndf.head()",
  "generation": "df = df.replace('.', pd.np.nan)\ndf = df.dropna()\ndf['DATE'] = pd.to_datetime(df['DATE'])\ndf['DCOILBRENTEU'] = pd.to_numeric(df['DCOILBRENTEU'])\ndf = df.rename(columns={'DATE': 'Date', 'DCOILBRENTEU': 'Price'})\ndf = df.set_index('Date')\ndf.head()"
 },
 {
  "nbid": "NB_24698",
  "file": "NB_24698,df-1.ipynb",
  "row_id": 5,
  "target_code": "df['Date_Time'           ] = pd.to_datetime(df['Date_Time'           ],format='%d %m %Y %H:%M'                )\ndf['Hour'      ] = df['Date_Time'           ].dt.hour\ndf['Minute'        ] = df['Date_Time'           ].dt.minute\ndf['Day'     ] = df['Date_Time'           ].dt.day\ndf['Month'       ] = df['Date_Time'           ].dt.month\ndf['Year'      ] = df['Date_Time'           ].dt.year\ndf.head()",
  "generation": "df['Date_Time'] = pd.to_datetime(df['Date_Time'], format='%d %m %Y %H:%M')\ndf['Hour'] = df['Date_Time'].dt.hour\ndf['Minute'] = df['Date_Time'].dt.minute\ndf['Day'] = df['Date_Time'].dt.day\ndf['Month'] = df['Date_Time'].dt.month\ndf['Year'] = df['Date_Time'].dt.year\ndf = df[['Date_Time', 'LV_ActivePower(kW)', 'Wind_Speed(m/s)', 'Theoretical_Power_Curve(KWh)', 'Wind_Direction', 'Hour', 'Minute', 'Day', 'Month', 'Year']]\ndf.head()"
 },
 {
  "nbid": "NB_261178",
  "file": "NB_261178,df_inter_te-1.ipynb",
  "row_id": 67,
  "target_code": "df_inter_te = df_inter_te.apply(lambda x: np.log1p(x) if x.name in inter_log_cols else x)\ndf_inter_te.head()",
  "generation": "df_inter_te[inter_log_cols[:-1]] = np.log(df_inter_te[inter_log_cols[:-1]])\ndf_inter_te.head()"
 },
 {
  "nbid": "NB_261178",
  "file": "NB_261178,df_simple_te-1.ipynb",
  "row_id": 40,
  "target_code": "simple_log_cols = ['Gr Liv Area'             ]\ndf_simple_te = df_simple_te.apply(lambda x: np.log1p(x) if x.name in simple_log_cols else x)\ndf_simple_te.head()",
  "generation": "df_simple_te[['Gr Liv Area', 'Overall Qual']] = np.log(df_simple_te[['Gr Liv Area', 'Overall Qual']])\ndf_simple_te.head()"
 },
 {
  "nbid": "NB_5599253",
  "file": "NB_5599253,ir-1.ipynb",
  "row_id": 15,
  "target_code": "ir.columns = [x.replace(\" \"   , \"_\"    ).replace(\"_(cm)\"       , \"\"   ) for x in ir.columns]\nir.head()",
  "generation": "ir.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\nir.head()"
 },
 {
  "nbid": "NB_5700285",
  "file": "NB_5700285,data-1.ipynb",
  "row_id": 4,
  "target_code": "data = data.join(pd.DataFrame({'dist'       : dists}))\ndata.head(n=10)",
  "generation": "from geopy.distance import geodesic\n\ndists = []\nfor i in range(len(data)):\n    dists.append(geodesic((data['lat'][i], data['lon'][i]), (data['plat'][i], data['plon'][i])).km)\n\ndata['dist'] = dists\ndata.head()"
 },
 {
  "nbid": "NB_1613720",
  "file": "NB_1613720,df-1.ipynb",
  "row_id": 5,
  "target_code": "df['start_time'            ] = pd.to_datetime(df['start_time'            ])\ndf['end_time'          ] = pd.to_datetime(df['end_time'          ])\ndf['amount'        ] = df['amount'        ].replace('[\\$,]'       , ''   , regex=True).astype(float)\ndf.head()",
  "generation": "df['amount'] = df['amount'].apply(lambda x: float(x.strip('$')))\ndf.head()"
 },
 {
  "nbid": "NB_1615083",
  "file": "NB_1615083,df-1.ipynb",
  "row_id": 18,
  "target_code": "df = df.drop(labels=columns_to_drop, axis=1)\ndf.head(2)",
  "generation": "df = pd.read_csv('tmdb-movies.csv')\ndf.drop(columns_to_drop, axis=1, inplace=True)\ndf.head(2)"
 },
 {
  "nbid": "NB_1615537",
  "file": "NB_1615537,ch_mort-1.ipynb",
  "row_id": 3,
  "target_code": "ch_mort['Death Rate'            ] = ch_mort['Death Rate'            ].str.replace(','   ,''  ).astype(float)\nch_mort.head()",
  "generation": "ch_mort['Death Rate'] = ch_mort['Death Rate'].str.replace(',', '').astype(float)\nch_mort.head()"
 },
 {
  "nbid": "NB_1615789",
  "file": "NB_1615789,dfr-1.ipynb",
  "row_id": 22,
  "target_code": "dfr['Predicted Tip'               ] = intercept + slope * dfr['x (total_bill)'                ]\ndfr['Residual'          ] = dfr['Predicted Tip'               ] - dfr['y (tip)'         ]\ndfr['RegSq'       ] = np.square(dfr['Predicted Tip'               ] - np.mean(dfr['Predicted Tip'               ]))\ndfr['ResSq'       ] = np.square(dfr['Residual'          ] - np.mean(dfr['Residual'          ]))\ndfr.head(10)",
  "generation": "#Calculate the correlation coefficient\nr = np.corrcoef(dfr['x (total_bill)'], dfr['y (tip)'])[0,1]\n\n#Calculate the standard deviation of x and y\nsd_x = np.std(dfr['x (total_bill)'])\nsd_y = np.std(dfr['y (tip)'])\n\n#Calculate the slope and y-intercept\nslope = ((r * sd_y) / sd_x)\nintercept = (np.mean(dfr['y (tip)']) - (slope * np.mean(dfr['x (total_bill)'])))\n\n#Calculate the predicted tip amount and residuals\ndfr['Predicted Tip'] = (slope * dfr['x (total_bill)']) + intercept\ndfr['Residual'] = dfr['y (tip)'] - dfr['Predicted Tip']\n\n#Calculate the regression and residual sum of squares\ndfr['RegSq'] = np.square(dfr['Predicted Tip'] - np.mean(dfr['y (tip)']))\ndfr['ResSq'] = np.square(dfr['Residual'])\n\n#Print the updated dataframe\ndfr.head()"
 },
 {
  "nbid": "NB_1623040",
  "file": "NB_1623040,df_year-1.ipynb",
  "row_id": 29,
  "target_code": "df_year['rate_percent'              ] = (df_year.Deaths / df_year.Population) * 100\ndf_year.head()",
  "generation": "drug_poison_df['rate_percent'] = (drug_poison_df['Deaths'] / drug_poison_df['Population']) * 100\ndrug_poison_df.head()"
 },
 {
  "nbid": "NB_2078860",
  "file": "NB_2078860,train-1.ipynb",
  "row_id": 42,
  "target_code": "train = train.rename(columns={'MSRP'      : 'price'        })\ntrain = train.dropna()\ntrain.head(5)",
  "generation": "train = train.rename(columns={\"MSRP\": \"price\"})\ntrain = train.dropna()\ntrain.head()"
 },
 {
  "nbid": "NB_2089486",
  "file": "NB_2089486,exam-1.ipynb",
  "row_id": 6,
  "target_code": "exam = exam.set_index([\"\u4ee3\u865f\"    ])\nexam.head(2)",
  "generation": "exam = exam[['\u4f86\u6e90', '\u985e\u5225', '\u5e74\u4efd', '\u984c\u578b', '\u984c\u865f', '\u984c\u76ee\u7c21\u8ff0', '\u7b54\u6848', '\u7ae0\u7bc0', '\u7ae0\u7bc0\uff08\u77ed\uff09', 'P', 'Ph', 'Pm', 'Pl', 'P90', 'P70', 'P50', 'P30', 'P10', 'T', 'D', 'D1', 'D2', 'D3', 'D4', 'TA', 'TB', 'TC', 'TD', 'TE', 'HA', 'HB', 'HC', 'HD', 'HE', 'LA', 'LB', 'LC', 'LD', 'LE']]\nexam.head(2)"
 },
 {
  "nbid": "NB_2093996",
  "file": "NB_2093996,res-1.ipynb",
  "row_id": 52,
  "target_code": "res = df_cast_splitted.groupby(['actor'       ], as_index=False)['id'    ].count().sort_values(by='id'    , ascending=False).head(10)\nres = res.rename(columns={'id'    :'total_movies'              })\nres",
  "generation": "df_cast_splitted = df_cast_splitted.explode('actor')\ndf_cast_splitted.groupby('actor').count().sort_values(by='id', ascending=False).head(5)"
 },
 {
  "nbid": "NB_2093996",
  "file": "NB_2093996,res-1.ipynb",
  "row_id": 56,
  "target_code": "res = df_genres_splitted.groupby(['genre'       ], as_index=False)['id'    ].count().sort_values(by='id'    , ascending=False).head(10)\nres = res.rename(columns={'id'    :'total_used'            })\nres",
  "generation": "res = df_genres_splitted.groupby(['genre'], as_index=False)['id'].count().sort_values(by='id', ascending=False).head(10)\nres = res.rename(columns={'id':'total_movies'})\nres"
 },
 {
  "nbid": "NB_2096317",
  "file": "NB_2096317,df-1.ipynb",
  "row_id": 6,
  "target_code": "df = df[['Exited_INDEXED'                , 'target_prediction'                    , 'target_gt'            , 'Age'      , 'Balance'          , 'NumOfProducts'                ]]\ndf = df.sort_values('Exited_INDEXED'                )\ndf.head()",
  "generation": "df_sorted = df.sort_values(by='target_prediction')\ndf_sorted = df_sorted[['Exited_INDEXED', 'target_prediction', 'target_gt', 'Age', 'Balance', 'NumOfProducts']]\ndf_sorted.head()"
 },
 {
  "nbid": "NB_2100228",
  "file": "NB_2100228,tmdb_data-1.ipynb",
  "row_id": 8,
  "target_code": "del_col=[ 'id'     , 'imdb_id'          , 'popularity'             , 'budget_adj'             , 'revenue_adj'              , 'homepage'           , 'keywords'           , 'overview'           , 'production_companies'                       , 'vote_count'             , 'vote_average'               ]\ntmdb_data= tmdb_data.drop(del_col,1)\ntmdb_data.head(4)",
  "generation": "tmdb_data.drop(['id', 'imdb_id', 'popularity', 'budget_adj', 'revenue_adj', 'homepage', 'keywords', 'overview', 'production_companies', 'vote_count', 'vote_average'], axis=1, inplace=True)\ntmdb_data.drop_duplicates(inplace=True)\ntmdb_data.dropna(subset=['budget', 'revenue'], how='any', inplace=True)\ntmdb_data['release_date'] = pd.to_datetime(tmdb_data['release_date'])\ntmdb_data['runtime'].replace(0, np.nan, inplace=True)\ntmdb_data['budget'] = tmdb_data['budget'].apply(lambda x: '${:,.2f}'.format(x))\ntmdb_data['revenue'] = tmdb_data['revenue'].apply(lambda x: '${:,.2f}'.format(x))\ntmdb_data.head()"
 },
 {
  "nbid": "NB_291632",
  "file": "NB_291632,df-1.ipynb",
  "row_id": 15,
  "target_code": "df = df.drop(nan_index)\ndf.reset_index()\ndf.head()",
  "generation": "df = df.drop(nan_index).reset_index(drop=True)\ndf.head()"
 },
 {
  "nbid": "NB_4692600",
  "file": "NB_4692600,features-1.ipynb",
  "row_id": 4,
  "target_code": "from sklearn.preprocessing import LabelEncoder\ndependent_variable = df['koi_disposition'                 ]\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(dependent_variable)\ndf['koi_disposition_enc'                     ] = label_encoder.transform(dependent_variable)\nfeatures.head()",
  "generation": "le = LabelEncoder()\nfeatures['koi_disposition'] = le.fit_transform(features['koi_disposition'])\nfeatures.head()"
 },
 {
  "nbid": "NB_4694105",
  "file": "NB_4694105,data-1.ipynb",
  "row_id": 8,
  "target_code": "le = preprocessing.LabelEncoder()\ndata = data.drop(data.index[data['num_pages'           ] >= 1000])\ndata = data.drop(data.index[data['ratings_count'               ] >= 1000000])\ndata = data.drop(data.index[data['text_reviews_count'                    ] >= 20000])\ndata['title'       ] = le.fit_transform(data['title'       ])\ndata['authors'         ] = le.fit_transform(data['authors'         ])\ndata['publication_date'                  ] = le.fit_transform(data['publication_date'                  ])\ndata['publisher'           ] = le.fit_transform(data['publisher'           ])\ndata.head()",
  "generation": "le = preprocessing.LabelEncoder()\ndata['title'] = le.fit_transform(data['title'])\ndata['authors'] = le.fit_transform(data['authors'])\ndata['publication_date'] = le.fit_transform(data['publication_date'])\ndata['publisher'] = le.fit_transform(data['publisher'])\ndata['language_code'] = le.fit_transform(data['language_code'])\ndata.head()"
 },
 {
  "nbid": "NB_4694105",
  "file": "NB_4694105,test-1.ipynb",
  "row_id": 11,
  "target_code": "le = preprocessing.LabelEncoder()\ntest['title'       ] = le.fit_transform(test['title'       ])\ntest['authors'         ] = le.fit_transform(test['authors'         ])\ntest['publication_date'                  ] = le.fit_transform(test['publication_date'                  ])\ntest['publisher'           ] = le.fit_transform(test['publisher'           ])\ntest = test.drop(['language_code'               , 'isbn'       , 'isbn13'         ], axis = 1)\ntest.head()",
  "generation": "enc_lang = pd.get_dummies(test['language_code'])\ntest = test.drop(['isbn', 'isbn13', 'language_code', 'title', 'authors', 'ratings_count', 'text_reviews_count', 'publication_date', 'publisher'], axis=1)\ntest = pd.concat([test, enc_lang], axis = 1)\ntest.tail()"
 },
 {
  "nbid": "NB_1488288",
  "file": "NB_1488288,sf-1.ipynb",
  "row_id": 2,
  "target_code": "sf['zindexvalue'             ] = sf['zindexvalue'             ].replace(','   , ''   )\nsf.lastsolddate.min(), sf.lastsolddate.max()\nsf.head()",
  "generation": "sf = sf[['address', 'bathrooms', 'bedrooms', 'finishedsqft', 'lastsolddate', 'lastsoldprice', 'latitude', 'longitude', 'neighborhood', 'totalrooms', 'usecode', 'yearbuilt', 'zindexvalue']]"
 },
 {
  "nbid": "NB_1490746",
  "file": "NB_1490746,for_goods-1.ipynb",
  "row_id": 47,
  "target_code": "for_goods['total'       ] = for_goods.sum(axis=1)\nfor_goods.head(1)",
  "generation": "for_goods['total'] = for_goods.sum(axis=1)\nfor_goods"
 },
 {
  "nbid": "NB_1491297",
  "file": "NB_1491297,tmdb_file-1.ipynb",
  "row_id": 6,
  "target_code": "del_col = ['imdb_id'         , 'budget'         , 'revenue'          , 'homepage'           , 'keywords'           , 'overview'           , 'production_companies'                       , 'tagline'          ]\ntmdb_file = tmdb_file.drop(del_col,1)\ntmdb_file.head(3)",
  "generation": "tmdb_file.drop(['imdb_id', 'homepage', 'keywords', 'overview', 'production_companies'], axis=1, inplace=True)\ntmdb_file = tmdb_file[tmdb_file.budget != 0]\ntmdb_file = tmdb_file[tmdb_file.revenue != 0]\ntmdb_file.dropna(inplace=True)\ntmdb_file.drop_duplicates(inplace=True)\ntmdb_file.reset_index(drop=True, inplace=True)\ntmdb_file.head()"
 },
 {
  "nbid": "NB_1493270",
  "file": "NB_1493270,sleep_Data-1.ipynb",
  "row_id": 8,
  "target_code": "to_drop = ['pitchs'        ]\nsleep_Data.drop(to_drop, inplace=True, axis=1)\nsleep_Data['id'    ].is_unique\nsleep_Data = sleep_Data.set_index('id'    )\nsleep_Data.head()",
  "generation": "sleep_Data.drop(['pitchs'], axis=1, inplace=True)\nsleep_Data = sleep_Data.rename(columns={'yaw': 'yawns'})\nsleep_Data = sleep_Data[['image', 'time', 'initial_time', 'pitch', 'yawns', 'roll', 'tag']]\nsleep_Data.head()"
 },
 {
  "nbid": "NB_1493899",
  "file": "NB_1493899,df-1.ipynb",
  "row_id": 9,
  "target_code": "df['sex'     ] = df['sex'     ].map({' Male'       :1,' Female'         :0}).astype(int)\ndf.head(10)",
  "generation": "df['sex']=df['sex'].map({' Male':1,' Female':0}).astype(int)\ndf.head()"
 },
 {
  "nbid": "NB_1493899",
  "file": "NB_1493899,df-1.ipynb",
  "row_id": 16,
  "target_code": "print(df.shape)\ndf['workclass'           ] = df['workclass'           ].replace(' ?'    ,np.nan)\ndf['occupation'            ] = df['occupation'            ].replace(' ?'    ,np.nan)\ndf.dropna(how='any'     ,inplace=True)\nprint(df.shape)\ndf.head(10)",
  "generation": "df = pd.read_csv(\"./large_train_sample.csv\",1,\",\")\ndf = df.rename(index=str, columns={\"native-country\": \"country\", \"wage\": \"salary\"})\nsalary_map={' <=50K':1,' >50K':0}\ndf['salary']=df['salary'].map(salary_map).astype(int)\ndf['sex'] = df['sex'].map({' Male':1,' Female':0}).astype(int)\ndf = df[df.occupation != '?']"
 },
 {
  "nbid": "NB_1494525",
  "file": "NB_1494525,df_car-1.ipynb",
  "row_id": 3,
  "target_code": "df_car = df_car.drop('Unnamed: 0'            , axis=1)\ndf_car.head()",
  "generation": "df_car = df_car[['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', 'drive-wheels', 'engine-location', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type', 'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-L/100km', 'highway-L/100km', 'price', 'price-binned']]\ndf_car.head()"
 },
 {
  "nbid": "NB_1498706",
  "file": "NB_1498706,df-1.ipynb",
  "row_id": 5,
  "target_code": "df['Date'      ] = pd.period_range(start='2009-01'         , end='2019-12'         , freq='M'   )\ndf.index = df['Date'      ]\ndf = df.drop(['Year'      , 'Month'        , 'Date'       ], axis=1)\ndf.Sales = df.Sales.astype('float64'         ) \ndf.head()",
  "generation": "df['Sales']"
 },
 {
  "nbid": "NB_1503632",
  "file": "NB_1503632,df-1.ipynb",
  "row_id": 8,
  "target_code": "df = expand_gen(df)\ndf.head()",
  "generation": "df = df.drop('japanese_name', 1)\ndf = df.drop('classfication', 1)\noha = expand_abilities(df)\ndf = df.drop('abilities', 1)\ndf = append_df(oha, df)\ndf = one_hot_types(df)\ndf.head()"
 },
 {
  "nbid": "NB_1503632",
  "file": "NB_1503632,df-1.ipynb",
  "row_id": 10,
  "target_code": "df = create_weakness(df)\ndf.head()",
  "generation": "pca = PCA(n_components=2)\npca.fit(df)\ndf_pca = pca.transform(df)\ndf_pca = pd.DataFrame(df_pca, columns=['PC1', 'PC2'])\ndf_pca.head()"
 },
 {
  "nbid": "NB_1505741",
  "file": "NB_1505741,df-1.ipynb",
  "row_id": 4,
  "target_code": "header = df['Herkunftsland'               ]\ndf = df.transpose()\ndf.head(3)",
  "generation": "header = df.columns[0]\ndf = pd.melt(df, id_vars=[header], var_name=\"country\", value_name=\"nights\")\ndf.head()"
 },
 {
  "nbid": "NB_172969",
  "file": "NB_172969,rotten_reviews_Polarity-1.ipynb",
  "row_id": 8,
  "target_code": "rotten_reviews_Polarity['Recommended IND'                 ] = [1 if x >= 0 else 0 for x in rotten_reviews_Polarity['polarity'          ]] \nrotten_reviews_Polarity.head()",
  "generation": "rotten_reviews_Polarity['Recommended IND'] = (rotten_reviews_Polarity['polarity'] > 0).astype(int)\nrotten_reviews_Polarity.head()"
 },
 {
  "nbid": "NB_176042",
  "file": "NB_176042,subway-1.ipynb",
  "row_id": 7,
  "target_code": "subway['label'       ] = (subway['Min Delay'           ] > 5).astype(int)\nsubway.head()",
  "generation": "subway['label'] = (subway['dayofweek'] < 5) & (subway['Time'] >= '07:00:00') & (subway['Time'] <= '10:00:00')\nsubway['label'] = subway['label'].astype(int)\nsubway.head()"
 },
 {
  "nbid": "NB_176152",
  "file": "NB_176152,calls_df-1.ipynb",
  "row_id": 23,
  "target_code": "calls_df['Reason'        ] = calls_df['title'       ].apply(lambda title: title.split(':'   )[0])\ncalls_df.head()",
  "generation": "calls_df['Reason'] = calls_df['title'].apply(lambda x: x.split(':')[0])\ncalls_df.head()"
 },
 {
  "nbid": "NB_4169659",
  "file": "NB_4169659,test_df-1.ipynb",
  "row_id": 17,
  "target_code": "test_df['label1'        ] = np.where(test_df['RUL'     ] <= w1, 1, 0 )\ntest_df['label2'        ] = test_df['label1'        ]\ntest_df.loc[test_df['RUL'     ] <= w0, 'label2'         ] = 2\ntest_df.head()",
  "generation": "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0)\ntest_df['label2'] = test_df['label1']"
 },
 {
  "nbid": "NB_4169659",
  "file": "NB_4169659,train_df-1.ipynb",
  "row_id": 9,
  "target_code": "rul = pd.DataFrame(train_df.groupby('id'    )['cycle'       ].max()).reset_index()\nrul.columns = ['id'    , 'max'      ]\ntrain_df = train_df.merge(rul, on=['id'    ], how='left'      )\ntrain_df['RUL'     ] = train_df['max'     ] - train_df['cycle'       ]\ntrain_df.drop('max'     , axis=1, inplace=True)\ntrain_df.head()",
  "generation": "train_df['RUL'] = train_df.groupby(['id'])['cycle'].transform(max)-train_df['cycle']\ntrain_df"
 },
 {
  "nbid": "NB_4169659",
  "file": "NB_4169659,train_df-1.ipynb",
  "row_id": 10,
  "target_code": "w1 = 30\nw0 = 24\ntrain_df['label1'        ] = np.where(train_df['RUL'     ] <= w1, 1, 0 )\ntrain_df['label2'        ] = train_df['label1'        ]\ntrain_df.loc[train_df['RUL'     ] <= w0, 'label2'         ] = 2\ntrain_df.head()",
  "generation": "train_df['label1'] = np.where(train_df['RUL'] <= 30, 1, 0 )\ntrain_df['label2'] = train_df['label1']\ncols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\nmin_max_scaler = preprocessing.MinMaxScaler()\nnorm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n                             columns=cols_normalize, \n                             index=train_df.index)\njoin_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\ntrain_df = join_df.reindex(columns = train_df.columns)\ntrain_df.to_csv('data/train_01.csv', index = None)"
 },
 {
  "nbid": "NB_4177737",
  "file": "NB_4177737,df-1.ipynb",
  "row_id": 10,
  "target_code": "df = df.drop(['poutcome'          ], axis=1)\ndf.head()",
  "generation": "df.drop('poutcome', axis=1, inplace=True)\ndf.y = df.y.map(yes_no_dict)\ndf.head()"
 },
 {
  "nbid": "NB_4177737",
  "file": "NB_4177737,df-1.ipynb",
  "row_id": 14,
  "target_code": "df.month = df.month.apply(lambda x: month_dict[x])\ndf.head()",
  "generation": "df['month'] = df['month'].map(month_dict)\ndf['y'] = df['y'].map(yes_no_dict)\ndf.head()"
 },
 {
  "nbid": "NB_4179985",
  "file": "NB_4179985,data-1.ipynb",
  "row_id": 14,
  "target_code": "data['diagnosis'           ] = data['diagnosis'           ].map({'M'   :1, 'B'    :0})\ndata.head()",
  "generation": "data['id'] = data['id'].astype(float)\ndata['diagnosis'] = np.where(data['diagnosis'] == 'M', 1, 0)\ndata = data.set_index('id')\ndata.head()"
 },
 {
  "nbid": "NB_4185664",
  "file": "NB_4185664,dataset-1.ipynb",
  "row_id": 5,
  "target_code": "x = dataset.values\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndataset = pd.DataFrame(x_scaled,columns=cols)\ndataset.head()",
  "generation": "dataset['Sleep duration'] = preprocessing.scale(dataset['Sleep duration'])\ndataset.head()"
 },
 {
  "nbid": "NB_4190059",
  "file": "NB_4190059,df-1.ipynb",
  "row_id": 2,
  "target_code": "from io import StringIO\ncol = ['category'          , 'label'        , 'text'       ]\ndf = df[col]\ndf = df[pd.notnull(df['text'      ])]\ndf['category_id'             ] = df['category'          ]\ncategory_id_df = df[['category'          , 'label'        ]].drop_duplicates().sort_values('label'       )\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['label'       , 'category'           ]].values)\ndf.head()",
  "generation": "df = df.drop(df.columns[[1, 2, 3]], axis=1)\ndf = df.rename(columns={df.columns[0]: \"message\", df.columns[1]: \"category\"})\ndf[\"message\"] = df[\"message\"].apply(lambda x: \" \".join(deepcut.tokenize(x)))\ndf = df.groupby(\"category\").apply(lambda x: x.sample(n=1, random_state=1)).reset_index(drop=True)\ndf.head(2)"
 },
 {
  "nbid": "NB_4199051",
  "file": "NB_4199051,res_model-1.ipynb",
  "row_id": 9,
  "target_code": "res_model.dropna(how=\"all\"     )\nres_model.drop_duplicates()\nres_model = res_model.fillna(0)\nres_model.head()",
  "generation": "res_model = res_model.fillna(0)\nres_model = res_model.drop_duplicates()\nres_model.head()"
 },
 {
  "nbid": "NB_8825583",
  "file": "NB_8825583,data-1.ipynb",
  "row_id": 21,
  "target_code": "data['THI'     ] = data.Temp\u00e9rature - ((0.55 - 0.0055*data.Humidit\u00e9) * (data.Temp\u00e9rature -14.5))\ndata.head()",
  "generation": "data['Temp\u00e9rature'] = data['Temp\u00e9rature'] - 273.15\ndata['THI'] = data['Temp\u00e9rature'] - ((0.55 - 0.0055*data['Humidit\u00e9']) * (data['Temp\u00e9rature'] - 14.5))\ndata.head()"
 },
 {
  "nbid": "NB_1513949",
  "file": "NB_1513949,fb_df-1.ipynb",
  "row_id": 3,
  "target_code": "fb_df.info()\nfb_df['Date'      ] = pd.to_datetime(fb_df['Date'      ])\nfb_df.index = fb_df['Date'      ]\nfb_df = fb_df.sort_index()\nfb_df.info()\nfb_df.head()",
  "generation": "fb_df[\"Date\"] = pd.to_datetime(fb_df[\"Date\"])\nfb_df.head()"
 },
 {
  "nbid": "NB_1516435",
  "file": "NB_1516435,df-1.ipynb",
  "row_id": 3,
  "target_code": "df=df.drop(df.columns[df.columns.str.contains('unnamed:'          ,case = False)],axis = 1)\ndf.dtypes\ndf.head()",
  "generation": "df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27', 'Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31', 'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_1516435",
  "file": "NB_1516435,df-1.ipynb",
  "row_id": 4,
  "target_code": "df[\"FECHA\"       ] = df[\"DATE\"      ] + \" \"    + df[\"TIME\"      ]\ndf.drop( [\"DATE\"      ,\"TIME\"      ], axis=1, inplace=True )\ndf.head()",
  "generation": "df['FECHA'] = df['DATE'] + ' ' + df['TIME']\ndf = df.drop(['DATE', 'TIME'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_1516435",
  "file": "NB_1516435,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.drop(df.index[0]).reset_index(drop=True)\ndf.head()",
  "generation": "df.drop(0, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_1518283",
  "file": "NB_1518283,df-1.ipynb",
  "row_id": 8,
  "target_code": "df.Response = df.Response.apply(lambda X : 0 if X == 'No'      else 1)\ndf.columns = df.columns.str.replace(' '   ,'_'   )\ndf.head()",
  "generation": "df['Response'] = df['Response'].replace({'No': 0, 'Yes': 1})\ndf = df.rename(columns=lambda x: x.replace(' ', '_'))\ndf.head()"
 },
 {
  "nbid": "NB_1519631",
  "file": "NB_1519631,bicycle-1.ipynb",
  "row_id": 182,
  "target_code": "bicycle['Total'       ] = bicycle['East'      ]+bicycle['West'      ]\nbicycle.head(3)",
  "generation": "bicycle['Total'] = bicycle['Fremont Bridge East Sidewalk'] + bicycle['Fremont Bridge West Sidewalk']\nbicycle = bicycle[['Fremont Bridge East Sidewalk', 'Fremont Bridge West Sidewalk', 'Total']]\nbicycle.head()"
 },
 {
  "nbid": "NB_1520561",
  "file": "NB_1520561,unemp-1.ipynb",
  "row_id": 8,
  "target_code": "unemp.loc[:, 'unemployment_rate'                    ] = (\n    unemp.loc[:, 'unemployment_rate'                    ].str.replace('%'   ,''  ).apply(float)\n)\nunemp.head()",
  "generation": "unemp['year_quarter'] = pd.to_datetime(unemp['Quarter']).dt.to_period('Q')\nunemp['unemployment_rate'] = unemp['Seasonally adjusted quarterly U.S. unemployment rates from 1948 to 1993'].str.rstrip('%').astype('float') / 100.0\nunemp = unemp[['year_quarter', 'unemployment_rate']]\nunemp.head()"
 },
 {
  "nbid": "NB_1520561",
  "file": "NB_1520561,unemp-1.ipynb",
  "row_id": 10,
  "target_code": "unemp.loc[:, 'date'       ] = pd.to_datetime(unemp.loc[:, 'year_quarter'               ]).dt.to_period('Q'   )\nunemp.set_index('date'      , inplace=True)\nunemp.head()",
  "generation": "unemp['year_quarter'] = pd.PeriodIndex(unemp['year_quarter'], freq='Q')\nunemp.set_index('year_quarter', inplace=True)\nunemp.head()"
 },
 {
  "nbid": "NB_1525566",
  "file": "NB_1525566,df_CfDs-1.ipynb",
  "row_id": 7,
  "target_code": "df_CfDs = df_CfDs.rename(columns={'Capacity\\n(MW)'                :'Capacity (MW)'               })\ncols_2_convert = ['Capacity (MW)'               , 'Homes Powered'                ]\nfor col in cols_2_convert:\n    df_CfDs[col] = (df_CfDs\n                    [col]\n                    .str.replace(','   , ''   )\n                    .astype(float)\n                   )\ndf_CfDs.head()",
  "generation": "import pandas as pd\ndf_CfDs['Capacity (MW)'] = df_CfDs['Capacity\\n(MW)']\ndf_CfDs.drop('Capacity\\n(MW)', axis=1, inplace=True)\ndf_CfDs.head()"
 },
 {
  "nbid": "NB_1525566",
  "file": "NB_1525566,df_CfDs-1.ipynb",
  "row_id": 11,
  "target_code": "s_commissioned = ((df_CfDs['Year End'          ] + df_CfDs['Year Start'            ])/2).round()\ndf_CfDs['capture_price'               ] = s_pred.loc[s_commissioned].values\ndf_CfDs['effective_subsidy'                   ] = df_CfDs['Strike Price (\u00a3/MWh)'                      ] - df_CfDs['capture_price'               ]\ndf_CfDs.head()",
  "generation": "df_CfDs['capture_price'] = s_pred.loc[df_CfDs['Year Start'], 'price'].values\ndf_CfDs['effective_subsidy'] = df_CfDs['Strike Price (\u00a3/MWh)'] - df_CfDs['capture_price']\ndf_CfDs.head()"
 },
 {
  "nbid": "NB_1530093",
  "file": "NB_1530093,mpg_df-1.ipynb",
  "row_id": 5,
  "target_code": "mpg_df = pd.get_dummies(mpg_df, columns=['origin'        ])\nmpg_df.head()",
  "generation": "mpg_df = pd.get_dummies(mpg_df, columns=['origin'], prefix=['origin'])\nmpg_df.head()"
 },
 {
  "nbid": "NB_1532696",
  "file": "NB_1532696,berri_bikes-1.ipynb",
  "row_id": 28,
  "target_code": "berri_bikes.loc[:, u'D\u00eda de la semana'                    ] = berri_bikes.index.weekday\nberri_bikes.head()",
  "generation": "berri_bikes['D\u00eda de la semana'] = berri_bikes.index.weekday\nberri_bikes.head()"
 },
 {
  "nbid": "NB_1532806",
  "file": "NB_1532806,year_df-1.ipynb",
  "row_id": 40,
  "target_code": "for name, column in year_df.items():\n    year_df[name] = (year_df[name] - year_df[name].mean())/year_df[name].std(ddof=0)\nyear_df.head()",
  "generation": "year_df = (year_df - year_df.mean()) / year_df.std()\nyear_df.head()"
 },
 {
  "nbid": "NB_1537255",
  "file": "NB_1537255,TMDb_data-1.ipynb",
  "row_id": 7,
  "target_code": "import numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\ndel_col=[ 'id'     , 'imdb_id'          , 'popularity'             , 'budget_adj'             , 'revenue_adj'              , 'homepage'           , 'keywords'           , 'overview'           , 'production_companies'                       , 'vote_count'             , 'vote_average'               ]\nTMDb_data= TMDb_data.drop(del_col,1)\nTMDb_data.head(5)",
  "generation": "TMDb_data.drop(['id', 'imdb_id', 'budget_adj', 'revenue_adj', 'homepage', 'keywords', 'overview', 'production_companies', 'tagline', 'vote_count', 'vote_average', 'popularity'], axis=1, inplace=True)\nTMDb_data['release_date'] = pd.to_datetime(TMDb_data['release_date'])\nTMDb_data['budget'] = TMDb_data['budget'].apply(lambda x: '${:,.2f}'.format(x))\nTMDb_data['revenue'] = TMDb_data['revenue'].apply(lambda x: '${:,.2f}'.format(x))\nTMDb_data['profit'] = TMDb_data['revenue'] - TMDb_data['budget']\nTMDb_data = TMDb_data[TMDb_data['profit'] != 0]\nTMDb_data = TMDb_data[TMDb_data['budget'] != '$0.00']\nTMDb_data = TMDb_data[TMDb_data['revenue'] != '$0.00']\nTMDb_data = TMDb_data[TMDb_data['runtime'] != 0]\nTMDb_data = TMDb_data.dropna()\nTMDb_data = TMDb_data.reset_index(drop=True)\nTMDb_data = TMDb_data[['budget', 'revenue', 'original_title', 'cast', 'director', 'overview', 'runtime', 'genres', 'release_date']]\nTMDb_data['release_year'] = TMDb_data['release_date'].dt.year\nTMDb_data = TMDb_data.drop(['release_date'], axis=1)\nTMDb_data.head()"
 },
 {
  "nbid": "NB_1539310",
  "file": "NB_1539310,data-1.ipynb",
  "row_id": 7,
  "target_code": "data['mpg'     ] = 235 / data['mpg'     ]\ndata.rename(columns={'mpg'     :'l/100 km'          }, inplace=True)\ndata['displacement'              ] = 16 * data['displacement'              ]\ndata.rename(columns={'displacement'              :'ccm'     }, inplace=True)\ndata['weight'        ] = data['weight'        ] / 2.5\ndata.rename(columns={'weight'        :'weight in kg'              }, inplace=True)\ndata['acceleration'              ] = data['acceleration'              ] / 2.1\ndata.rename(columns={'acceleration'              :'0 - 100'         }, inplace=True)\ndata = data.dropna()\ndata = data[data.horsepower != '?'    ]\ndata['horsepower'            ] = data['horsepower'            ].astype('int64'       )\ndata.head()",
  "generation": "data['l/100 km'] = 235/data['mpg']\ndata['ccm'] = data['displacement']*16\ndata['weight in kg'] = data['weight']*0.453592\ndata['horsepower'] = pd.to_numeric(data['horsepower'], errors='coerce')\ndata = data.drop(['model year', 'origin'], axis=1)\ndata['0 - 100'] = 1/data['acceleration']*60*60/1000\ndata.head(5)"
 },
 {
  "nbid": "NB_1540232",
  "file": "NB_1540232,dados_plot-1.ipynb",
  "row_id": 9,
  "target_code": "dados_plot = pd.melt(dados_plot, id_vars=\"diagnostico\"             ,\n                    var_name=\"exames\"        ,\n                    value_name='valores'         )\ndados_plot.head()",
  "generation": "data_melt = pd.melt(valores_exames_v1, id_vars=None, value_vars=valores_exames_v1.columns, var_name='exames', value_name='valores')\ndados_plot = pd.concat([diagnostico, data_melt],axis = 1)\ndados_plot = dados_plot[['diagnostico', 'exames', 'valores']]\ndados_plot.head()"
 },
 {
  "nbid": "NB_1540475",
  "file": "NB_1540475,new_clt_climate_df-1.ipynb",
  "row_id": 8,
  "target_code": "new_clt_climate_df['DATE'      ] = pd.to_datetime(new_clt_climate_df['DATE'      ])\nnew_clt_climate_df.set_index('DATE'      , inplace=True)\nnew_clt_climate_df.index = new_clt_climate_df.index.normalize()\nnew_clt_climate_df.head()",
  "generation": "new_clt_climate_df = new_clt_climate_df.set_index('DATE')"
 },
 {
  "nbid": "NB_1540475",
  "file": "NB_1540475,new_clt_climate_df-1.ipynb",
  "row_id": 9,
  "target_code": "new_clt_climate_df = new_clt_climate_df.dropna()\nnew_clt_climate_df['DAILYPrecip'             ].replace(['T'   ], '0'    , inplace=True)\nnew_clt_climate_df[['DAILYPrecip'             ]] = new_clt_climate_df[['DAILYPrecip'             ]].apply(pd.to_numeric)\nnew_clt_climate_df = new_clt_climate_df.rename(columns={'DAILYMaximumDryBulbTemp'                         : 'DailyMaxTemp'               , 'DAILYMinimumDryBulbTemp'                          : 'DailyMinTemp'               , 'DAILYAverageDryBulbTemp'                          : 'DailyAvgTemp'               , 'DAILYAverageRelativeHumidity'                               : 'DailyAvgRelHumidity'                      , 'DAILYAverageDewPointTemp'                           : 'DailyAvgDewPointTemp'                       , 'DAILYPrecip'              :'DailyPrecip'             })\nnew_clt_climate_df.head()",
  "generation": "new_clt_climate_df.dropna(inplace=True)\nnew_clt_climate_df['DAILYPrecip'] = new_clt_climate_df['DAILYPrecip'].replace('T', 0).astype('float64')\nnew_clt_climate_df.rename(columns={'DAILYMaximumDryBulbTemp': 'DailyMaxTemp',\n                                   'DAILYMinimumDryBulbTemp': 'DailyMinTemp',\n                                   'DAILYAverageDryBulbTemp': 'DailyAvgTemp',\n                                   'DAILYAverageRelativeHumidity': 'DailyAvgRelHumidity',\n                                   'DAILYAverageDewPointTemp': 'DailyAvgDewPointTemp',\n                                   'DAILYPrecip': 'DailyPrecip'}, inplace=True)\nnew_clt_climate_df.head()"
 },
 {
  "nbid": "NB_1544024",
  "file": "NB_1544024,admissions-1.ipynb",
  "row_id": 7,
  "target_code": "admissions.drop('Serial No.'            , axis=1, inplace=True)\nadmissions.loc[admissions['Chance of Admit '                  ]>=0.72,'Admitted'          ]=1\nadmissions.loc[admissions['Chance of Admit '                  ]<0.72,'Admitted'          ]=0\nadmissions['Admitted'          ]=admissions['Admitted'          ].astype(np.int64)\nadmissions.head()",
  "generation": "admissions['Admitted'] = admissions['Chance of Admit '].apply(lambda x: 1 if x >= 0.75 else 0)\nadmissions.drop(['Chance of Admit '], axis=1, inplace=True)\nadmissions.head()"
 },
 {
  "nbid": "NB_1545525",
  "file": "NB_1545525,df_loan_records-1.ipynb",
  "row_id": 9,
  "target_code": "df_loan_records = df_loan_records[[col for col in list(df_loan_records) if len(df_loan_records[col].unique()) > 1]]\nprint(df_loan_records.shape)\ndf_loan_records.head()",
  "generation": "df_loan_records = df_loan_records.loc[:, df_loan_records.apply(pd.Series.nunique) != 1]\ndf_loan_records.head()"
 },
 {
  "nbid": "NB_1545525",
  "file": "NB_1545525,df_loan_records-1.ipynb",
  "row_id": 15,
  "target_code": "df_loan_records.drop(columns=['url'     ],inplace=True)\ndf_loan_records.drop(columns=['loan_amnt'           ],inplace=True)\ndf_loan_records.drop(columns=['funded_amnt_inv'                 , 'out_prncp_inv'                , 'total_pymnt_inv'                  ],inplace=True)\ndf_loan_records['zip_code'          ] = df_loan_records['zip_code'          ].str.strip('x'   )\ndf_loan_records.drop(columns=['zip_code'          ],inplace=True)\ndf_loan_records.head()",
  "generation": "df_loan_records = df_loan_records[['id', 'member_id', 'loan_amnt', 'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership', 'annual_inc', 'verification_status', 'issue_d', 'loan_status', 'purpose', 'title', 'addr_state', 'dti', 'delinq_2yrs', 'earliest_cr_line', 'inq_last_6mths', 'mths_since_last_delinq', 'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'out_prncp', 'total_pymnt', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d']]\ndf_loan_records"
 },
 {
  "nbid": "NB_1545782",
  "file": "NB_1545782,df-1.ipynb",
  "row_id": 13,
  "target_code": "df['salary'        ] = df['salary'        ].apply(lambda x: 0 if x == '<=80K'         else 1)\ndf.head()",
  "generation": "df['salary'] = df['salary'].apply(lambda x: 0 if x == '<=80K' else 1)\ndf.head()"
 },
 {
  "nbid": "NB_1546129",
  "file": "NB_1546129,data-1.ipynb",
  "row_id": 7,
  "target_code": "data[\"year\"      ]=data[\"Date_of_Journey\"                 ].apply(lambda date: date.year)\ndata[\"month\"       ]=data[\"Date_of_Journey\"                 ].apply(lambda date: date.month)\ndata[\"day\"     ]=data[\"Date_of_Journey\"                 ].apply(lambda date: date.day)\ndata.drop([\"Date_of_Journey\"                 ],axis=1,inplace=True)\ndata.head()",
  "generation": "data['year'] = data['Date_of_Journey'].dt.year\ndata['month'] = data['Date_of_Journey'].dt.month\ndata['day'] = data['Date_of_Journey'].dt.day\ndata.drop(columns=['Date_of_Journey'], inplace=True)\ndata.head()"
 },
 {
  "nbid": "NB_1546129",
  "file": "NB_1546129,data-1.ipynb",
  "row_id": 8,
  "target_code": "data[\"Arrival_Time\"              ]=data[\"Arrival_Time\"              ].str.split(' '   ).str[0]\ndata[\"Arrival_hour\"              ]=data[\"Arrival_Time\"              ].str.split(':'   ).str[0].astype(int)\ndata[\"Arrival_minute\"                ]=data[\"Arrival_Time\"              ].str.split(':'   ).str[1].astype(int)\ndata.drop([\"Arrival_Time\"              ],axis=1,inplace=True)\ndata.head()",
  "generation": "data['Arrival_Time'] = pd.to_datetime(data['Arrival_Time'])\ndata['Arrival_hour'] = data['Arrival_Time'].dt.hour\ndata['Arrival_minute'] = data['Arrival_Time'].dt.minute\ndata.drop(['Arrival_Time'], axis=1, inplace=True)\ndata.head()"
 },
 {
  "nbid": "NB_1546129",
  "file": "NB_1546129,data-1.ipynb",
  "row_id": 9,
  "target_code": "data[\"Dep_Time\"          ]=data[\"Dep_Time\"          ].str.split(' '   ).str[0]\ndata[\"Dep_hour\"          ]=data[\"Dep_Time\"          ].str.split(':'   ).str[0].astype(int)\ndata[\"Dep_minute\"            ]=data[\"Dep_Time\"          ].str.split(':'   ).str[1].astype(int)\ndata.drop([\"Dep_Time\"          ],axis=1,inplace=True)\ndata.head()",
  "generation": "data[\"Dep_hour\"]=data[\"Dep_Time\"].str.split(':').str[0].astype(int)\ndata[\"Dep_minute\"]=data[\"Dep_Time\"].str.split(':').str[1].astype(int)\ndata.drop([\"Dep_Time\"],axis=1,inplace=True)\ndata.head()"
 },
 {
  "nbid": "NB_1547718",
  "file": "NB_1547718,df-1.ipynb",
  "row_id": 20,
  "target_code": "from decimal import Decimal\nbolsas = {0.00: 'b'    ,0.05: 'r'    , 0.10: 'g'    , 0.15: 'm'    , 0.20: 'y'    , 0.25: 'k'    }\ndf['cor'     ] = [bolsas[float(round(Decimal(codigo),2))] for codigo in df['bolsa'       ]]\ndf.head()",
  "generation": "df['cor'] = pd.cut(df['bolsa'], bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5], labels=['g', 'y', 'm', 'r', 'k'])\ndf.head()"
 },
 {
  "nbid": "NB_1556304",
  "file": "NB_1556304,df-1.ipynb",
  "row_id": 3,
  "target_code": "df['label'       ] = df['label'       ].astype(str)\ndf['label'       ] = df['label'       ].str.replace('0'   , 'fail'       )\ndf['label'       ] = df['label'       ].str.replace('1'   , 'success'          )\ndf.head()",
  "generation": "df['label'] = np.where(df['label']==1, 'success', 'failure')\ndf.head()"
 },
 {
  "nbid": "NB_1556707",
  "file": "NB_1556707,train_df-1.ipynb",
  "row_id": 7,
  "target_code": "w1 = 30\nw0 = 15\ntrain_df['label1'        ] = np.where(train_df['RUL'     ] <= w1, 1, 0 )\ntrain_df['label2'        ] = train_df['label1'        ]\ntrain_df.loc[train_df['RUL'     ] <= w0, 'label2'         ] = 2\ntrain_df.head()",
  "generation": "train_df['label1'] = np.where(train_df['RUL'] <= 30, 1, 0)\ntrain_df['label2'] = train_df['label1']\ntrain_df['label2'] = np.where(train_df['RUL'] <= 15, 2, train_df['label2'])\ntrain_df.head()"
 },
 {
  "nbid": "NB_1557852",
  "file": "NB_1557852,ts_log-1.ipynb",
  "row_id": 25,
  "target_code": "b = ts_log.columns[0]\nts_log['%s 1st difference'                   %b] = ts_log - ts_log.shift(1)\nts_log.head()",
  "generation": "ts_log_diff = ts_log - ts_log.shift()\nts_log_diff.columns = ['%s 1st difference'%a]\nts_log_diff.head()"
 },
 {
  "nbid": "NB_1560807",
  "file": "NB_1560807,data-1.ipynb",
  "row_id": 5,
  "target_code": "data['FamilySize'            ] = data ['SibSp'       ] + data['Parch'       ] + 1\ndata['IsAlone'         ] = 1 \ndata.loc[data['FamilySize'            ] > 1,'isAlone'         ] = 0 \ndata['Who'     ] = data['Sex'     ] \ndata['IsChild'         ] = 0 \ndata.loc[data['Age'     ]<=16,'Who'     ] = 'child'        \ndata.loc[data['Age'     ]<=16,'isChild'         ] = 1\ndata.info()\ndata.head()",
  "generation": "data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\ndata['IsAlone'] = 0\ndata.loc[data['FamilySize'] == 1, 'IsAlone'] = 1\ndata['Who'] = data['Sex']\ndata['IsChild'] = np.nan\ndata.loc[data['Age'] < 18, 'IsChild'] = 1\ndata.loc[data['Age'] >= 18, 'IsChild'] = 0\ndata.head()"
 },
 {
  "nbid": "NB_1564932",
  "file": "NB_1564932,df-1.ipynb",
  "row_id": 7,
  "target_code": "df['datetime'          ] = pd.to_datetime(df['Date'      ] + ' '     + df['Time'      ], format = '%d-%m-%Y %H:%M:%S'                    )\ndf.head(2)",
  "generation": "df['datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d-%m-%Y %H:%M:%S')\ndf = df.set_index('datetime')\ndf.head()"
 },
 {
  "nbid": "NB_1564932",
  "file": "NB_1564932,df-1.ipynb",
  "row_id": 8,
  "target_code": "df = df[['datetime'          , 'home_1_Wh'            ]]\ndf.head(2)",
  "generation": "df = df[['datetime', 'home_1_Wh']]\ndf.head()"
 },
 {
  "nbid": "NB_1564932",
  "file": "NB_1564932,df-1.ipynb",
  "row_id": 9,
  "target_code": "df = df.set_index('datetime'          )\ndf.head(2)",
  "generation": "df = df[['home_1_Wh']]\ndf = df.set_index(df.index.rename('datetime'))\ndf.head()"
 },
 {
  "nbid": "NB_1569381",
  "file": "NB_1569381,starwars-1.ipynb",
  "row_id": 3,
  "target_code": "starwars = starwars[pd.notnull(starwars[\"RespondentID\"              ])]\nstarwars.head(2)",
  "generation": "starwars = starwars.dropna(subset=['RespondentID'])\nstarwars.head(3)"
 },
 {
  "nbid": "NB_1569381",
  "file": "NB_1569381,starwars-1.ipynb",
  "row_id": 5,
  "target_code": "yes_no = {\"Yes\"     :True,\"No\"    :False}\nstarwars[\"Have you seen any of the 6 films in the Star Wars franchise?\"                                                              ] = starwars[\"Have you seen any of the 6 films in the Star Wars franchise?\"                                                              ].map(yes_no)\nstarwars[\"Do you consider yourself to be a fan of the Star Wars film franchise?\"                                                                       ] = starwars[\"Do you consider yourself to be a fan of the Star Wars film franchise?\"                                                                       ].map(yes_no)\nstarwars.head(2)",
  "generation": "starwars['Have you seen any of the 6 films in the Star Wars franchise?'] = starwars['Have you seen any of the 6 films in the Star Wars franchise?'].map({'Yes': True, 'No': False})\nstarwars['Do you consider yourself to be a fan of the Star Wars film franchise?'] = starwars['Do you consider yourself to be a fan of the Star Wars film franchise?'].map({'Yes': True, 'No': False})\nstarwars.head(2)"
 },
 {
  "nbid": "NB_1569381",
  "file": "NB_1569381,starwars-1.ipynb",
  "row_id": 7,
  "target_code": "mapper = {\"Star Wars: Episode I  The Phantom Menace\"                                           : True,\n          \"Star Wars: Episode II  Attack of the Clones\"                                                        : True,\n          \"Star Wars: Episode III  Revenge of the Sith\"                                                        : True,\n          \"Star Wars: Episode IV  A New Hope\"                                              : True,\n          \"Star Wars: Episode V The Empire Strikes Back\"                                                         : True,\n          \"Star Wars: Episode VI Return of the Jedi\"                                                     : True,\n          np.nan : False}\ncounter = 1\nfor s in starwars.columns[3:9]:\n    starwars[s] = starwars[s].map(mapper)\n    starwars.rename(columns={s:\"seen_\"        + str(counter)},inplace=True)\n    counter+=1\nstarwars.head(2)",
  "generation": "yes_no = {\"Yes\":True,\"No\":False}\nstarwars[\"Have you seen any of the 6 films in the Star Wars franchise?\"] = starwars[\"Have you seen any of the 6 films in the Star Wars franchise?\"].map(yes_no)\nstarwars[\"Do you consider yourself to be a fan of the Star Wars film franchise?\"] = starwars[\"Do you consider yourself to be a fan of the Star Wars film franchise?\"].map(yes_no)\n\ncheckbox_cols = starwars.columns[3:9]\ncheckbox_map = {f\"Star Wars: Episode {i}  {title}\":True for i,title in zip(range(1,7),['The Phantom Menace','Attack of the Clones','Revenge of the Sith','A New Hope','The Empire Strikes Back','Return of the Jedi'])}\nfor col in checkbox_cols:\n    starwars[col] = starwars[col].map(checkbox_map).fillna(False)"
 },
 {
  "nbid": "NB_1571048",
  "file": "NB_1571048,drones-1.ipynb",
  "row_id": 15,
  "target_code": "drones['week'      ] = pd.to_datetime(drones.week)\ndrones.set_index('week'      , inplace=True)\ndrones.head()",
  "generation": "drones['week'] = pd.to_datetime(drones['week'])\ndrones = drones.set_index('week')\ndrones = drones['drone_racing_interest']"
 },
 {
  "nbid": "NB_1572972",
  "file": "NB_1572972,ont-1.ipynb",
  "row_id": 9,
  "target_code": "ont = ont.drop(columns=[\"Unnamed: 2\"            , \"Unnamed: 3\"             ])\nont.head()",
  "generation": "ont = ont.drop(['Unnamed: 2', 'Unnamed: 3'], axis=1)\nont = ont.dropna()\nont = ont.rename(columns={'Lake Ontario annual averages': 'Year'})\nont['Year'] = ont['Year'].astype(int)\nont = ont.set_index('Year')\nont = ont[['AnnAvg']]\nont.head()"
 },
 {
  "nbid": "NB_180288",
  "file": "NB_180288,BUSL_MON-1.ipynb",
  "row_id": 131,
  "target_code": "BUSL_MON=BUSL_MON.shift(-1)\nBUSL_MON.dropna(axis=0,inplace=True)\nBUSL_MON.head()",
  "generation": "BUSL_MON.shift(1).dropna().head()"
 },
 {
  "nbid": "NB_180288",
  "file": "NB_180288,CONS_MON-1.ipynb",
  "row_id": 119,
  "target_code": "CONS_MON=CONS_MON.shift(-1)\nCONS_MON.dropna(axis=0,inplace=True)\nCONS_MON.head()",
  "generation": "CONS_MON.shift(-1).dropna()"
 },
 {
  "nbid": "NB_180288",
  "file": "NB_180288,CPUSM_MON-1.ipynb",
  "row_id": 55,
  "target_code": "CPUSM_MON=CPUSM_MON.shift(-1)\nCPUSM_MON.dropna(axis=0,inplace=True)\nCPUSM_MON.head()",
  "generation": "CPUSM_MON = CPUSM.resample('M').mean()\nCPUSM_MON = CPUSM_MON.shift(-1)\nCPUSM_MON = CPUSM_MON.dropna()\nCPUSM_MON.head()"
 },
 {
  "nbid": "NB_180288",
  "file": "NB_180288,CSCICP_MON-1.ipynb",
  "row_id": 60,
  "target_code": "CSCICP_MON=CSCICP_MON.shift(-1)\nCSCICP_MON.dropna(axis=0,inplace=True)\nCSCICP_MON.head()",
  "generation": "CSCICP = pd.read_csv('CSCICP03USM665S.csv', parse_dates=['DATE'], index_col='DATE')\nCSCICP_MON=CSCICP['2000':].resample('M').mean()/100\nCSCICP_MON = CSCICP_MON.shift(-1)\nCSCICP_MON.head()"
 },
 {
  "nbid": "NB_180288",
  "file": "NB_180288,INDPRO_MON-1.ipynb",
  "row_id": 166,
  "target_code": "INDPRO_MON=INDPRO_MON.shift(-1)\nINDPRO_MON.dropna(axis=0,inplace=True)\nINDPRO_MON.head()",
  "generation": "INDPRO = INDPRO / 100\nINDPRO_MON = INDPRO['2000':'2020-02'].resample('M').mean() / 100\nINDPRO_MON.rename(columns={\"INDPRO\": \"INDPRO_MON\"}, inplace=True)\nINDPRO_MON.head()"
 },
 {
  "nbid": "NB_180288",
  "file": "NB_180288,PPIACO_MON-1.ipynb",
  "row_id": 172,
  "target_code": "PPIACO_MON=PPIACO_MON.shift(-1)\nPPIACO_MON.dropna(axis=0,inplace=True)\nPPIACO_MON.head()",
  "generation": "PPIACO_MON = PPIACO['2000':].resample('M').last()\nPPIACO_MON.head()"
 },
 {
  "nbid": "NB_180288",
  "file": "NB_180288,REL_MON-1.ipynb",
  "row_id": 125,
  "target_code": "REL_MON=REL_MON.shift(-1)\nREL_MON.dropna(axis=0,inplace=True)\nREL_MON.head()",
  "generation": "REL_MON.shift(1).dropna()"
 },
 {
  "nbid": "NB_180288",
  "file": "NB_180288,SAVE_MON-1.ipynb",
  "row_id": 84,
  "target_code": "SAVE_MON=SAVE_MON.shift(-1)\nSAVE_MON.dropna(axis=0,inplace=True)\nSAVE_MON.head()",
  "generation": "SAVE_MON = SAVE['2000':].resample('M').mean().shift(-1).dropna()\nSAVE_MON.head()"
 },
 {
  "nbid": "NB_180288",
  "file": "NB_180288,UNRATE_MON-1.ipynb",
  "row_id": 177,
  "target_code": "UNRATE_MON=UNRATE_MON.shift(-1)\nUNRATE_MON.dropna(axis=0,inplace=True)\nUNRATE_MON.head()",
  "generation": "UNRATE_MON.shift(1).head()"
 },
 {
  "nbid": "NB_4210843",
  "file": "NB_4210843,dataset-1.ipynb",
  "row_id": 7,
  "target_code": "dataset.Gender = pd.factorize(dataset.Gender)[0]\nDummies = pd.get_dummies(dataset[\"Geography\"           ],drop_first=True)\ndataset = pd.concat([Dummies,dataset],axis=1)\ndataset.drop([\"Geography\"           ],axis=1,inplace=True)\ndataset.head()",
  "generation": "dataset = pd.get_dummies(dataset, columns=['Geography', 'Gender'], drop_first=True)\ndataset.head()"
 },
 {
  "nbid": "NB_4221053",
  "file": "NB_4221053,wap-1.ipynb",
  "row_id": 9,
  "target_code": "wap = wap.drop(['Type'      , 'Label'        , 'timeset'          ], axis=1)\nwap.head()",
  "generation": "wap = wap.drop(['Type', 'Label', 'timeset'], axis=1)\nwap.head()"
 },
 {
  "nbid": "NB_4231037",
  "file": "NB_4231037,Label_train-1.ipynb",
  "row_id": 20,
  "target_code": "Label_train = Label_train.drop(0) \nLabel_train.head(5)",
  "generation": "Label_train = Label_train.dropna()\nLabel_train['date'] = pd.to_datetime(Label_train['date'])\nLabel_train = Label_train.set_index('date')\nLabel_train = Label_train.resample('D').mean()\nLabel_train = Label_train.reset_index()\nLabel_train['label'] = Label_train['label'].fillna(0)\nLabel_train = Label_train.rename(columns={'date': 'Fecha'})\nLabel_train.head()"
 },
 {
  "nbid": "NB_4233618",
  "file": "NB_4233618,df-1.ipynb",
  "row_id": 4,
  "target_code": "df.docID = pd.Series([\"D\"   +str(ind) for ind in df.docID])\ndf.head(5)",
  "generation": "df['docID'] = 'D' + df['docID'].astype(str)\ndf['tags'] = df['tags'].str.replace('[^\\w\\s]','').str.lower().str.strip()\nstop_words = set(stopwords.words('english'))\ndf['tags'] = df['tags'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\ndf.head()"
 },
 {
  "nbid": "NB_4236609",
  "file": "NB_4236609,dev-1.ipynb",
  "row_id": 7,
  "target_code": "df_occ = pd.get_dummies(data = dev['Occupation'            ], dummy_na = True)\ndf_occ = df_occ.rename(columns = {'SELF-EMP'           : 'SELF_EMP'           })\ndev = pd.concat([dev, df_occ.iloc[:, 0:4]], axis = 1)\ndev.head()",
  "generation": "dev = pd.concat([dev, pd.get_dummies(dev['Occupation'])], axis=1)\ndev.drop(['Occupation'], axis=1, inplace=True)\ndev.head()"
 },
 {
  "nbid": "NB_4240538",
  "file": "NB_4240538,df_data-1.ipynb",
  "row_id": 2,
  "target_code": "df_data = df_data[['city'      , 'toss_winner'              , 'winner'         ]]\ndf_data.head()",
  "generation": "df_data = pd.read_csv('./ipl/matches.csv')\ndf_decision = pd.get_dummies(df_data['toss_winner'])\ndf_output = pd.concat([df_data['city'], df_data['toss_winner'], df_data['winner']], axis=1)\ndf_output.columns = ['city', 'toss_winner', 'winner']\ndf_output.head()"
 },
 {
  "nbid": "NB_4240538",
  "file": "NB_4240538,df_data-1.ipynb",
  "row_id": 4,
  "target_code": "df_data = df_data[pd.notnull(df_data.city) & pd.notnull(df_data.winner)]\ndf_data.head()",
  "generation": "df_data = pd.read_csv('./ipl/matches.csv')\ndf_data = df_data[['city', 'toss_winner', 'winner']]\ndf_data = df_data.dropna()\ndf_data.head()"
 },
 {
  "nbid": "NB_4240538",
  "file": "NB_4240538,df_data-1.ipynb",
  "row_id": 7,
  "target_code": "df_data.drop(['toss_winner'             , 'winner'         , 'is_toss_winner_winner'                        ], axis=1, inplace=True)\ndf_data = pd.concat([df_data, df_decision], axis=1)\ndf_data.head()",
  "generation": "df_data = pd.read_csv('./ipl/matches.csv')\ndf_decision = pd.get_dummies(df_data.is_toss_winner_winner)\ndf_data = df_data[['city', 'toss_winner', 'winner']]\ndf_data = df_data[pd.notnull(df_data.city) & pd.notnull(df_data.winner)]\ndf_decision = pd.get_dummies(df_data.is_toss_winner_winner)\ndf_decision = df_decision.rename(columns={False: \"no\", True: \"yes\"})\ndf_decision.head()"
 },
 {
  "nbid": "NB_4250274",
  "file": "NB_4250274,survey-1.ipynb",
  "row_id": 7,
  "target_code": "survey[\"comment\"         ] = survey[\"comment\"         ].str.replace(\"\\(.+?\\)\"         , \"\"   , regex=True)\nsurvey.head()",
  "generation": "survey[\"comment\"] = survey[\"comment\"].str.replace(\"AA\", \"\")"
 },
 {
  "nbid": "NB_4250274",
  "file": "NB_4250274,survey-1.ipynb",
  "row_id": 8,
  "target_code": "survey[\"comment\"         ] = survey[\"comment\"         ].str.replace(\"\\\uff08.+?\\\uff09\"         , \"\"   , regex=True)\nsurvey.head()",
  "generation": "survey[\"comment\"] = survey[\"comment\"].str.replace(\"\\(.+?\\)\", \"\", regex=True)\nsurvey.head()"
 },
 {
  "nbid": "NB_4250274",
  "file": "NB_4250274,survey-1.ipynb",
  "row_id": 10,
  "target_code": "survey[\"length\"        ] = survey[\"comment\"         ].str.len()\nsurvey.head()",
  "generation": "survey[\"length\"] = survey[\"comment\"].str.len()\nsurvey.head()"
 },
 {
  "nbid": "NB_4253070",
  "file": "NB_4253070,df_sns-1.ipynb",
  "row_id": 8,
  "target_code": "df_sns = df_sns.drop(columns=['gradyear'          , 'gender'         , 'age'      , 'friends'          ])\ndf_sns.head()",
  "generation": "df_sns.drop(['gradyear', 'gender', 'age', 'friends'], axis=1, inplace=True)\ndf_sns = (df_sns - df_sns.mean()) / df_sns.std()\nkmeans.fit(df_sns)\ndf_sns['cluster'] = kmeans.labels_"
 },
 {
  "nbid": "NB_4255590",
  "file": "NB_4255590,df-1.ipynb",
  "row_id": 3,
  "target_code": "df['trans_type'            ] = df['trans'       ].apply(lambda x: x.split()[0])\ndf.head()",
  "generation": "df['trans_type'] = df['trans'].apply(lambda x: x.split()[0])\ndf.head()"
 },
 {
  "nbid": "NB_4258338",
  "file": "NB_4258338,df1-1.ipynb",
  "row_id": 1,
  "target_code": "df1=df1[['Description'             ,'Browser_Used'              ,'Device_Used'             ,'Is_Response'             ]]\ndf1.head()",
  "generation": "df1['review'] = df1['review'].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))\ndf1.head()"
 },
 {
  "nbid": "NB_4261429",
  "file": "NB_4261429,df-1.ipynb",
  "row_id": 9,
  "target_code": "df = df[df['isVal'       ]==True]\ndf = df[df['cv_fold'         ]<999]\ndf = df[df['person_count'              ]==2]\ndf.head()",
  "generation": "df.loc[df[\"isVal\"]==True, \"split\"] = \"2nd_level\"\ndf.head()"
 },
 {
  "nbid": "NB_4261496",
  "file": "NB_4261496,df-1.ipynb",
  "row_id": 8,
  "target_code": "df['On_N_Line'           ] = df.LINENAME.map(contains_n)\ndf.head(2)",
  "generation": "df['On_N_Line'] = df['LINENAME'].str.contains('N')\ndf.head()"
 },
 {
  "nbid": "NB_4268410",
  "file": "NB_4268410,df-1.ipynb",
  "row_id": 2,
  "target_code": "df['Churn'       ] = df['Churn'       ].astype(int)\ndf.head()",
  "generation": "df['Churn'] = df['Churn'].astype(int)"
 },
 {
  "nbid": "NB_4269816",
  "file": "NB_4269816,df-1.ipynb",
  "row_id": 4,
  "target_code": "df = df.rename(columns = lambda x : x.lower()) \ndf.head()",
  "generation": "df.columns = df.columns.str.lower()\ndf.head()"
 },
 {
  "nbid": "NB_4269816",
  "file": "NB_4269816,df-1.ipynb",
  "row_id": 6,
  "target_code": "df = df.set_index('linename'          ) \ndf.head()",
  "generation": "df.set_index('linename', inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_4269816",
  "file": "NB_4269816,df-1.ipynb",
  "row_id": 10,
  "target_code": "df['Num_lines'           ] = df['linename'          ].map(lambda x: len(x)) \ndf.head()",
  "generation": "df['Num_Lines'] = df['linename'].map(lambda x: len(x))\ndf = df.sort_values(by=['Num_Lines'], ascending=False)\ndf.head()"
 },
 {
  "nbid": "NB_4274545",
  "file": "NB_4274545,feature_df-1.ipynb",
  "row_id": 2,
  "target_code": "feature_df = feature_df.drop(labels='highest_education'                   , axis=1, level=None, inplace=False)\nfeature_df = feature_df.drop(labels='id_student'            , axis=1, level=None, inplace=False)\nfeature_df.head()",
  "generation": "feature_df = feature_df.drop(columns=['id_student', 'highest_education'])\nfeature_df['final_result'] = feature_df['final_result'].astype(int)\nfeature_df = feature_df.rename(columns={'Unnamed: 0': 'index'})\nfeature_df = feature_df.set_index('index')\nfeature_df.head()"
 },
 {
  "nbid": "NB_4280164",
  "file": "NB_4280164,train_df-1.ipynb",
  "row_id": 6,
  "target_code": "train_df['year'      ] = train_df['datetime'          ].map(get_year)\ntrain_df['month'       ] = train_df['datetime'          ].map(get_month)\ntrain_df['day'     ] = train_df['datetime'          ].map(get_day)\ntrain_df['hour'      ] = train_df['datetime'          ].map(hour_of_day)\ntrain_df['day_of_week'             ] = train_df['datetime'          ].map(get_day_of_week)\ntrain_df['n_temp'        ] = train_df['temp'      ].map(norm_temp)\ntrain_df['n_atemp'         ] = train_df['atemp'       ].map(norm_atemp)\ntrain_df['n_humidity'            ] = train_df['humidity'          ].map(norm_humidity)\ntrain_df['n_windspeed'             ] = train_df['windspeed'           ].map(norm_windspeed)\ntrain_df['n_season'          ] = train_df['season'        ].map(season_new)\ntrain_df.head()\ntrain_df.describe()",
  "generation": "train_df['datetime'] = pd.to_datetime(train_df['datetime'])\ntrain_df['year'] = train_df['datetime'].dt.year\ntrain_df['month'] = train_df['datetime'].dt.month\ntrain_df['day'] = train_df['datetime'].dt.day\ntrain_df['hour'] = train_df['datetime'].dt.hour\ntrain_df['day_of_week'] = train_df['datetime'].dt.dayofweek\ntrain_df['n_temp'] = (train_df['temp'] - train_df['temp'].mean()) / train_df['temp'].std()\ntrain_df['n_atemp'] = (train_df['atemp'] - train_df['atemp'].mean()) / train_df['atemp'].std()\ntrain_df['n_humidity'] = (train_df['humidity'] - train_df['humidity'].mean()) / train_df['humidity'].std()\ntrain_df['n_windspeed'] = (train_df['windspeed'] - train_df['windspeed'].mean()) / train_df['windspeed'].std()\ntrain_df['n_season'] = train_df['season'].apply(lambda x: (x - 1) / 3)\ntrain_df.head()"
 },
 {
  "nbid": "NB_4285709",
  "file": "NB_4285709,df_group_year-1.ipynb",
  "row_id": 5,
  "target_code": "df_group_year['Gt_1945'         ] = [0 if year < 1945 else 10 for year in df_group_year.Year]\ndf_group_year.head(n=2)",
  "generation": "df_group_year['Gt_1945'] = [0 if year < 1945 else 10 for year in df_group_year.Year]\ndf_group_year.head(n=2)"
 },
 {
  "nbid": "NB_4294109",
  "file": "NB_4294109,df-1.ipynb",
  "row_id": 5,
  "target_code": "df = df.drop(columns=['player'        , 'positions'            ])\ndf.head()",
  "generation": "df.drop(['player', 'positions'], axis=1, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_4306336",
  "file": "NB_4306336,df-1.ipynb",
  "row_id": 3,
  "target_code": "df = pd.get_dummies(df)\ndel df[\"y_no\"      ]\ndel df[\"duration\"          ]\ndf.head()",
  "generation": "from sklearn.preprocessing import LabelEncoder\ncategorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\nfor column in categorical_columns:\n    label_encoder = LabelEncoder()\n    label_encoder.fit(df[column])\n    df[column + '_enc'] = label_encoder.transform(df[column])\ndf = df.drop(categorical_columns, axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_4332634",
  "file": "NB_4332634,df-1.ipynb",
  "row_id": 6,
  "target_code": "df = df [['Age'     ,'Income'        ]]\ndf.head()",
  "generation": "scaled_features = scale(df[['Age', 'Income']])\ncluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\ncluster.fit_predict(scaled_features)\ndf['cluster'] = cluster.labels_\ndf_clusters = df[['Age', 'Income', 'cluster']]\ndf_clusters"
 },
 {
  "nbid": "NB_4333284",
  "file": "NB_4333284,pdf-1.ipynb",
  "row_id": 12,
  "target_code": "pdf = pdf[pd.notnull(pdf['Tm'    ])]\npdf = pdf.reset_index(drop = True)\npdf.head(10)",
  "generation": "pdf = pdf.dropna(subset=['Tm'])"
 },
 {
  "nbid": "NB_4336250",
  "file": "NB_4336250,df_forward-1.ipynb",
  "row_id": 29,
  "target_code": "df_forward = pd.concat([df_forward, df['GarageArea'            ]], axis=1)\ndf_forward.head(5)",
  "generation": "df_forward = df.loc[:,['LotArea', 'SalePrice', 'GarageArea']]"
 },
 {
  "nbid": "NB_4349020",
  "file": "NB_4349020,fords-1.ipynb",
  "row_id": 4,
  "target_code": "fords= pd.get_dummies(fords)\nfords.head()",
  "generation": "# One-hot encoding\nfords = pd.get_dummies(fords, columns=['Color', 'Location', 'Model'])\n# Impute missing values\nimputer = SimpleImputer(strategy='mean')\nfords[['Year', 'Mileage', 'Price', 'Age']] = imputer.fit_transform(fords[['Year', 'Mileage', 'Price', 'Age']])\n# Reorder columns\nfords = fords[['Year', 'Mileage', 'Price', 'Age', 'Color_beige', 'Color_black', 'Color_blue', 'Color_brown', 'Color_burgundy', 'Color_gold', 'Color_gray', 'Color_green', 'Color_red', 'Color_white', 'Location_Cambridge', 'Location_Dallas', 'Location_Fresno', 'Location_Philadelphia', 'Location_Phoenix', 'Location_St Paul', 'Model_GL', 'Model_LX', 'Model_Limited', 'Model_SE', 'Model_SEL', 'Model_SES']]\nfords.head()"
 },
 {
  "nbid": "NB_1004976",
  "file": "NB_1004976,univ_df-1.ipynb",
  "row_id": 7,
  "target_code": "univ_df.set_index('Univ'      , inplace=True)\nuniv_df.drop(['State'       ], axis=1, inplace=True)\nuniv_df = univ_df.apply(lambda x: x.astype('float64'         ))\nuniv_df.head()",
  "generation": "univ_df[['SAT', 'Top10', 'Accept', 'SFRatio', 'Expenses', 'GradRate']] = preprocessing.scale(univ_df[['SAT', 'Top10', 'Accept', 'SFRatio', 'Expenses', 'GradRate']].astype('float64'))\nuniv_df.head()"
 },
 {
  "nbid": "NB_1007440",
  "file": "NB_1007440,airp-1.ipynb",
  "row_id": 39,
  "target_code": "airp.Month = pd.to_datetime(airp.Month)\nairp.set_index('Month'       , inplace=True)\nairp.head()",
  "generation": "airp = airp.drop(columns=['Month'])\nairp.head()"
 },
 {
  "nbid": "NB_1007440",
  "file": "NB_1007440,data-1.ipynb",
  "row_id": 7,
  "target_code": "data.Frequency = pd.to_datetime(data.Frequency)\ndata.set_index('Frequency'           , inplace=True)\ndata.head()",
  "generation": "data = data.drop(columns=[\"Frequency\"])\ndata.head()"
 },
 {
  "nbid": "NB_1007440",
  "file": "NB_1007440,nyse-1.ipynb",
  "row_id": 56,
  "target_code": "nyse.Month = pd.to_datetime(nyse.Month)\nnyse.set_index('Month'       , inplace=True)\nnyse.head()",
  "generation": "nyse['monthly_return']"
 },
 {
  "nbid": "NB_1010359",
  "file": "NB_1010359,df-1.ipynb",
  "row_id": 25,
  "target_code": "df['Reason'        ] = df['title'       ].apply(lambda x:x.split(':'   )[0])\ndf.head()",
  "generation": "df['Reason'] = df['title'].apply(lambda title: title.split(':')[0])\ndf.head()"
 },
 {
  "nbid": "NB_151655",
  "file": "NB_151655,df-1.ipynb",
  "row_id": 25,
  "target_code": "df['Month'       ] = df.index.strftime('%b'    )\ndf['Year'      ] = df.index.year\ndf.head()",
  "generation": "df['Month'] = df.index.strftime('%b')\ndf['Year'] = df.index.year\ndf = df.reset_index(drop=True)[['Total_Demand_KW', 'Month', 'Year']]\ndf.head()"
 },
 {
  "nbid": "NB_153573",
  "file": "NB_153573,reviews_df-1.ipynb",
  "row_id": 3,
  "target_code": "reviews_df[\"review\"        ] = reviews_df[\"Negative_Review\"                 ] + reviews_df[\"Positive_Review\"                 ]\npd.set_option('max_colwidth'              , 400)\nprint(type(reviews_df))\nreviews_df.shape\nreviews_df[\"is_bad_review\"               ] = reviews_df[\"Reviewer_Score\"                ].apply(lambda x: 1 if x < 5 else 0)\nreviews_df = reviews_df[[\"review\"        , \"is_bad_review\"                ]]\nreviews_df.head()",
  "generation": "reviews_df['review'] = reviews_df['Positive_Review'] + reviews_df['Negative_Review']\nreviews_df['label'] = reviews_df['Reviewer_Score'].apply(lambda x: 'good' if x >= 5 else 'bad')\nreviews_df = reviews_df[['review', 'label']]\nreviews_df.head()"
 },
 {
  "nbid": "NB_154530",
  "file": "NB_154530,salary-1.ipynb",
  "row_id": 16,
  "target_code": "salary=salary.merge(rates,left_on='CompensationCurrency'                      ,right_on='originCountry'               ,how='left'      )\nsalary.head()",
  "generation": "salary['originCountry'] = salary['CompensationCurrency']\nrates.set_index('originCountry',inplace=True)\nsalary.set_index('originCountry',inplace=True)\nsalary['exchangeRate'] = salary['CompensationCurrency'].map(rates['exchangeRate'])\nsalary.reset_index(inplace=True)\nsalary.head()"
 },
 {
  "nbid": "NB_156045",
  "file": "NB_156045,df-1.ipynb",
  "row_id": 17,
  "target_code": "df = rawdf.dropna(axis=0 , how='any'     )\ndf = df[['Date'      , 'Time'       , 'Location'           , 'Light'        , 'X'    , 'Y'    , 'Impact_type'              ,'Traffic_Control'                 ,\"Collision_Location\"                    ,'Road_Surface'              ]]\ndf['date_m'        ] =  df['Date'      ].str.split('/'   , expand=True)[[0]].astype(int)\ndf['time'      ] =  df['Time'      ].str.split(':'   , expand=True)[[0]].astype(int)\ndf['road_Surface'              ] =  df['Road_Surface'              ].str.split('-'   , expand=True)[[0]].astype(int)\ndf['light'       ] =  df['Light'       ].str.split('-'   , expand=True)[[0]].astype(int)\ndf['traffic_Control'                 ] =  df['Traffic_Control'                 ].str.split('-'   , expand=True)[[0]].astype(int)\ndf['X'   ] = df['X'   ].str.replace('\\,'    , ''   ).astype(float)\ndf['Y'   ] = df['Y'   ].str.replace('\\,'    , ''   ).astype(float)\ndf = df.drop(axis = 0, index = df[df.X > 1000000].index)\ndf = df.drop(axis = 0, index = df[df[\"light\"       ] == 99].index)\ndf.head()",
  "generation": "df = rawdf.drop(['Continent', 'Country'], axis=1)\ncorr_matrix = df.corr()\ntop_corr_features = corr_matrix.index[abs(corr_matrix[\"Sort_gen.rating\"])>0.5]\ndf = df[top_corr_features]\ndf = df.dropna(axis=0 , how='any')\ndf = df[['Date', 'Time', 'Location', 'Light', 'X', 'Y', 'Collision_Classification', 'Traffic_Control']] \ndf['date_m'] = pd.to_datetime(df['Date']).dt.month\ndf['time'] = pd.to_datetime(df['Time']).dt.hour\ndf['road_Surface'] = df['Road_Surface'].astype('category').cat.codes\ndf['light'] = df['Light'].astype('category').cat.codes\ndf['traffic_Control'] = df['Traffic_Control'].astype('category').cat.codes\ndf = df.drop(['Date', 'Time', 'Road_Surface', 'Light', 'Traffic_Control'], axis=1)\ndf = df.rename(columns={'Collision_Classification':'Impact_type'})"
 },
 {
  "nbid": "NB_1575066",
  "file": "NB_1575066,df-1.ipynb",
  "row_id": 33,
  "target_code": "df['main_prod_comp'                ] = df['production_companies'                      ].str.split('|'   , 1, expand=True)[0]\ndf.head(5)",
  "generation": "df['production_companies'] = df['production_companies'].str.split('|').str[0]\ndf.drop(columns = {'id', 'imdb_id', 'budget', 'revenue', 'homepage', 'director', 'tagline', 'keywords', 'overview', 'runtime', 'vote_count', 'vote_average', 'release_date', 'popularity', 'cast', 'genres', 'original_title', 'spoken_languages', 'status', 'production_countries'}, inplace = True)\ndf.dropna(inplace = True)\ndf.reset_index(inplace = True, drop = True)\ndf.head()"
 },
 {
  "nbid": "NB_1575066",
  "file": "NB_1575066,df_year-1.ipynb",
  "row_id": 24,
  "target_code": "df_year['bb_percent'            ] = df_year['bb'    ]/df_year['movie_count'             ]\ndf_year.head()",
  "generation": "df_year['movie_count'] = df_budget.groupby('release_year', as_index=False)['bb'].count()['bb']\ndf_year['bb_percent'] = df_year['bb'] / df_year['movie_count'] * 100\ndf_year"
 },
 {
  "nbid": "NB_1583930",
  "file": "NB_1583930,df1-1.ipynb",
  "row_id": 63,
  "target_code": "df1[\"season_char\"             ] =df1.index\ndf1=df1.reset_index(drop=True)\ndf1[\"season\"        ]=df1[\"season_char\"             ].apply(lambda x: x[0])\ndf1[\"char\"      ]=df1[\"season_char\"             ].apply(lambda x: x[1])\ndf1=df1.drop(columns=[\"season_char\"             ])\ndf1.head()",
  "generation": "df1['season'] = df1.index.map(lambda x: x[0])\ndf1['char'] = df1.index.map(lambda x: x[1])\ndf1 = df1[['count', 'season', 'char']]\ndf1.head()"
 },
 {
  "nbid": "NB_1585326",
  "file": "NB_1585326,data-1.ipynb",
  "row_id": 5,
  "target_code": "data=data.loc[:, data.isnull().mean() < .8]\ndata.head()",
  "generation": "data = data.dropna(thresh=len(data)*0.8, axis=1)\ndata.head()"
 },
 {
  "nbid": "NB_1585326",
  "file": "NB_1585326,data-1.ipynb",
  "row_id": 7,
  "target_code": "data=data.drop(axis=0,columns=['tornado'         ,'hail'      ,'thunder'         ,'snow'      ,'fog'     ,'Unnamed: 0'            ,'dir'     ,'cond'      ])\ndata.head()",
  "generation": "data = data.drop(columns=['dir', 'cond', 'snow', 'hail', 'thunder', 'tornado', 'fog'])\ndata.head()"
 },
 {
  "nbid": "NB_1590258",
  "file": "NB_1590258,gym_census_df-1.ipynb",
  "row_id": 9,
  "target_code": "gym_census_df[\"% High Education\"                  ] = (gym_census_df[\"Education Bachelors\"                     ]+gym_census_df[\"Education Masters\"                   ]+gym_census_df[\"Education Professional\"                        ]+gym_census_df[\"Education Doctorate\"                     ])/gym_census_df[\"Population\"            ]\ngym_census_df[\"% Poverty\"           ] = gym_census_df[\"Poverty\"         ] / gym_census_df[\"Population\"            ]\ngym_census_df[\"% Black\"         ] = (gym_census_df[\"Population Black\"                  ]/gym_census_df[\"Population\"            ])\ngym_census_df[\"% White\"         ] = gym_census_df[\"Population White\"                  ] / gym_census_df[\"Population\"            ]\ngym_census_df[\"% Asian\"         ] = (gym_census_df[\"Population Asian\"                  ]/gym_census_df[\"Population\"            ])\ngym_census_df[\"% Hispanic\"            ] = (gym_census_df[\"Population Hispanic\"                     ]/gym_census_df[\"Population\"            ])\ngym_census_df.head()",
  "generation": "gym_census_df['Percent of White'] = gym_census_df['White'] / gym_census_df['Total Population']\ngym_census_df['Percent of Black'] = gym_census_df['Black or African American'] / gym_census_df['Total Population']\ngym_census_df['Percent of Asian'] = gym_census_df['Asian'] / gym_census_df['Total Population']\ngym_census_df['Percent of Hispanic'] = gym_census_df['Hispanic or Latino'] / gym_census_df['Total Population']\ngym_census_df['Percent of Native'] = gym_census_df['American Indian and Alaska Native'] / gym_census_df['Total Population']\ngym_census_df['Percent of Pacific'] = gym_census_df['Native Hawaiian and Other Pacific Islander'] / gym_census_df['Total Population']\ngym_census_df.head(2)"
 },
 {
  "nbid": "NB_1594170",
  "file": "NB_1594170,datm-1.ipynb",
  "row_id": 13,
  "target_code": "status = pd.get_dummies(datm['Month'       ], drop_first = True)\ndatm = pd.concat([datm, status], axis = 1)\ndatm.head()",
  "generation": "datm = dat.drop(['Mean_temperature_in_degree_C_Minimum'], axis=1)\nmonths = pd.get_dummies(datm['Month'], drop_first=True)\ndatm = pd.concat([datm, months], axis=1)\ndatm.head()"
 },
 {
  "nbid": "NB_1595246",
  "file": "NB_1595246,data-1.ipynb",
  "row_id": 16,
  "target_code": "data.index = data['NAME'      ]\ndata = data.drop('NAME'      , axis=1)\ndata.head()",
  "generation": "data.set_index('NAME', inplace=True)\ndata.drop('NAME', axis=1, inplace=True)"
 },
 {
  "nbid": "NB_1595918",
  "file": "NB_1595918,df-1.ipynb",
  "row_id": 6,
  "target_code": "df = df.drop(columns='Unnamed: 0'            )\ndf.head()",
  "generation": "df = df.drop(columns=['Unnamed: 0'])\ndf.head()"
 },
 {
  "nbid": "NB_1599510",
  "file": "NB_1599510,bank_o-1.ipynb",
  "row_id": 56,
  "target_code": "bank_o['poutcome'          ] = labelencoder_X.fit_transform(bank_o['poutcome'          ])\nbank_o.head()",
  "generation": "from sklearn.preprocessing import LabelEncoder\nlabelencoder_X.fit(db['poutcome'])\ndb['poutcome'] = labelencoder_X.transform(db['poutcome'])\nbank_o = db.loc[: , ['campaign', 'pdays','previous', 'poutcome']]\nbank_o.head()"
 },
 {
  "nbid": "NB_1600185",
  "file": "NB_1600185,df-1.ipynb",
  "row_id": 3,
  "target_code": "print(\"Num rows: {}, Num rows no null date: {}\"                                         .format(df.shape[0], df[~df[\"Date\"      ].isnull()].shape[0]))\ndf = df.dropna()\nprint(\"Num after dropping nulls: {}\"                              .format(df.shape[0]))\ndf.head()\ndf[\"date_and_time\"               ] = pd.to_datetime(df[\"Time_of_day\"             ] + \" \"     + df[\"Date\"      ])\ndf.index = df[\"date_and_time\"               ]\ndf = df.sort_index()\ndf.head()",
  "generation": "df = df.dropna(subset=[\"Date\"])\ndf[\"date_and_time\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time_of_day\"])\ndf.head()"
 },
 {
  "nbid": "NB_1602664",
  "file": "NB_1602664,mail-1.ipynb",
  "row_id": 5,
  "target_code": "mail['label'       ] = mail['labels'        ].map({'ham'     : 0, 'spam'       : 1})\nmail.head()",
  "generation": "mail['label'] = mail['v1'].map({'ham': 0, 'spam': 1})\nmail = mail.rename(columns={'v2': 'message'})[['message', 'label']]\nmail.head()"
 },
 {
  "nbid": "NB_1604684",
  "file": "NB_1604684,dataset-1.ipynb",
  "row_id": 4,
  "target_code": "dataset = dataset.drop(columns=0)\ndataset = dataset.rename(columns={0:\"ID\"    , 1:\"Date-Time\"           ,2:\"Flow[veh/h.lane]\"                  , 3:\"Speed[km/h]\"              })\ndataset[\"Flow[veh/h.lane]\"                  ] = dataset[\"Flow[veh/h.lane]\"                  ] * 60 / 5 / 2 \ndataset[\"Density[veh/km.lane]\"                      ] = dataset[\"Flow[veh/h.lane]\"                  ] / dataset['Speed[km/h]'             ]\ndataset['Date-Time'           ] = pd.to_datetime(dataset['Date-Time'           ], format='%d/%m/%Y %H:%M'                )\ndataset.set_index('Date-Time'           ,inplace=True)\ndataset.head()",
  "generation": "dataset = pd.read_csv('SP-280-Leste.csv', sep=',',header=None)\ndataset = dataset.drop(columns=[0])\ndataset.columns = ['date_time', 'flow', 'speed']\ndataset['flow'] = dataset['flow']*2\ndataset['density'] = dataset['flow']/(dataset['speed']*1000/3600)\ndataset['date_time'] = pd.to_datetime(dataset['date_time'])\ndataset = dataset.set_index('date_time')\ndataset = dataset[['flow', 'speed', 'density']]\ndataset.head()"
 },
 {
  "nbid": "NB_4370075",
  "file": "NB_4370075,data-1.ipynb",
  "row_id": 5,
  "target_code": "data = pd.get_dummies(data, columns=['Channel'         , 'Region'         ]) \ndata.head()",
  "generation": "one_hot_encoded_data = pd.get_dummies(data, columns=['Channel', 'Region'])\none_hot_encoded_data.head()"
 },
 {
  "nbid": "NB_4384218",
  "file": "NB_4384218,D1-1.ipynb",
  "row_id": 8,
  "target_code": "D1=DF.copy(deep=True)\nD1 = D1.replace('?'   , np.NaN)\nD1=D1.dropna()\nD1.head()",
  "generation": "D1 = DF.copy(deep=True)\nD1['normalized-losses'].replace('?', np.nan, inplace=True)\nD1.dropna(subset=['normalized-losses'], axis=0, inplace=True)\nD1.reset_index(drop=True, inplace=True)\nD1.head()"
 },
 {
  "nbid": "NB_4385155",
  "file": "NB_4385155,df-1.ipynb",
  "row_id": 13,
  "target_code": "df['Num_Lines'           ] = df.linename.map(lambda x: len(x))\ndf.head()",
  "generation": "df['Num_Lines'] = df['linename'].apply(lambda x: len(x))\ndf = df.sort_values(by='Num_Lines', ascending=False)\ndf.head()"
 },
 {
  "nbid": "NB_4399087",
  "file": "NB_4399087,b_b_data-1.ipynb",
  "row_id": 13,
  "target_code": "b_b_data.week =b_b_data.week.str.replace('wk'    ,''  )\nb_b_data.head()",
  "generation": "b_b_data['week'] = b_b_data['week'].str.extract('(\\d+)').astype(int)\nb_b_data.head()"
 },
 {
  "nbid": "NB_4421886",
  "file": "NB_4421886,df_new-1.ipynb",
  "row_id": 84,
  "target_code": "df_new[['CA'    ,'UK'    ,'US'    ]] = pd.get_dummies(df_new['country'         ])\ndf_new.head()",
  "generation": "df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])\ndf_new.head()"
 },
 {
  "nbid": "NB_4436227",
  "file": "NB_4436227,titanic-1.ipynb",
  "row_id": 7,
  "target_code": "titanic['Cabin_ind'           ] = np.where(titanic['Cabin'       ].isnull(), 0, 1)\ntitanic.head(10)",
  "generation": "titanic['Cabin_ind'] = np.where(titanic['Cabin'].isnull(), 0, 1)\ntitanic[['Survived', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Cabin_ind']].head()"
 },
 {
  "nbid": "NB_4445003",
  "file": "NB_4445003,cc_apps-1.ipynb",
  "row_id": 6,
  "target_code": "import numpy as np\nprint(cc_apps.tail(17))\ncc_apps = cc_apps.replace('?'   , np.NaN)\ncc_apps.tail(17)",
  "generation": "cc_apps = cc_apps.replace('?', np.nan)"
 },
 {
  "nbid": "NB_4469139",
  "file": "NB_4469139,titanic-1.ipynb",
  "row_id": 5,
  "target_code": "titanic['Cabin_ind'           ] = np.where(titanic['Cabin'       ].isnull(), 0, 1)\ntitanic.head()",
  "generation": "titanic['Cabin_ind'] = np.where(titanic['Cabin'].isnull(), 0, 1)\ntitanic[['Survived', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Cabin_ind']].head()"
 },
 {
  "nbid": "NB_4471729",
  "file": "NB_4471729,x-1.ipynb",
  "row_id": 19,
  "target_code": "x = load.iloc[:, :-1]\nx = x[['Protein_(g)'             , 'Lipid_Tot_(g)'                , 'Carbohydrt_(g)'                 ]]\nx.head()",
  "generation": "poly = PolynomialFeatures(degree=2, interaction_only=True)\nx_poly = poly.fit_transform(x)\nx = pd.DataFrame(x_poly, columns=poly.get_feature_names(x.columns))\nx = x[['Protein_(g)', 'Lipid_Tot_(g)', 'Carbohydrt_(g)', 'Protein_(g) Lipid_Tot_(g)', 'Protein_(g) Carbohydrt_(g)', 'Lipid_Tot_(g) Carbohydrt_(g)']]\nx.head()"
 },
 {
  "nbid": "NB_4487205",
  "file": "NB_4487205,df-1.ipynb",
  "row_id": 2,
  "target_code": "import math\nsl = math.floor(df['SepalLength'             ].mean())\nsl\ndf['SepalLength'             ] = df['SepalLength'             ].fillna(sl)\ndf.head()",
  "generation": "df = df.fillna(value='NA')\ndf.head()"
 },
 {
  "nbid": "NB_4487205",
  "file": "NB_4487205,df-1.ipynb",
  "row_id": 3,
  "target_code": "sl = math.floor(df['SepalWidth'            ].mean())\nsl\ndf['SepalWidth'            ] = df['SepalWidth'            ].fillna(sl)\ndf.head()",
  "generation": "df['SepalLength'] = df['SepalLength'].fillna(math.floor(df['SepalLength'].mean()))\ndf.head()"
 },
 {
  "nbid": "NB_4487205",
  "file": "NB_4487205,df-1.ipynb",
  "row_id": 4,
  "target_code": "sl = math.floor(df['PetalLength'             ].mean())\nsl\ndf['PetalLength'             ] = df['PetalLength'             ].fillna(sl)\ndf.head()",
  "generation": "sl = math.floor(df['SepalLength'].mean())\ndf['SepalLength'] = df['SepalLength'].fillna(sl)\n\nsl = math.floor(df['SepalWidth'].mean())\ndf['SepalWidth'] = df['SepalWidth'].fillna(sl)"
 },
 {
  "nbid": "NB_4487205",
  "file": "NB_4487205,df-1.ipynb",
  "row_id": 5,
  "target_code": "sl = math.floor(df['PetalWidth'            ].mean())\nsl\ndf['PetalWidth'            ] = df['PetalWidth'            ].fillna(sl)\ndf.head()",
  "generation": "import pandas as pd\nimport math\nfrom matplotlib import pyplot as plt\ndf = pd.read_csv(\"iris_MV.csv\")\ndf.head()\nsl = math.floor(df['SepalLength'].mean())\ndf['SepalLength'] = df['SepalLength'].fillna(sl)\nsl = math.floor(df['SepalWidth'].mean())\ndf['SepalWidth'] = df['SepalWidth'].fillna(sl)\nsl = math.floor(df['PetalLength'].mean())\ndf['PetalLength'] = df['PetalLength'].fillna(sl)\ndf.head()"
 },
 {
  "nbid": "NB_4488070",
  "file": "NB_4488070,df-1.ipynb",
  "row_id": 9,
  "target_code": "from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndf['Weight'        ] = le.fit_transform(df.Weight)\ndf['Size'      ] = le.fit_transform(df.Size)\ndf['Class'       ] = le.fit_transform(df.Class)\ndf.head()",
  "generation": "le = preprocessing.LabelEncoder()\ndf['Class'] = le.fit_transform(df['Class'])\ndf['Weight'] = np.where(df['Weight'] > 70, 0, df['Weight'])\ndf['Size'] = np.where(df['Size'] > 5, 1, df['Size'])\ndf.rename(columns={'Class': 'Fruit'}, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_4492125",
  "file": "NB_4492125,data-1.ipynb",
  "row_id": 5,
  "target_code": "data = data.drop([\"Unnamed: 2\"            , \"Unnamed: 3\"             , \"Unnamed: 4\"             ], axis=1)\ndata = data.rename(columns={\"v1\"    :\"class\"       , \"v2\"     :\"text\"      })\ndata.head()",
  "generation": "data = pd.read_csv('spam.csv', encoding='latin-1', usecols=[0,1])\ndata.columns = ['class', 'text']\ndata.head()"
 },
 {
  "nbid": "NB_4492125",
  "file": "NB_4492125,data-1.ipynb",
  "row_id": 6,
  "target_code": "data['length'        ] = data['text'      ].apply(len)\ndata.head()",
  "generation": "data['length'] = data['text'].apply(len)\ndata.head()"
 },
 {
  "nbid": "NB_4516947",
  "file": "NB_4516947,df-1.ipynb",
  "row_id": 3,
  "target_code": "df['Price1'        ] = df['Close'       ].shift(-1)\ndf.head()",
  "generation": "df['Price1'] = df['Close'].shift(-1)\ndf.head()"
 },
 {
  "nbid": "NB_4516947",
  "file": "NB_4516947,df-1.ipynb",
  "row_id": 4,
  "target_code": "df['PriceDiff'           ] = df['Price1'        ] - df['Close'       ] \ndf['Return'        ] = df['PriceDiff'           ]/df['Close'       ]\ndf.head()",
  "generation": "df['PriceDiff'] = df['Price1'] - df['Close']\ndf['Return'] = df['PriceDiff'] / df['Close']\ndf.head()"
 },
 {
  "nbid": "NB_4519872",
  "file": "NB_4519872,df-1.ipynb",
  "row_id": 4,
  "target_code": "channel_col = df[\"Channel\"         ]\ndf = df.drop([\"Channel\"         , \"Region\"         ], axis=1)\ndf.head()",
  "generation": "channel = df['Channel']\ndf = df.drop(['Channel', 'Region'], axis=1)\nprint(df.head())"
 },
 {
  "nbid": "NB_4522384",
  "file": "NB_4522384,df2-1.ipynb",
  "row_id": 67,
  "target_code": "df2[['ab_page'         , 'old_page'           ]] = pd.get_dummies(df2['landing_page'              ])\ndf2['intercept'           ] = 1\ndf2.head()",
  "generation": "df2['intercept'] = 1\ndf2[['ab_page', 'old_page']] = pd.get_dummies(df2['landing_page'])\ndf2 = df2.drop('old_page', axis=1)\ndf2.head()"
 },
 {
  "nbid": "NB_4523541",
  "file": "NB_4523541,data-1.ipynb",
  "row_id": 5,
  "target_code": "import seaborn as sns\nimport matplotlib.pyplot as plt\nfig_size = (20, 10)\nfig, ax = plt.subplots(figsize = fig_size)\nsns.boxplot(x=\"variable\"          , y=\"value\"       , data=pd.melt(df2), ax=ax)\nquantile1 = data.quantile(0.25)\nquantile3 = data.quantile(0.75)\nIQR = quantile3 - quantile1\nprint(IQR)\ndata = data[~((data < (quantile1 - 1.5 * IQR)) |(data > (quantile3 + 1.5 * IQR))).any(axis=1)]\nprint(data.shape)\ndata.reset_index(inplace=True)\ndata.drop('index'       , axis=1, inplace=True)\ndata.head()",
  "generation": "Q1 = data.quantile(0.25)\nQ3 = data.quantile(0.75)\nIQR = Q3 - Q1\ndata = data[~((data < (Q1 - 1.5 * IQR)) |(data > (Q3 + 1.5 * IQR))).any(axis=1)]\ndata.head()"
 },
 {
  "nbid": "NB_1016080",
  "file": "NB_1016080,movie_data-1.ipynb",
  "row_id": 4,
  "target_code": "import numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\ndel_col = [ 'id'     , 'imdb_id'          , 'popularity'             , 'budget_adj'             , 'revenue_adj'              , 'homepage'           , 'keywords'           , 'overview'           , 'production_companies'                       , 'vote_count'             , 'vote_average'               ]\nmovie_data = movie_data.drop(del_col, 1)\nmovie_data.head()",
  "generation": "movie_data.drop(['id', 'imdb_id', 'popularity', 'budget_adj', 'revenue_adj', 'homepage', 'keywords', 'overview', 'production_companies', 'vote_count', 'vote_average'], axis=1, inplace=True)\nmovie_data.head()"
 },
 {
  "nbid": "NB_1021915",
  "file": "NB_1021915,df_2016-1.ipynb",
  "row_id": 65,
  "target_code": "df_2016 = df_2016.merge(df_pop, on='State'       )\ndf_2016.head()",
  "generation": "df_pop['2016'] = df_pop['2016'].str.replace(',', '').astype(int)\ndf_2016 = df_merged[df_merged['Year']=='2016']\ndf_2016 = df_2016[df_2016['Measures']=='Total Homeless']\ndf_2016 = df_2016.groupby(['State','Year',])[['Count']].sum()\ndf_2016 = df_2016.merge(df_pop[['State', '2010', '2011', '2012', '2013', '2014', '2015', '2016']], on='State')\ndf_2016"
 },
 {
  "nbid": "NB_1022972",
  "file": "NB_1022972,data-1.ipynb",
  "row_id": 13,
  "target_code": "columns = ['category'          ,'rating'        ,'installs'          ,'sizes'       ,'prices'        ,'content_rating'                ,'month_last_updated'                    ,'years_last_update'                   ]\ndata = pd.DataFrame(data) \ndata.columns = columns\ndata.head()",
  "generation": "data['sizes'] = pd.cut(data['sizes'], bins=[-1, 25, float('inf')], labels=['Size <= 25MB', 'Size > 25MB'])\ndata['prices'] = data['prices'].replace(0.0, 'GRATIS')\ndata['rating'] = data['rating'].apply(lambda x: 1 if x >= 4 else 0)\ndata = data.rename(columns={'month_last_update': 'month_last_updated'})\ndata = data[['category', 'rating', 'installs', 'sizes', 'prices', 'content_rating', 'month_last_updated', 'years_last_update']]\ndata.head()"
 },
 {
  "nbid": "NB_1028873",
  "file": "NB_1028873,combined-1.ipynb",
  "row_id": 31,
  "target_code": "combined.loc[:,\"Price per SF\"              ] = combined.loc[:,\"Sale Price\"            ] / combined.loc[:,\"Square Feet\"             ]\ncombined.head()",
  "generation": "combined[\"Price per SF\"] = combined[\"Sale Price\"] / combined[\"Square Feet\"]\ncombined.head()"
 },
 {
  "nbid": "NB_1030962",
  "file": "NB_1030962,news_4_sub_topics-1.ipynb",
  "row_id": 60,
  "target_code": "news_4_sub_topics['year-month'            ] = news_4_sub_topics['year'      ].map(str) +'-'   + news_4_sub_topics['month'       ].map(str)\nnews_4_sub_topics.head()",
  "generation": "news_4_sub_topics = news_4_sub_topics.drop(columns=['content', 'topic'], axis=1)\nnews_4_sub_topics['year'] = news['year']\nnews_4_sub_topics['month'] = news['month']\nnews_4_sub_topics = news_4_sub_topics.groupby(['year', 'month'])['content_processed'].apply(lambda x: ' '.join(x)).reset_index()\nnews_4_sub_topics.head()"
 },
 {
  "nbid": "NB_1030962",
  "file": "NB_1030962,news_4_sub_topics-1.ipynb",
  "row_id": 62,
  "target_code": "news_4_sub_topics = news_4_sub_topics.groupby('year-month'            )['content_processed'                   ].apply(lambda x: \"{%s}\"        % '.\\n '       .join(x))\nnews_4_sub_topics = news_4_sub_topics.reset_index()\nnews_4_sub_topics.head()",
  "generation": "There is no input or output dataframe provided in the context, so I cannot write the target code. Can you please provide the input and output dataframes?"
 },
 {
  "nbid": "NB_1033686",
  "file": "NB_1033686,df-1.ipynb",
  "row_id": 4,
  "target_code": "df['MONTH'       ] = pd.to_datetime(df['MONTH'       ])\ndf.columns = ['month'       ,'energy_gen'            ] \ndf.head()",
  "generation": "df['month'] = pd.to_datetime(df['MONTH'], format='%Y-%m')\ndf = df.drop(['MONTH'], axis=1)\ndf = df.rename(columns={\"ENERGY GEN\":\"energy_gen\"})\ndf.head()"
 },
 {
  "nbid": "NB_1036452",
  "file": "NB_1036452,hr-1.ipynb",
  "row_id": 33,
  "target_code": "hr=hr.rename(columns = {'sales'       :'department'            })\nhr.head()",
  "generation": "a = hr[['department', 'Work_accident']]\na = a.groupby(['department']).sum()\na.reset_index(inplace=True)\na.rename(columns={'Work_accident': 'No_of_accidents'}, inplace=True)\nhr = pd.merge(hr, a, on='department')\nhr.head()"
 },
 {
  "nbid": "NB_1038217",
  "file": "NB_1038217,df-1.ipynb",
  "row_id": 3,
  "target_code": "df = df.loc[df.Area == 'India'        ]\ndf.reset_index(inplace = True, drop = True) \ndf.drop(columns = {'Area Code'           ,'Area'      ,'Year Code'           ,'Unit'      ,'Flag'      }, inplace = True)\ndf.head()",
  "generation": "india_data = df[df['Area'] == 'India']\nindia_data = india_data.drop(['Area Code', 'Element Code', 'Year Code', 'Flag'], axis=1)\nindia_data = india_data.rename(columns={'Item Code': 'Item Code', 'Item': 'Item', 'Element': 'Element', 'Year': 'Year', 'Unit': 'Unit', 'Value': 'Value'})\nindia_data = india_data[india_data['Element'] == 'Import Quantity']\nindia_data = india_data.drop(['Element'], axis=1)\nindia_data.head()"
 },
 {
  "nbid": "NB_1039219",
  "file": "NB_1039219,ts-1.ipynb",
  "row_id": 2,
  "target_code": "ts['Month'       ] = pd.to_datetime(ts['Month'       ])\nts.set_index('Month'       , inplace=True)\nts.head()",
  "generation": "ts['Month'] = pd.to_datetime(ts['Month'])\nts.set_index('Month', inplace=True)\nts = ts['#Passengers']\nts.head()"
 },
 {
  "nbid": "NB_1042607",
  "file": "NB_1042607,df_grippal-1.ipynb",
  "row_id": 13,
  "target_code": "df_grippal = df_grippal[['PERIODE'         , 'IAS_brut'           ]].set_index('PERIODE'         )\ndf_grippal.head()",
  "generation": "df_grippal['IAS_brut'].head()"
 },
 {
  "nbid": "NB_1042699",
  "file": "NB_1042699,df_sars-1.ipynb",
  "row_id": 25,
  "target_code": "df_sars['visitor_count'               ]= pd.to_numeric(df_sars['visitor_count'               ],errors='coerce'        )\ndf_sars = df_sars.groupby('month_year'            )['visitor_count'               ].sum()\ndf_sars = pd.DataFrame(df_sars)\ndf_sars = df_sars.reset_index()\ndf_sars['year'      ], df_sars['month'       ] = df_sars['month_year'            ].str.split('-'   , 1).str\ndf_sars['year'      ] = pd.to_numeric(df_sars['year'      ],errors='coerce'        )\ndf_sars.head()",
  "generation": "df_sars['visitor_count'] = pd.to_numeric(df_sars['visitor_count'], errors='coerce')\ndf_sars['year'] = pd.DatetimeIndex(df_sars['month_year']).year\ndf_sars['month'] = pd.DatetimeIndex(df_sars['month_year']).month\ndf_sars = df_sars.groupby(['year', 'month'])['visitor_count'].sum().reset_index()\ndf_sars.head()"
 },
 {
  "nbid": "NB_1042724",
  "file": "NB_1042724,df_f-1.ipynb",
  "row_id": 33,
  "target_code": "df_f['Month'       ] = pd.to_datetime(df_f.Date).dt.strftime('%b'    )\ndf_f.head()",
  "generation": "df_f['Month'] = df_f['Date'].dt.strftime('%b')\ndf_f.head()"
 },
 {
  "nbid": "NB_1042806",
  "file": "NB_1042806,growth_rate-1.ipynb",
  "row_id": 14,
  "target_code": "growth_rate = pd.melt(growth_rate, id_vars=['geo\\\\time'           ], var_name = \"date\"       , value_name = 'growth_rate'              )\ngrowth_rate['date'      ] = pd.to_datetime(growth_rate['date'      ])\ngrowth_rate.head()",
  "generation": "growth_rate = pd.melt(growth_rate, id_vars=['geo\\\\time'], var_name='date', value_name='growth_rate')\ngrowth_rate['date'] = pd.to_datetime(growth_rate['date'], format='%Y')\ngrowth_rate = growth_rate.rename(columns={'geo\\\\time': 'country'})\ngrowth_rate.head()"
 },
 {
  "nbid": "NB_1044027",
  "file": "NB_1044027,data_population-1.ipynb",
  "row_id": 5,
  "target_code": "data_population = data_population.drop(columns=['2017'      ,'Unnamed: 62'             , 'Indicator Name'                 , 'Indicator Code'                 ])\ndata_population.head()",
  "generation": "data_population.drop(['Unnamed: 62', '2017', 'Indicator Code', 'Country Code', 'Indicator Name'], axis=1, inplace=True)\ndata_population.head(2)"
 },
 {
  "nbid": "NB_1044343",
  "file": "NB_1044343,df-1.ipynb",
  "row_id": 6,
  "target_code": "split_points = [\n    ('FE'    , 60, [False, True]),\n    ('SIO2'      , 9, [True, False]),\n    ('AL2O3'       , 2, [True, False]),\n    ('P'   , 0.08, [True, False]),  \n]\ndf['is_ore'        ] = np.vstack([\n    pd.cut(df[elem], bins=[0, split, 100], labels=is_ore)\n    for elem, split, is_ore in split_points\n]).sum(axis=0) == 4\ndf.tail()",
  "generation": "df['is_ore'] = np.where((df['FE']>60) & (df['SIO2']<9) & (df['AL2O3']<2) & (df['P']<0.08), True, False)\ndf.head()"
 },
 {
  "nbid": "NB_1044921",
  "file": "NB_1044921,shksprData-1.ipynb",
  "row_id": 1,
  "target_code": "del shksprData['Dataline'          ]\nshksprData = shksprData.dropna()\nshksprData.head()",
  "generation": "shksprData.drop(['Dataline'], axis=1, inplace=True)\nshksprData.dropna(inplace=True)\nshksprData.head()"
 },
 {
  "nbid": "NB_1048753",
  "file": "NB_1048753,sales-1.ipynb",
  "row_id": 11,
  "target_code": "sales = sales.drop(\"Checker\"         , axis = 1)\nsales.tail(5)",
  "generation": "sales.drop(['Checker', 'Total'], axis=1, inplace=True)\nsales.head()"
 },
 {
  "nbid": "NB_1048753",
  "file": "NB_1048753,sales-1.ipynb",
  "row_id": 13,
  "target_code": "sales = sales.loc[:2191]\nsales.tail(5)",
  "generation": "sales = sales.dropna()"
 },
 {
  "nbid": "NB_1051261",
  "file": "NB_1051261,previsores-1.ipynb",
  "row_id": 5,
  "target_code": "previsores = previsores.drop(\n    columns=['acao'      ,'pessoa1'         ,'pessoa2'         ,'objeto'        ,'tipo_preco'            ,'tratamento'            ])\nprevisores = pd.concat([previsores,pd.get_dummies(\n    df[['acao'      ,'pessoa1'         ,'pessoa2'         ,'objeto'        ,'tipo_preco'            ,'tratamento'            ]])], axis=1)\npd.set_option('display.max_columns'                     , None)\nprevisores['parcela'         ].fillna(value=0, inplace=True)\nprevisores.head()",
  "generation": "le = preprocessing.LabelEncoder()\nfor i in range(0, previsores.shape[1]):\n    if type(previsores.iloc[0,i]) == str:\n        previsores.iloc[:,i] = le.fit_transform(previsores.iloc[:,i])\nprevisores = pd.get_dummies(previsores, columns=[0,4,5,8,10,11,12,13,14,15,16,17,18,19,20,21]) \nprevisores"
 },
 {
  "nbid": "NB_1051774",
  "file": "NB_1051774,data-1.ipynb",
  "row_id": 10,
  "target_code": "length = lambda x: len(x)\ndata[\"text_length\"             ] = data[\"text\"      ].map(length)  \ndata.head(10)",
  "generation": "data['text_length'] = data['text'].apply(len)\ndata.head()"
 },
 {
  "nbid": "NB_1051862",
  "file": "NB_1051862,df_milk-1.ipynb",
  "row_id": 17,
  "target_code": "df_milk['Month'       ] = pd.to_datetime(df_milk['Month'       ])\ndf_milk.set_index('Month'       , inplace=True)\ndf_milk.head()",
  "generation": "df_milk = df_milk[\"Monthly milk production: pounds per cow. Jan 62 ? Dec 75\"]\ndf_milk"
 },
 {
  "nbid": "NB_1052724",
  "file": "NB_1052724,df-1.ipynb",
  "row_id": 20,
  "target_code": "df = df.replace({'Gender'        : {'Female'        : 0, 'Male'       : 1}})\ndf.head()",
  "generation": "df['Gender'] = df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)\ndf.head()"
 },
 {
  "nbid": "NB_1052724",
  "file": "NB_1052724,df-1.ipynb",
  "row_id": 21,
  "target_code": "df['HasOtherProducts'                  ] = df['NumOfProducts'               ] - df['HasCrCard'           ]\ndf.drop('NumOfProducts'               , axis=1, inplace=True)\ndf.head()",
  "generation": "df['Gender'] = df['Gender'].replace({0: 'Female', 1: 'Male'})\ndf['CreditScore'] = pd.cut(df['CreditScore'], bins=[0, 579, 669, 739, 799, 850], labels=['Very Poor', 'Fair', 'Good', 'Very Good', 'Exceptional'])\ndf['Age'] = pd.cut(df['Age'], bins=[0, 19, 29, 39, 49, 59, 69, 79, 89, 99], labels=['0-19', '20-29', '30-39', '40-49', '50-59', '60-69', '70-79', '80-89', '90-99'])\ndf['HasOtherProducts'] = df['NumOfProducts'] - df['HasCrCard']\ndf = df.drop(['NumOfProducts'], axis=1)"
 },
 {
  "nbid": "NB_160048",
  "file": "NB_160048,df-1.ipynb",
  "row_id": 3,
  "target_code": "columns = ['Title'       , 'Genres'         ]\nencoder = [LabelEncoder() for i in range(2)]\nfor i in range(2):\n    encoder[i].fit(df[columns[i]])\n    df[columns[i]] = encoder[i].transform(df[columns[i]]) \ndf.head()",
  "generation": "le = LabelEncoder()\ndf['Title'] = le.fit_transform(df['Title'])\ndf['Genres'] = le.fit_transform(df['Genres'])\ndf.head()"
 },
 {
  "nbid": "NB_3716506",
  "file": "NB_3716506,house_data-1.ipynb",
  "row_id": 13,
  "target_code": "house_data['Bedrooms'          ] = house_data['size'      ].str.split().str[0]\nhouse_data.head(10)",
  "generation": "house_data[\"Bedrooms\"] = house_data[\"size\"].apply(lambda x: int(x.split(\" \")[0]))\nhouse_data.head()"
 },
 {
  "nbid": "NB_3733452",
  "file": "NB_3733452,lyrics-1.ipynb",
  "row_id": 3,
  "target_code": "lyrics['clean text'            ] = (\n    lyrics['Lyrics'        ]\n    .str.replace(\"\\n\"    , \" \"    )\n    .str.lower()\n    .str.replace(r\"[^a-z\\s]\"           , \" \"    )\n)\nlyrics.head()",
  "generation": "vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', strip_accents='unicode', use_idf=True, norm='l2', analyzer='word', token_pattern=r'\\b[a-zA-Z]{3,}\\b')\nlyrics['lyrics_clean'] = lyrics['lyrics'].str.lower().str.replace('\\n', ' ').str.replace('[^\\w\\s]', '')\nlyrics_tfidf = vectorizer.fit_transform(lyrics['lyrics_clean'])\nlyrics_tfidf_df = pd.DataFrame(lyrics_tfidf.toarray(), columns=vectorizer.get_feature_names())\nlyrics_tfidf_df.head()"
 },
 {
  "nbid": "NB_4535697",
  "file": "NB_4535697,pred_df-1.ipynb",
  "row_id": 4,
  "target_code": "pred_df = pred_df.loc[(pred_df['date'      ] >= '2020-05-04'             ) & \n                      (pred_df['date'      ] <= '2020-05-11'             )]\npred_df = pred_df[['state'       , 'date'       , 'est_infections'                 ]]\nprint(pred_df.shape)\npred_df.head()",
  "generation": "pred_df = pred_df[pred_df['date'].between('2020-05-04', '2020-05-11')]\npred_df = pred_df[['state', 'date', 'est_infections']]\npred_df.head()"
 },
 {
  "nbid": "NB_4536909",
  "file": "NB_4536909,income_matrix-1.ipynb",
  "row_id": 5,
  "target_code": "income_matrix = income_matrix.drop(['1984'      ,'1985'      ,'1986'      ,'1987'      ,'1988'      ,'1989'      ,'1990'      ,'1991'      ,'1992'      ,'1993'      ,'1994'      ], axis=1)\nincome_matrix.head()",
  "generation": "income_matrix = income_matrix.loc[:, '1995':'2016']\nincome_matrix = income_matrix.reset_index()\nincome_matrix.head()"
 },
 {
  "nbid": "NB_4540333",
  "file": "NB_4540333,cons_2002-1.ipynb",
  "row_id": 7,
  "target_code": "cons_2002['Time'      ] = cons_2002['Hour_End'          ].apply(lambda x: dt.datetime.strptime(x, '%m/%d/%Y %H:%M'                 ))\ncons_2002 = cons_2002[['Hour_End'          ,'Time'      ,'COAST'       ,'EAST'      ,'FAR_WEST'          ,'NORTH'       ,'NORTH_C'         ,'SOUTHERN'          ,\n                       'SOUTH_C'                                ,'WEST'      ,'ERCOT'       ]]\ncons_2002.head()",
  "generation": "cons_2002['Time'] = pd.to_datetime(cons_2002['Hour_End'])\ncons_2002 = cons_2002[['Hour_End', 'Time', 'COAST', 'EAST', 'FAR_WEST', 'NORTH', 'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST', 'ERCOT']]\ncons_2002.head()"
 },
 {
  "nbid": "NB_4546127",
  "file": "NB_4546127,df-1.ipynb",
  "row_id": 9,
  "target_code": "df = df.drop(['MOVE6'       ],axis=1)\ndf = df.drop(['MOVE7'       ],axis=1)\ndf.head()",
  "generation": "df = df.drop(['MOVE6', 'MOVE7'], axis=1)\ndf.head()"
 },
 {
  "nbid": "NB_4546392",
  "file": "NB_4546392,df-1.ipynb",
  "row_id": 1,
  "target_code": "del df['date'      ]\ndf=df.fillna(0)\ndf.head()",
  "generation": "scaler = MinMaxScaler()\ndf.iloc[:, :8] = scaler.fit_transform(df.iloc[:, :8])\ndf.head()"
 },
 {
  "nbid": "NB_4549881",
  "file": "NB_4549881,output1-1.ipynb",
  "row_id": 3,
  "target_code": "output1 = output1[['datetime'          ,'temperature'             ,'humidity'          ,'co2'     ,'light'       ,'noise'       ,'bluetooth_devices'                   ,'count_total'             ]]\noutput1.head()",
  "generation": "input_df = pd.read_csv('input.csv')\noutput_df = input_df[['datetime', 'temperature', 'humidity', 'co2', 'light', 'noise', 'bluetooth_devices', 'count_total']]\noutput_df.head()"
 },
 {
  "nbid": "NB_4549881",
  "file": "NB_4549881,output1-1.ipynb",
  "row_id": 4,
  "target_code": "output1 =output1.dropna()\noutput1.head()",
  "generation": "output1 = output1.dropna()\noutput1 = output1.reset_index(drop=True)"
 },
 {
  "nbid": "NB_4553750",
  "file": "NB_4553750,data-1.ipynb",
  "row_id": 2,
  "target_code": "data = data.drop([\"Unnamed: 2\"            , \"Unnamed: 3\"             , \"Unnamed: 4\"             ], axis=1)\ndata = data.rename(columns={\"v1\"    :\"label\"       , \"v2\"     :\"message\"         })\ndata.head()",
  "generation": "data = data.rename(columns={'v1': 'label', 'v2': 'message'})\ndata = data[['label', 'message']]"
 },
 {
  "nbid": "NB_4558776",
  "file": "NB_4558776,df-1.ipynb",
  "row_id": 13,
  "target_code": "df[\"DNPOR\"       ]=df['RHOB'      ].apply(den, rf=1, rm=2.71 )\ndf.head()",
  "generation": "df['DNPOR'] = df['CNPOR'].apply(lambda x: (x - df['CNPOR'].min()) / (df['CNPOR'].max() - df['CNPOR'].min()) * 100 - 100)\ndf.head()"
 },
 {
  "nbid": "NB_4558776",
  "file": "NB_4558776,df-1.ipynb",
  "row_id": 21,
  "target_code": "df['TPOR'      ] = Tot_por(df['CNPOR'       ], df[\"DNPOR\"       ])\ndf.head()",
  "generation": "df['TPOR'] = (df['DNPOR'] + df['CNPOR']) / 2\ndf.head()"
 },
 {
  "nbid": "NB_4568362",
  "file": "NB_4568362,data-1.ipynb",
  "row_id": 4,
  "target_code": "data['last'      ] = data['name'      ].apply(feature_last)\ndata.head()",
  "generation": "data['last'] = data['name'].apply(lambda x: x[-1])\ndata.head()"
 },
 {
  "nbid": "NB_4584820",
  "file": "NB_4584820,df-1.ipynb",
  "row_id": 20,
  "target_code": "df['length'        ] = df['text'      ].apply(len)\ndf.head()",
  "generation": "df['percentage digits'] = df['text'].apply(lambda x: len([char for char in x if char.isdigit()])/len(x)*100)\ndf['percentage question mark'] = df['text'].apply(lambda x: len([char for char in x if char == '?'])/len(x)*100)\ndf['percentage exclamation mark'] = df['text'].apply(lambda x: len([char for char in x if char == '!'])/len(x)*100)\ndf['percentage period'] = df['text'].apply(lambda x: len([char for char in x if char == '.'])/len(x)*100)\ndf['percentage caps'] = df['text'].apply(lambda x: len([char for char in x if char.isupper()])/len(x)*100)\ndf['contains emoji'] = df['text'].apply(lambda x: 1 if any(char in x for char in ['\ud83d\ude02', '\u2764\ufe0f', '\ud83d\ude0a', '\ud83d\udc95', '\ud83d\ude0d', '\ud83d\udc4d']) else 0)\ndf['length'] = df['text'].apply(len)\ndf.head()"
 },
 {
  "nbid": "NB_4599249",
  "file": "NB_4599249,titanic-1.ipynb",
  "row_id": 7,
  "target_code": "titanic['CabinInd'          ] = np.where(titanic['Cabin'       ].isnull(), 0, 1) \ntitanic.head(10)",
  "generation": "titanic[\"CabinInd\"] = np.where(titanic[\"Cabin\"].isnull(), 0, 1)\ntitanic = titanic.drop([\"Cabin\"], axis=1)\ntitanic.head()"
 },
 {
  "nbid": "NB_4604411",
  "file": "NB_4604411,house_age_df-1.ipynb",
  "row_id": 11,
  "target_code": "house_age_df[\"scaled\"        ] = scale(house_age_df)\nhouse_age_df.head()",
  "generation": "scaler = MinMaxScaler(feature_range=(range_min, range_max))\nhouse_age_df['scaled'] = scaler.fit_transform(house_age_df[['house_age']])\nhouse_age_df.head()"
 },
 {
  "nbid": "NB_4604411",
  "file": "NB_4604411,house_age_df-1.ipynb",
  "row_id": 14,
  "target_code": "house_age_df['range_scaled'              ] = ((house_age_df['house_age'           ] - house_age_min) /                                (house_age_max - house_age_min)) *                                (range_max - range_min) +                                range_min\nhouse_age_df.head()",
  "generation": "house_age_df['range_scaled'] = range_min + ((house_age_df['scaled'] - house_age_min) / (house_age_max - house_age_min)) * (range_max - range_min)\nhouse_age_df"
 },
 {
  "nbid": "NB_4610137",
  "file": "NB_4610137,user1-1.ipynb",
  "row_id": 19,
  "target_code": "user1slice3A = (df.loc[:, 'CallTime'           ] < '17:00:00'           )\nuser1 = user1.loc[user1slice3A, :]\nuser1.head(10)",
  "generation": "user1sliceA = (df.loc[:, 'In'] == uniquecallers[0])\nuser1sliceB = (df.loc[:, 'DOW'].isin(['Mon', 'Tue', 'Wed', 'Thr', 'Fri']))\nuser1slice2 = (user1sliceA & user1sliceB)\nuser1 = df.loc[user1slice2, :]\nuser1.loc[:, 'CallTime'] = pd.to_timedelta(user1.loc[:, 'CallTime'], errors='coerce')\nuser1 = user1.loc[user1.loc[:, 'CallTime'] < '17:00:00', :]\nuser1 = user1.loc[(user1.loc[:, 'DOW'] != 'Sat') & (user1.loc[:, 'DOW'] != 'Sun'), :]\nuser1.head()"
 },
 {
  "nbid": "NB_4654144",
  "file": "NB_4654144,ml_data-1.ipynb",
  "row_id": 2,
  "target_code": "from sklearn.preprocessing import LabelEncoder \nlabelencoder= LabelEncoder() \nml_data['State'       ] = labelencoder.fit_transform(ml_data['State'       ]) \nml_data.head(2)",
  "generation": "le = LabelEncoder()\nml_data['State'] = le.fit_transform(ml_data['State'])\nml_data.head()"
 },
 {
  "nbid": "NB_1058006",
  "file": "NB_1058006,df2-1.ipynb",
  "row_id": 16,
  "target_code": "df2['Totalcharges'              ] = pd.to_numeric(df2['Totalcharges'              ], errors='coerce'        )\ndf2['Monthlycharges'                ] = pd.to_numeric(df2['Monthlycharges'                ], errors='coerce'        )\ndf2['Extracharges'              ] = abs(round(df2['Totalcharges'              ] - df2['Monthlycharges'                ],0))\ndf2['Extracharges'              ] = df2['Extracharges'              ].fillna(0.0)\ndf2['Monthlycharges'                ] = df2['Monthlycharges'                ].apply(np.ceil)\ndf2.head()",
  "generation": "df2['Extracharges'] = df2['Totalcharges'] - df2['Monthlycharges'] * df2['Tenure']\ndf2 = df2[['Gender', 'Seniorcitizen', 'Partner', 'Dependents', 'Tenure', 'Phoneservice', 'Multiplelines', 'Internetservice', 'Onlinesecurity', 'Onlinebackup', 'Deviceprotection', 'Techsupport', 'Streamingtv', 'Streamingmovies', 'Contract', 'Paperlessbilling', 'Paymentmethod', 'Monthlycharges', 'Totalcharges', 'Churn', 'Extracharges']]\ndf2.head()"
 },
 {
  "nbid": "NB_1061168",
  "file": "NB_1061168,df-1.ipynb",
  "row_id": 32,
  "target_code": "df = df.iloc[2:]\ndf.head(100)",
  "generation": "upsampled_data = upsampled_data.fillna(method='ffill')\nmodel_fit = model.fit(disp=0)\nsplit_point = round((len(df) * percent_training))\ndf = upsampled_data['diesel'].astype(int)\ndf.head()"
 },
 {
  "nbid": "NB_1061168",
  "file": "NB_1061168,df-1.ipynb",
  "row_id": 33,
  "target_code": "df=df['2016-5-2'          :'2018-10-9'           ]\ndf.tail(24)",
  "generation": "df = df.fillna(method='ffill')\ndf = df.astype(int)\ndf = df.iloc[2:100]"
 },
 {
  "nbid": "NB_1067939",
  "file": "NB_1067939,df-1.ipynb",
  "row_id": 5,
  "target_code": "df['profit'        ] = df['gross'       ] - df['budget'        ]\ndf['age'     ] = 2014 - df['name'      ].str[-5:-1].astype(int)\ndf.head()",
  "generation": "df['profit'] = df['gross'] - df['budget']\ndf['age'] = 2022 - df['name'].str[-5:-1].astype(int)\ndf.head()"
 },
 {
  "nbid": "NB_1069659",
  "file": "NB_1069659,data-1.ipynb",
  "row_id": 10,
  "target_code": "if (data[\"Residencial\"             ].dtype != int):\n    data[\"Residencial\"             ] = data[\"Residencial\"             ].str.replace(\".\"   ,\"\"  ).astype(int)\nif (data[\"Comercial\"           ].dtype != int):\n    data[\"Comercial\"           ] = data[\"Comercial\"           ].str.replace(\".\"   ,\"\"  ).astype(int)\nif (data[\"Industrial/Comercial Grande\"                             ].dtype != int):\n    data[\"Industrial/Comercial Grande\"                             ] = data[\"Industrial/Comercial Grande\"                             ].str.replace(\".\"   ,\"\"  ).astype(int)\nif (data[\"TOTAL\"       ].dtype != int):\n    data[\"TOTAL\"       ] = data[\"TOTAL\"       ].str.replace(\".\"   ,\"\"  ).astype(int)\ndata.tail()",
  "generation": "data.columns = data.columns.str.lower()\ndata.columns = data.columns.str.replace(' ', '_')\ndata.tail()"
 },
 {
  "nbid": "NB_1072936",
  "file": "NB_1072936,consumer_prices-1.ipynb",
  "row_id": 7,
  "target_code": "consumer_prices['Year'      ] = pd.to_datetime(consumer_prices['Year'      ], format='%Y'    )\nconsumer_prices.head()",
  "generation": "consumer_prices['Year'] = pd.to_datetime(consumer_prices['Year'], format='%Y')\nconsumer_prices.head()"
 },
 {
  "nbid": "NB_1075690",
  "file": "NB_1075690,df-1.ipynb",
  "row_id": 12,
  "target_code": "df.columns = df.iloc[0]\ndf = df.drop(0)\ndf.columns = df.columns.fillna('to_drop'         )\ndf.drop('to_drop'         , axis = 1, inplace = True)\ndf.head()",
  "generation": "df = df.iloc[:, :-1]\ndf.head()"
 },
 {
  "nbid": "NB_1077230",
  "file": "NB_1077230,num-1.ipynb",
  "row_id": 103,
  "target_code": "num['t1'    ] = num['type 1'        ].apply(tc)\nnum['t2'    ] = num['type 2'        ].apply(tc)\nnum.head()",
  "generation": "df['t1'] = df['type 1'].astype('category').cat.codes\ndf['t2'] = df['type 2'].astype('category').cat.codes\ndf.head()"
 },
 {
  "nbid": "NB_1079040",
  "file": "NB_1079040,dfscaled-1.ipynb",
  "row_id": 34,
  "target_code": "type1=pd.get_dummies(dfscaled['Type 1'        ])\ndfscaled=pd.concat([dfscaled.drop('Type 1'        ,axis=1),type1],axis=1)\ndfscaled.head()",
  "generation": "encoder1 = ce.OneHotEncoder(cols=['Type 1'])\ndf_encoded1 = encoder1.fit_transform(dfscaled)\n\nencoder2 = ce.BinaryEncoder(cols=['Type 2'], drop_invariant=True)\ndf_encoded2 = encoder2.fit_transform(df_encoded1)\n\ndf_encoded2.head()"
 },
 {
  "nbid": "NB_1082825",
  "file": "NB_1082825,SMS-1.ipynb",
  "row_id": 6,
  "target_code": "SMS = SMS.drop(labels = [\"Unnamed: 2\"            , \"Unnamed: 3\"             , \"Unnamed: 4\"             ], axis = 1)\nSMS.columns = [\"Tipo\"      , \"Mensaje\"          ]\nSMS.head()",
  "generation": "SMS = pd.read_csv('spam.csv',delimiter=',',encoding='latin-1')\nSMS = SMS[['v1', 'v2']]\nSMS.columns = ['Tipo', 'Mensaje']\nSMS.head()"
 },
 {
  "nbid": "NB_1082835",
  "file": "NB_1082835,refugees-1.ipynb",
  "row_id": 6,
  "target_code": "refugees['year'      ] = refugees['year'      ].astype('int32'       )\nrefugees['value'       ] = refugees['value'       ].astype('int32'       )\nrefugees.head()",
  "generation": "refugees['Year'                             ] = refugees['Year'                             ].astype(int)\nrefugees.dropna(inplace=True)\nrefugees.columns = ['year'                   , 'country_territory_of_asylum' , 'origin' , 'population_type' , 'value' ]\nrefugees.head()"
 },
 {
  "nbid": "NB_1083225",
  "file": "NB_1083225,df-1.ipynb",
  "row_id": 33,
  "target_code": "df=df['2016-5-2'          :'2018-10-9'           ]\ndf.head(24)",
  "generation": "df = df.iloc[2:].head(5)"
 },
 {
  "nbid": "NB_1084026",
  "file": "NB_1084026,df-1.ipynb",
  "row_id": 30,
  "target_code": "df['gender_numeric'                ] = np.where(df['gender'        ] == 'Male'       , 1, np.where(df['gender'        ] == 'Female'         , 0, df['gender'        ]))\ndf.head(5)",
  "generation": "df['gender_numeric'] = np.where(df['gender']=='Male', 1, 0)\ndf.head()"
 },
 {
  "nbid": "NB_1086687",
  "file": "NB_1086687,df-1.ipynb",
  "row_id": 20,
  "target_code": "df = df.loc[:, df.dtypes == np.int64]\ndf.head()",
  "generation": "df = df.select_dtypes(include=['int64'])\ndf['ProductCode'] = df['Product'].astype('category').cat.codes\ndf['LoanStatus'] = df['LoanStatus360'].apply(lambda x: 1 if x == 'Active' else 0)\ndf = df[['TotalPrice', 'Deposit', 'DailyRate', 'TotalDays', 'AmountPaid30', 'AmountPaid60', 'AmountPaid360', 'Gender', 'RegionCode', 'LocationCode', 'ProductCode', 'LoanStatus']]\ndf.head()"
 },
 {
  "nbid": "NB_137168",
  "file": "NB_137168,dfjumps_by_year-1.ipynb",
  "row_id": 20,
  "target_code": "dfjumps_by_year=dfjumps_by_year[['Date'      ,'ScaledVal'           ]]\ndfjumps_by_year.set_index('Date'      , inplace=True)\ndfjumps_by_year.head(3)",
  "generation": "std_dev_yearly[\"ScaledVal\"] = std_dev_yearly[\"Total Stock Market\"].apply(lambda x: x / 4000)\nstd_dev_yearly[\"ScaledVal\"]"
 },
 {
  "nbid": "NB_137401",
  "file": "NB_137401,data-1.ipynb",
  "row_id": 5,
  "target_code": "data=data.sort_values(['Loan rate'           ],ascending=[0])\ndata.head()",
  "generation": "private_school=data[data['Public/Private']==0]\npublic_school=data[data['Public/Private']==1]\nsns.distplot(private_school['Loan rate'], hist=False, rug=True)\nsns.distplot(public_school['Loan rate'], hist=False, rug=True)"
 },
 {
  "nbid": "NB_3759351",
  "file": "NB_3759351,df-1.ipynb",
  "row_id": 7,
  "target_code": "from sklearn.preprocessing import LabelEncoder\nnumber = LabelEncoder()\ndf[\"day_of_week\"             ] = df['day_of_week'             ].map({'Monday'        : 1,'Tuesday'         :2,'Wednesday'           :3,'Thursday'          :4,'Friday'        :5,'Saturday'          :6,'Sunday'        :7 })\ndf[\"encode_air_area_name\"                      ] = number.fit_transform(df[\"air_area_name\"               ].astype('str'     ))\ndf.head()",
  "generation": "number = LabelEncoder()\ndf[\"encode_air_area_name\"] = number.fit_transform(df[\"air_area_name\"].astype('str'))\ndf.head()"
 },
 {
  "nbid": "NB_3778062",
  "file": "NB_3778062,infarkty-1.ipynb",
  "row_id": 6,
  "target_code": "for label in labels:\n    infarkty[label] = pd.to_numeric(infarkty[label])\ninfarkty['death'       ] = infarkty['death'       ].map({0.0: 0, 1.0: 1})\nprint(infarkty.dtypes)\ninfarkty.head()",
  "generation": "infarkty['death'] = infarkty['death'].astype(float)\ninfarkty.index = infarkty.index.astype(float)\ninfarkty.head()"
 },
 {
  "nbid": "NB_3787421",
  "file": "NB_3787421,bankdata-1.ipynb",
  "row_id": 5,
  "target_code": "education_map={'unknown'         :1,'primary'         :2,'secondary'           :3,'tertiary'          :4}\nbankdata['education'           ]=bankdata['education'           ].map(education_map)\njob_map={'unknown'         :1,'student'         :2,'unemployed'            :3,'retired'         :4,'housemaid'           :5,'services'          :6,'blue-collar'             :7,'technician'            :8,'self-employed'               :9,'admin.'        :10,'management'            :11,'entrepreneur'              :12}\nbankdata['job'     ]=bankdata['job'     ].map(job_map)\nmonth_map={'jan'     :1,'feb'     :2,'mar'     :3,'apr'     :4,'may'     :5,'jun'     :6,'jul'     :7,'aug'     :8,'sep'     :9,'oct'     :10,'nov'     :11,'dec'     :12}\nbankdata['month'       ]=bankdata['month'       ].map(month_map)\nthis_map={'no'    :0,'yes'     :1}\nbankdata['default'         ]=bankdata['default'         ].map(this_map)\nbankdata['housing'         ]=bankdata['housing'         ].map(this_map)\nbankdata['loan'      ]=bankdata['loan'      ].map(this_map)\nbankdata['y'   ]=bankdata['y'   ].map(this_map)\nbankdata=pd.get_dummies(bankdata)\nbankdata.head()",
  "generation": "# Mapping ordinal features\nbankdata['education'] = bankdata['education'].map({'primary':0, 'secondary':1, 'tertiary':2})\n# Encoding class labels\nlabelencoder = LabelEncoder()\nbankdata['y'] = labelencoder.fit_transform(bankdata['y'])\n# nominal features\nbankdata = pd.get_dummies(bankdata, columns=['job', 'marital', 'contact', 'poutcome'])\nbankdata.head()"
 },
 {
  "nbid": "NB_3796552",
  "file": "NB_3796552,df-1.ipynb",
  "row_id": 20,
  "target_code": "one_hot_encoded = pd.get_dummies(df['Embarked'          ],prefix='Embarked'          )\ndf = df.join(one_hot_encoded)\ndf.head(5)",
  "generation": "df = pd.get_dummies(df, columns=['Embarked'])"
 },
 {
  "nbid": "NB_3796552",
  "file": "NB_3796552,df-1.ipynb",
  "row_id": 21,
  "target_code": "df = df.drop(['Embarked'          ], axis=1)\ndf.head(5)",
  "generation": "le = preprocessing.LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\ndf.head()"
 },
 {
  "nbid": "NB_3799271",
  "file": "NB_3799271,df-1.ipynb",
  "row_id": 3,
  "target_code": "df['date'      ] = pd.to_datetime(df.date, format=\"%d-%b-%y\"          )\ndf.head()",
  "generation": "df['date'] = pd.to_datetime(df['date'])\ndf.head()"
 },
 {
  "nbid": "NB_3806184",
  "file": "NB_3806184,df-1.ipynb",
  "row_id": 8,
  "target_code": "df = df.rename(columns={\"Cement (component 1)(kg in a m^3 mixture)\"                                           :\"Cement\"        ,\"Blast Furnace Slag (component 2)(kg in a m^3 mixture)\"                                                       :\"Slag\"      ,\"Fly Ash (component 3)(kg in a m^3 mixture)\"                                            :\"Ash\"     ,\"Water (component 4)(kg in a m^3 mixture)\"                                          :\"Water\"       ,\"Superplasticizer (component 5)(kg in a m^3 mixture)\"                                                     :\"Superplastic\"              ,\"Coarse Aggregate (component 6)(kg in a m^3 mixture)\"                                                     :\"Coarse_agg\"            ,\"Fine Aggregate (component 7)(kg in a m^3 mixture)\"                                                   :\"Fine_agg\"          ,\"Age (day)\"           :\"Age\"     ,\"Concrete compressive strength(MPa, megapascals)\"                                                 :\"Strength\"          })\ndf.head()",
  "generation": "df.columns = ['Cement', 'Slag', 'Ash', 'Water', 'Superplastic', 'Coarse_agg', 'Fine_agg', 'Age', 'Strength']\ndf.head()"
 },
 {
  "nbid": "NB_3818421",
  "file": "NB_3818421,titanic-1.ipynb",
  "row_id": 6,
  "target_code": "titanic = titanic.drop(columns=['cabin'       , 'deck'       , 'title'        , 'ticket'         ])\ntitanic.head()",
  "generation": "titanic.drop(['ticket', 'cabin', 'deck', 'fare_ordinal', 'title', 'age_ordinal'], axis=1, inplace=True)\ntitanic.rename(columns={'fare': 'fare_ordinal', 'title_short': 'title'}, inplace=True)\ntitanic.head()"
 },
 {
  "nbid": "NB_3821094",
  "file": "NB_3821094,dfx-1.ipynb",
  "row_id": 3,
  "target_code": "dfx = dfx.drop(labels=['C2'    , 'C3p'      ],axis=1)\ndfx.head()",
  "generation": "dfx.drop(['C3p', 'C5p', 'C6p', 'T4p', 'T3p', 'S2p', 'C2'], axis=1, inplace=True)\ndfx.head()"
 },
 {
  "nbid": "NB_3829960",
  "file": "NB_3829960,df_new-1.ipynb",
  "row_id": 83,
  "target_code": "df_new[['ca'    , 'uk'     , 'us'     ]] = pd.get_dummies(df_new['country'         ])\ndf_new.head()",
  "generation": "df_new[['ca', 'uk', 'us']] = pd.get_dummies(df_new['country'])\ndf_new.head()"
 },
 {
  "nbid": "NB_3835060",
  "file": "NB_3835060,mush_data-1.ipynb",
  "row_id": 7,
  "target_code": "y = mush_data['class_edible'              ]\nmush_data = mush_data.drop(labels=['class_edible'              ], axis=1)\nmush_data.head()",
  "generation": "mush_data = pd.get_dummies(mush_data, columns=['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat'])\nmush_data.head()"
 },
 {
  "nbid": "NB_3851721",
  "file": "NB_3851721,df-1.ipynb",
  "row_id": 6,
  "target_code": "df['pickup'        ]= df['tpep_pickup_datetime'                      ].apply(stringToDatetime)\ndf['dropoff'         ]= df['tpep_dropoff_datetime'                       ].apply(stringToDatetime)\ndf.drop(['tpep_pickup_datetime'                      ,'tpep_dropoff_datetime'                       ],axis=1,inplace=True)\ndf['timediff'          ]= df['dropoff'         ]- df['pickup'        ]\ndf['time'      ]= df['timediff'          ].apply(dateToNumber)\ndf.drop(['pickup'        ,'dropoff'         ,'timediff'          ],axis=1,inplace=True)\ndf.head(10)",
  "generation": "df = df[['trip_distance', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'total_amount', 'time']]\ndf['time'] = pd.to_datetime(df['time'], unit='s')\ndf.head()"
 },
 {
  "nbid": "NB_3851721",
  "file": "NB_3851721,df-1.ipynb",
  "row_id": 7,
  "target_code": "df=df[df.dropoff_latitude>0]\ndf= df[df.total_amount>0]\ndf = df[(df.time>0) & (df.time<20000)]\ndf.head(20)",
  "generation": "df = pd.read_csv('taxi_trial.csv')\ndf['pickup'] = pd.to_datetime(df['tpep_pickup_datetime'])\ndf['dropoff'] = pd.to_datetime(df['tpep_dropoff_datetime'])\ndf['timediff'] = df['dropoff'] - df['pickup']\ndf['time'] = df['timediff'].dt.total_seconds()\ndf = df.drop(['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'pickup', 'dropoff', 'timediff'], axis=1)\ndf = df[df['time'] > 0]\ndf.head()"
 },
 {
  "nbid": "NB_3887524",
  "file": "NB_3887524,dd-1.ipynb",
  "row_id": 29,
  "target_code": "dd=dd.dropna(axis='columns'         )\ndd.head()",
  "generation": "dd['Datetime'] = pd.to_datetime(dd['Datetime'])\ndd = dd.dropna()\ndd = dd[['Datetime', 'Values']]\ndd.head()"
 },
 {
  "nbid": "NB_3887524",
  "file": "NB_3887524,dd-1.ipynb",
  "row_id": 31,
  "target_code": "dd['Datetime'          ]=pd.to_datetime(dd['Datetime'          ],format='%Y-%m-%dT%H:%M:%S'                   ,utc=True) \ndd.head()",
  "generation": "dd['Datetime'] = pd.to_datetime(dd['Datetime'], format='%Y-%m-%dT%H:%M:%S')\ndd.head()"
 },
 {
  "nbid": "NB_3894696",
  "file": "NB_3894696,medit-1.ipynb",
  "row_id": 30,
  "target_code": "min_max_scaler = preprocessing.MinMaxScaler()\nscaled = min_max_scaler.fit_transform(medit)\nmedit = pd.DataFrame(scaled)\nmedit.head(2)",
  "generation": "min_max_scaler = preprocessing.MinMaxScaler()\nmedit[['age', 'balance', 'duration', 'campaign']] = min_max_scaler.fit_transform(medit[['age', 'balance', 'duration', 'campaign']])\nmedit.head()"
 },
 {
  "nbid": "NB_3898435",
  "file": "NB_3898435,reviews-1.ipynb",
  "row_id": 6,
  "target_code": "reviews = reviews.iloc[:, 1:]\nreviews.head()",
  "generation": "reviews = reviews.drop(columns=['Unnamed: 0'])"
 },
 {
  "nbid": "NB_3901848",
  "file": "NB_3901848,data-1.ipynb",
  "row_id": 10,
  "target_code": "data[\"100ma\"       ] = data[\"Adj Close\"           ].rolling(window=100,min_periods=0).mean()\ndata.head()",
  "generation": "data['100ma'] = data['Adj Close'].rolling(window=100).mean()\ndata.head()"
 },
 {
  "nbid": "NB_1105678",
  "file": "NB_1105678,data-1.ipynb",
  "row_id": 9,
  "target_code": "data['Date'      ] = pd.to_datetime(data['Date'      ], dayfirst=True)\ndata.head()",
  "generation": "data[\"Date\"] = pd.to_datetime(data[\"Day\"], format='%m/%d/%y')\ndata.drop([\"Day\"], axis = 1, inplace = True)\ndata = data[['Date', 'Unique Visitors', 'Cart Adds(Visitor)', 'Cart Removal (Visitor)_SJ', 'Start Checkouts (Visitor)', 'Orders (Visitor)', 'Revenue']]\ndata.columns = ['Date', 'Unique_Visitors', 'Cart_Adds', 'Cart_Removal', 'Start_Checkouts', 'Orders', 'Revenue']\ndata.head()"
 },
 {
  "nbid": "NB_1105678",
  "file": "NB_1105678,data-1.ipynb",
  "row_id": 14,
  "target_code": "data['Average-Order-Value'                     ] = data['Revenue'         ]/data['Orders'        ]\ndata.head()",
  "generation": "data['Average-Order-Value'] = data['Revenue']/data['Orders']\ndata.head()"
 },
 {
  "nbid": "NB_1105678",
  "file": "NB_1105678,data-1.ipynb",
  "row_id": 15,
  "target_code": "data['purchase_frequency'                    ] = data['Orders'        ]/data['Unique_Visitors'                 ]\ndata.head()",
  "generation": "data['purchase_frequency'] = data['Orders']/data['Unique_Visitors']\ndata.head()"
 },
 {
  "nbid": "NB_1112413",
  "file": "NB_1112413,data-1.ipynb",
  "row_id": 7,
  "target_code": "data=data.drop(['HourlyRate'            ,'Over18'        ,'MonthlyRate'             ,'DailyRate'           ,'StandardHours'               ,'EmployeeCount'               ],axis=1)\ndata.head()",
  "generation": "data.drop(['DailyRate', 'EmployeeCount', 'HourlyRate', 'MonthlyRate', 'Over18'], axis=1, inplace=True)\ndata.Attrition.replace({'Yes':1, 'No':0}, inplace=True)\ndata.OverTime.replace({'Yes':1, 'No':0}, inplace=True)\ndata = pd.get_dummies(data, columns=['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus'], drop_first=True)\ndata.head()"
 },
 {
  "nbid": "NB_1113604",
  "file": "NB_1113604,wid_fin1-1.ipynb",
  "row_id": 56,
  "target_code": "wid_fin1 = wid_fin1[['Pays'      , 'quantile'           , 'income'         , 'gdpppp'         , 'ISO3'       , '2008'       ]]\nwid_fin1.rename(columns = {'2008'       : 'Population'             }, inplace = True)\nwid_fin1.head(3)",
  "generation": "wid2 = wid1.drop(columns='year_survey')\npop_monde = pd.merge(code_pays, pop, how='left', left_on='ISO3', right_on='Country Code')\nwid_fin1 = pd.merge(wid2, pop_monde, how='left', on=['Pays', 'ISO3'])\nwid_fin1 = wid_fin1[['Pays', 'quantile', 'income', 'gdpppp', 'ISO3', 'Population']]\nwid_fin1 = wid_fin1.rename(columns={'Population': 'Population'})\nwid_fin1.head()"
 },
 {
  "nbid": "NB_1113822",
  "file": "NB_1113822,auto_mpg-1.ipynb",
  "row_id": 8,
  "target_code": "auto_mpg = auto_mpg.iloc[:,0:6]\nauto_mpg.head(2)",
  "generation": "auto_mpg = pd.read_csv('auto-mpg.data', delim_whitespace=True, header=None, names=['mpg', 'Cylinders', 'Displacement', 'HorsePower', 'Weight', 'Acceleration', 'Model Year', 'Origin', 'Car Name'])\nauto_mpg = auto_mpg[['mpg', 'Cylinders', 'Displacement', 'HorsePower', 'Weight', 'Acceleration']]\nauto_mpg.head()"
 },
 {
  "nbid": "NB_1116050",
  "file": "NB_1116050,data-1.ipynb",
  "row_id": 6,
  "target_code": "data = data.drop(['DailyRate'           ,'EmployeeNumber'                ,'EnvironmentSatisfaction'                         ,'JobInvolvement'                ,'JobSatisfaction'                 ,'MonthlyRate'             ,'Over18'        ,'PerformanceRating'                   ,'RelationshipSatisfaction'                          ,'StockOptionLevel'                  ], axis =1)\ndata.head()",
  "generation": "data.drop(['DailyRate', 'EmployeeNumber', 'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement', 'JobSatisfaction', 'MonthlyRate', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TrainingTimesLastYear', 'YearsSinceLastPromotion'], axis=1, inplace=True)\ndata.head()"
 },
 {
  "nbid": "NB_1116050",
  "file": "NB_1116050,data-1.ipynb",
  "row_id": 18,
  "target_code": "data['Attrition'           ] = data['Attrition'           ].map({ \n    'No'        : 0, \n    'Yes'         : 1\n}.get)\ndata.head()",
  "generation": "data['Attrition'] = data['Attrition'].apply(lambda x: 1 if x == 'Yes' else 0)\ndata.head()"
 },
 {
  "nbid": "NB_1116050",
  "file": "NB_1116050,data-1.ipynb",
  "row_id": 19,
  "target_code": "data['Gender'        ] = data['Gender'        ].map({ \n    'Male'          : 0, \n    'Female'            : 1\n}.get)\ndata.head()",
  "generation": "data['Gender'] = data['Gender'].map({\n    'Female': 1,\n    'Male': 0\n}.get)\ndata.head()"
 },
 {
  "nbid": "NB_1116959",
  "file": "NB_1116959,data-1.ipynb",
  "row_id": 11,
  "target_code": "data['more_than_6000'                ] = data.hourly_wage > 6000\ndata.head()",
  "generation": "data['seoul'] = data['area1'].apply(lambda x: 'in seoul' if x in ['\uac15\ub0a8\uad6c', '\uac15\ub3d9\uad6c', '\uac15\ubd81\uad6c', '\uac15\uc11c\uad6c', '\uad00\uc545\uad6c', '\uad11\uc9c4\uad6c', '\uad6c\ub85c\uad6c', '\uae08\ucc9c\uad6c', '\ub178\uc6d0\uad6c', '\ub3c4\ubd09\uad6c', '\ub3d9\ub300\ubb38\uad6c', '\ub3d9\uc791\uad6c', '\ub9c8\ud3ec\uad6c', '\uc11c\ub300\ubb38\uad6c', '\uc11c\ucd08\uad6c', '\uc131\ub3d9\uad6c', '\uc131\ubd81\uad6c', '\uc1a1\ud30c\uad6c', '\uc591\ucc9c\uad6c', '\uc601\ub4f1\ud3ec\uad6c', '\uc6a9\uc0b0\uad6c', '\uc740\ud3c9\uad6c', '\uc885\ub85c\uad6c', '\uc911\uad6c', '\uc911\ub791\uad6c'] else 'not in seoul')\ndata['more_than_6000'] = data['hourly_wage'].apply(lambda x: True if x > 6000 else False)\ndata.head(5)"
 },
 {
  "nbid": "NB_1119479",
  "file": "NB_1119479,dataset-1.ipynb",
  "row_id": 15,
  "target_code": "dataset['clean'       ] = dataset['clean'       ].str.replace(\"[^a-zA-Z#]\"            , \" \"    ) \ndataset.head()",
  "generation": "dataset['clean'] = dataset['sms'].str.replace(\"[^a-zA-Z#]\", \" \")\ndataset.head()"
 },
 {
  "nbid": "NB_1119617",
  "file": "NB_1119617,df1-1.ipynb",
  "row_id": 2,
  "target_code": "df1['Fouls'       ] = df1['PF'    ]\ndf1 = df1.drop(['PF'    ], axis=1)\ndum1 = pd.get_dummies(df1['Pos'     ])\ndf1 = pd.concat([df1, dum1], axis=1)\ndf1 = df1.drop(['Pos'     ], axis=1)\ndf1.head()",
  "generation": "df1[['C', 'C-PF', 'PF', 'PG', 'PG-SG', 'SF', 'SF-PF', 'SG', 'SG-PG']] = pd.get_dummies(df1['Pos'])\ndf1 = df1.drop('Pos', axis=1)"
 },
 {
  "nbid": "NB_1119617",
  "file": "NB_1119617,df1-1.ipynb",
  "row_id": 3,
  "target_code": "schoolnames = df1.School.unique()\nsc_dict = dict(zip(schoolnames, range(len(schoolnames))))\ndf1 = df1.replace({'School'        : sc_dict})\ndf1.head()",
  "generation": "df1['C'] = df1['C'] + df1['C-PF']\ndf1 = df1.drop(['C-PF'], axis=1)\ndf1 = df1.reset_index()\ndf1 = df1.rename(columns = {\"index\"       : \"Name\"         })\ndf1['Name'] = df['Name']\ndf1['Name'] = df1['Name'].astype(str)\ndf1['Name'] = df1['Name'].str.strip()\ndf1 = df1.set_index('Name')\ndf1 = df1.reset_index()\ndf1 = df1.drop(['index'], axis=1)\ndf1 = df1.reset_index()\ndf1 = df1.rename(columns = {\"index\"       : \"ID\"         })\ndf1['ID'] = df1['ID'] + 1\ndf1 = df1.set_index('ID')\ndf1"
 },
 {
  "nbid": "NB_3946728",
  "file": "NB_3946728,df-1.ipynb",
  "row_id": 10,
  "target_code": "dummies = pd.get_dummies(df['state'       ])\ndel df['state'       ]\ndf = pd.concat([df,dummies],axis=1)\ndf.head(3)",
  "generation": "from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Create dummy features for states\ndf = pd.get_dummies(df, columns=['state'])\n\n# Scale the data\nscaler = preprocessing.StandardScaler()\nX = scaler.fit_transform(df.drop(['Name', 'AdultWeekend'], axis=1))\ny = df.AdultWeekend.values\n\n# Create train and test splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the chosen model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Generate a model performance score based on the testing hold-out data set\nscore = model.score(X_test, y_test)\nprint(score)"
 },
 {
  "nbid": "NB_3966441",
  "file": "NB_3966441,data-1.ipynb",
  "row_id": 13,
  "target_code": "data['y_pred'        ] = (data.proba>=0.5).map({True:1,False:0})\nprint(data['y_pred'        ].value_counts())\ndata.head(3)",
  "generation": "data['y_pred'] = np.where(data['proba'] < 0.5, 0, 1)\nfrom sklearn.metrics import confusion_matrix, f1_score, accuracy_score, roc_curve, auc\nprint('Confusion Matrix:\\n', confusion_matrix(data['y'], data['y_pred']))\nprint('F1 Score:', f1_score(data['y'], data['y_pred']))\nfpr, tpr, thresholds = roc_curve(data['y'], data['proba'])\nprint('AUC Score:', auc(fpr, tpr))\nprint('Accuracy Score:', accuracy_score(data['y'], data['y_pred']))"
 },
 {
  "nbid": "NB_3978798",
  "file": "NB_3978798,df-1.ipynb",
  "row_id": 5,
  "target_code": "df=df[df.isnull().a != True]\ndf.head()",
  "generation": "df['a'] = df.iloc[:,0]\ndf['b'] = df.iloc[:,1]\ndf['c'] = df.iloc[:,2]\ndf['d'] = df.iloc[:,3]\ndf['e'] = df.iloc[:,4]\ndf['f'] = df.iloc[:,5]\ndf['g'] = df.iloc[:,6]\ndf['h'] = df.iloc[:,7]\ndf['i'] = df.iloc[:,8]\ndf = df[['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']]\ndf.head()"
 },
 {
  "nbid": "NB_3985751",
  "file": "NB_3985751,crime_df-1.ipynb",
  "row_id": 5,
  "target_code": "crime_df['Dates'       ] = crime_df['Dates'       ].apply(lambda x: time.mktime(time.strptime(x, '%m/%d/%Y %H:%M'                 ))/1000000)\ncrime_df.head()",
  "generation": "crime_df['Dates'] = pd.to_datetime(crime_df['Dates'])\ncrime_df['Dates'] = crime_df['Dates'].apply(lambda x: time.mktime(x.timetuple()) + (x.microsecond / 1000000.0))\ncrime_df.head()"
 },
 {
  "nbid": "NB_3997414",
  "file": "NB_3997414,df-1.ipynb",
  "row_id": 12,
  "target_code": "df = pd.concat([df, tmp_df], axis=1)\ndf.head()",
  "generation": "df[['sexo', 'idade_min', 'idade_max']] = df['sexo_e_idade'].str.extract('(\\D)(\\d{3})(\\d{3})')\ndf['idade'] = df['idade_min'] + '-' + df['idade_max']\ndf.drop('sexo_e_idade', axis=1, inplace=True)\ndf.head()"
 },
 {
  "nbid": "NB_3997414",
  "file": "NB_3997414,df-1.ipynb",
  "row_id": 14,
  "target_code": "df = df[['uf'    , 'ano'      , 'sexo'       , 'idade'        , 'casos'        ]]\ndf.head()",
  "generation": "tmp_df['idade'] = ((tmp_df['idade_min'] + '-') + tmp_df['idade_max'])\ndf = pd.melt(\n    df,\n    id_vars=['uf', 'ano', 'sexo_e_idade'],\n    value_vars=['casos'],\n    var_name='valor'\n)\ndf['sexo'] = df['sexo_e_idade'].str[0]\ndf['idade'] = df['sexo_e_idade'].str[1:]\ndf = df.drop(columns=['sexo_e_idade'])\ndf = df.pivot_table(index=['uf', 'ano', 'sexo', 'idade'], columns='valor', values='casos')\ndf = df.reset_index()\ndf.head()"
 },
 {
  "nbid": "NB_3999691",
  "file": "NB_3999691,water_treatment-1.ipynb",
  "row_id": 1,
  "target_code": "water_treatment = water_treatment.fillna(water_treatment.mean())\nwater_treatment.head()",
  "generation": "water_treatment.fillna(water_treatment.mean(), inplace=True)\nwater_treatment.head()"
 },
 {
  "nbid": "NB_4004702",
  "file": "NB_4004702,data-1.ipynb",
  "row_id": 5,
  "target_code": "data = data.drop(['id'    ], axis =1)\ndata.head()",
  "generation": "data = data.drop(['id', 'latitude', 'longitude', 'track_id', 'time.1'], axis=1)\ndata = data.dropna(subset=['linha'])\ndata = data.fillna(0)\ndata = data.drop(['Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'id.1'], axis=1)\ndata = data.rename(columns={'id_android': 'id'})\ndata = data.set_index('id')\ndata.head()"
 },
 {
  "nbid": "NB_4017612",
  "file": "NB_4017612,df-1.ipynb",
  "row_id": 5,
  "target_code": "df.date = pd.to_datetime(df.date, infer_datetime_format=True)\ndf = df.set_index(df['date'      ])\ndf = df.drop(columns='date'      )\ndf.head()",
  "generation": "df['date'] = pd.to_datetime(df['date'])\ndf.set_index('date', inplace=True)\ndf.head()"
 }
]